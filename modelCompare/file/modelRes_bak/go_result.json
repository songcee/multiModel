[{"prefix":"package main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MergeVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, ","infill":"version: %s\\n\", version)","suffix":"\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit() {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", \"/usr/local/conf/conf.yaml\", \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)","relevantFileList":[],"filePath":"main.go","template":"go","multiRes":{"hipilot":"Version: %s\\n\", version)","CodeQwen2.5-7B":"version: %s\", version)","copilot":"version: %s\\n\", version)","deepseek-chat-lite":"version: %s\", version)","CodeQwen1.5-7B":" version: %s\", version)"}},{"prefix":"package main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMe","infill":"tricsVersion()\n","suffix":"\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}\n","relevantFile":"<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)","relevantFileList":[],"filePath":"main.go","template":"go","multiRes":{"hipilot":"tricsVersion()","CodeQwen2.5-7B":"tricsVersion()","copilot":"tricsVersion()\n","deepseek-chat-lite":"tricsVersion()","CodeQwen1.5-7B":" tricsVersion()"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]","infill":"     string `yaml:\"name\" json:\"name\" binding:\"required\"`\n\tSchedule string `yaml:\"schedule\" json:\"schedule\" binding:\"required\"`","suffix":"\n}\n\ntype CronMetrics struct {\n\tCronTask\n}\n","relevantFile":"<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":";\n    if !ok {\n        return\n    }\n    for _, id := range taskDetail {\n        d.cron.Remove(id)\n    }\n    delete(d.scheduleDetail, task)","CodeQwen2.5-7B":"\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, id := range taskDetail {\n\t\td.cron.Remove(id)\n\t}\n\tdelete(d.scheduleDetail, task)\n}","copilot":"     string `yaml:\"name\" json:\"name\" binding:\"required\"`\n\tSchedule string `yaml:\"schedule\" json:\"schedule\" binding:\"required\"`","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\\","infill":"Task(Name string , schedule string) {\n\td.\n}\n","suffix":"","relevantFile":"<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"return}\n    for _, id := range taskDetail {\n        d.cron.Remove(id)\n    }\n}\n\nfunc (d *CronManager) Entries() []cron.Entry {\n    return d.cron.Entries()\n}\n\nfunc","CodeQwen2.5-7B":"\n\t\treturn\n\t}\n\tfor _, id := range taskDetail {\n\t\td.cron.Remove(id)\n\t}\n\tdelete(d.scheduleDetail, task)\n}","copilot":"Task(Name string , schedule string) {\n\td.\n}\n","deepseek-chat-lite":"","CodeQwen1.5-7B":" r\\n"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\\","infill":"schedule string) {\n\td.\n}\n","suffix":"","relevantFile":"<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"return}\n    for _, id := range taskDetail {\n        d.cron.Remove(id)\n    }\n}\n\nfunc (d *CronManager) GetEntry() map[string]CronEntryInfo {\n    return d.scheduleDetail\n}\n\nfunc","CodeQwen2.5-7B":"\n\t\treturn\n\t}\n\tfor _, id := range taskDetail {\n\t\td.cron.Remove(id)\n\t}\n\tdelete(d.scheduleDetail, task)\n}","copilot":"schedule string) {\n\td.\n}\n","deepseek-chat-lite":"","CodeQwen1.5-7B":" r\\n"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[ta","infill":"{\n\td.","suffix":"\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/metrics/metrics.go\n\t\t\tName: \"gms_etl_error\",\n\t\t\tHelp: \"errors\",\n\t\t},\n\t\t[]string{\"status\"},\n\t)\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"sk]\n    if ok {\n        return cron.ErrTaskExists\n    }\n    entryId, err := d.cron.AddFunc(schedule, cronFunc)\n    if err != nil {\n        return err\n    }\n    d.scheduleDetail[task","CodeQwen2.5-7B":"sk]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\tentryID, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = entryID\n\treturn nil\n}","copilot":"{\n\td.","deepseek-chat-lite":"sk]","CodeQwen1.5-7B":" sk]"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\\","infill":"string]map[string]prometheus.Gauge","suffix":"\n}\n\nfunc (d *CronMetrics) AddTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule] = promauto.NewGauge(prometheus.GaugeOpts{\n}\n","relevantFile":"<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"/     d.cron.Remove(d.scheduleDetail[task])","CodeQwen2.5-7B":"\n}","copilot":"string]map[string]prometheus.Gauge","deepseek-chat-lite":"","CodeQwen1.5-7B":"\\"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\ntype CronFuncWithMetrics struct {","infill":"CronFunc func()\n\tName     string\n\tSchedule string","suffix":"\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, \n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n\ntype CronMetrics struct {\n\tTaskExecAt map[string]map[string]prometheus.Gauge\n}\n\nfunc (d *CronMetrics) AddTask(Name string, schedule string) {\n\t_, ok := d.TaskExecAt[Name]\n\tif !ok {\n\t\td.TaskExecAt[Name] = make(map[string]prometheus.Gauge)\n\t}\n\td.TaskExecAt[Name][schedule] = prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"cron_task_exec_at\",\n\t\tHelp: \"cron task exec at\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     Name,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n}\n\nfunc (d *CronMetrics) SetTaskExecAt(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].SetToCurrentTime()\n}\n\nfunc (d *CronMetrics) RemoveTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].Set(0)\n\tdelete(d.TaskExecAt[Name], schedule)\n}\n","relevantFile":"<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n<file_path>cron.go\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\tc.remove <- id\n\t} else {\n\t\tc.removeEntry(id)\n\t}\n}\n\n// Start the cron scheduler in its own goroutine, or no-op if already started.\nfunc (c *Cron) Start() {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treturn\n\t}\n\tc.running = true\n\tgo c.run()\n}\n\n// Run the cron scheduler, or no-op if already running.\nfunc (c *Cron) Run() {\n\tc.runningMu.Lock()\n\tif c.running {\n\t\tc.runningMu.Unlock()\n\t\treturn\n\t}\n\tc.running = true\n\tc.runningMu.Unlock()\n\tc.run()\n}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"\n    Name string\n    Func func()","CodeQwen2.5-7B":"\n\tCronFunc func()\n\tMetrics  *CronMetrics\n}","copilot":"CronFunc func()\n\tName     string\n\tSchedule string","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\ntype CronFuncWithMetrics struct {\n\tCronFunc func()\n\tName     string\n\tSchedule string\n}\n\nfunc (d *CronFuncWithMetrics) CronFunc() {\n\tGCronMetrics.SetTaskExecAt(d.Name, d.Schedule)\n\n\td.CronFunc()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDeta","infill":"Schedule: schedule,\n\t}.CronFunc)","suffix":"\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n\ntype CronMetrics struct {\n\tTaskExecAt map[string]map[string]prometheus.Gauge\n}\n\nfunc (d *CronMetrics) AddTask(Name string, schedule string) {\n\t_, ok := d.TaskExecAt[Name]\n\tif !ok {\n\t\td.TaskExecAt[Name] = make(map[string]prometheus.Gauge)\n\t}\n\td.TaskExecAt[Name][schedule] = prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"cron_task_exec_at\",\n\t\tHelp: \"cron task exec at\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     Name,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n}\n\nfunc (d *CronMetrics) SetTaskExecAt(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].SetToCurrentTime()\n}\n\nfunc (d *CronMetrics) RemoveTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].Set(0)\n\tdelete(d.TaskExecAt[Name], schedule)\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"il[task]\n    if !ok {\n        d.scheduleDetail[task] = make(CronEntryInfo)\n    }\n    id, err := d.cron.AddJob(schedule, CronFuncWithMetrics{\n        CronFunc: cronFunc,","CodeQwen2.5-7B":"il[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\tid, err := d.cron.AddFunc(schedule, CronFuncWithMetrics{CronFunc: cronFunc, Name: task, Schedule: schedule})","copilot":"Schedule: schedule,\n\t}.CronFunc)","deepseek-chat-lite":"il[task]","CodeQwen1.5-7B":" il[task]"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\ntype CronFuncWithMetrics struct {\n\tCronFunc func()\n\tName     string\n\tSchedule string\n}\n\nfunc (d *CronFuncWithMetrics) CronFunc() {\n\tGCronMetrics.SetTaskExecAt(d.Name, d.Schedule)\n\n\td.CronFunc()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDeta","infill":".CronFunc)\n","suffix":"\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n\ntype CronMetrics struct {\n\tTaskExecAt map[string]map[string]prometheus.Gauge\n}\n\nfunc (d *CronMetrics) AddTask(Name string, schedule string) {\n\t_, ok := d.TaskExecAt[Name]\n\tif !ok {\n\t\td.TaskExecAt[Name] = make(map[string]prometheus.Gauge)\n\t}\n\td.TaskExecAt[Name][schedule] = prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"cron_task_exec_at\",\n\t\tHelp: \"cron task exec at\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     Name,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n}\n\nfunc (d *CronMetrics) SetTaskExecAt(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].SetToCurrentTime()\n}\n\nfunc (d *CronMetrics) RemoveTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].Set(0)\n\tdelete(d.TaskExecAt[Name], schedule)\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"il[task]\n    if !ok {\n        d.scheduleDetail[task] = make(CronEntryInfo)\n    }\n    id, err := d.cron.AddJob(schedule, CronFuncWithMetrics{\n        CronFunc: cronFunc,","CodeQwen2.5-7B":"il[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\tid, err := d.cron.AddFunc(schedule, CronFuncWithMetrics{CronFunc: cronFunc, Name: task, Schedule: schedule})\n","copilot":".CronFunc)\n","deepseek-chat-lite":"il[task]","CodeQwen1.5-7B":" il[task]"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\ntype CronFuncWithMetrics struct {\n\tFunc     func()\n\tName     string\n\tSchedule string\n\t\n}\n\nfu","infill":"nc (d *CronFuncWithMetrics) Func() {\n\tCronMetrics.SetTaskExecAt(d.Name, d.Schedule)\n\n\td.CronFunc()\n}","suffix":"\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, CronFuncWithMetrics{\n\t\tCronFunc: cronFunc,\n\t\tName:     task,\n\t\tSchedule: schedule,\n\t}.CronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n\ntype CronMetrics struct {\n\tTaskExecAt map[string]map[string]prometheus.Gauge\n}\n\nfunc (d *CronMetrics) AddTask(Name string, schedule string) {\n\t_, ok := d.TaskExecAt[Name]\n\tif !ok {\n\t\td.TaskExecAt[Name] = make(map[string]prometheus.Gauge)\n\t}\n\td.TaskExecAt[Name][schedule] = prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"cron_task_exec_at\",\n\t\tHelp: \"cron task exec at\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     Name,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n}\n\nfunc (d *CronMetrics) SetTaskExecAt(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].SetToCurrentTime()\n}\n\nfunc (d *CronMetrics) RemoveTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].Set(0)\n\tdelete(d.TaskExecAt[Name], schedule)\n}\n","relevantFile":"<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n<file_path>cron.go\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\tc.remove <- id\n\t} else {\n\t\tc.removeEntry(id)\n\t}\n}\n\n// Start the cron scheduler in its own goroutine, or no-op if already started.\nfunc (c *Cron) Start() {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"nc (c CronFuncWithMetrics) CronFunc() {\n    defer func(Name string, Schedule string) {\n        CronMetrics{}.SetTaskExecAt(Name, Schedule)\n    }(c.Name, c.Schedule)\n    c.Func()","CodeQwen2.5-7B":"nc (d CronFuncWithMetrics) CronFunc() {\n\td.Func()\n\tGCronManager.SetTaskExecAt(d.Name, d.Schedule)\n}","copilot":"nc (d *CronFuncWithMetrics) Func() {\n\tCronMetrics.SetTaskExecAt(d.Name, d.Schedule)\n\n\td.CronFunc()\n}","deepseek-chat-lite":"nc (d CronFuncWithMetrics) CronFunc() {","CodeQwen1.5-7B":" nc (d *CronFuncWithMetrics) CronFunc() {"}},{"prefix":"package cron\n\nimport (\n\t\"github.com/robfig/cron/v3\"\n\t\"time\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleD","infill":"etail[task] = ma","suffix":"ke(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n","relevantFile":"<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n<file_path>cron.go\nfunc New(opts ...Option) *Cron {\n\tc := &Cron{\n\t\tentries:   nil,\n\t\tchain:     NewChain(),\n\t\tadd:       make(chan *Entry),\n\t\tstop:      make(chan struct{}),\n\t\tsnapshot:  make(chan chan []Entry),\n\t\tremove:    make(chan EntryID),\n\t\trunning:   false,\n\t\trunningMu: sync.Mutex{},\n\t\tlogger:    DefaultLogger,\n\t\tlocation:  time.Local,\n\t\tparser:    standardParser,\n\t}\n\tfor _, opt := range opts {\n\t\topt(c)\n\t}\n\treturn c\n}\n\n// FuncJob is a wrapper that turns a func() into a cron.Job\ntype FuncJob func()\n\nfunc (f FuncJob) Run() { f() }\n\n// AddFunc adds a func to the Cron to be run on the given schedule.\n// The spec is parsed using the time zone of this Cron instance as the default.\n// An opaque ID is returned that can be used to later remove it.\nfunc (c *Cron) AddFunc(spec string, cmd func()) (EntryID, error) {\n\treturn c.AddJob(spec, FuncJob(cmd))\n}\n\n// AddJob adds a Job to the Cron to be run on the given schedule.\n// The spec is parsed using the time zone of this Cron instance as the default.\n// An opaque ID is returned that can be used to later remove it.\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"etail[task] = ma","CodeQwen2.5-7B":"etail[task] = ma<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"etail[task] = ma","deepseek-chat-lite":"etail[task] = ma","CodeQwen1.5-7B":" etail[task] = ma<file_sep> /src/main/java/com/example/demo/service/impl/UserServiceImpl.java"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != ","infill":"prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\": task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).Set\t\n\t","suffix":")\n\n\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"nil {\n        return err\n    }\n    d.scheduleDetail[task][schedule] = id","CodeQwen2.5-7B":"nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\tprometheus.MustRegister(CronTaskCount.WithLabelValues(task, schedule)","copilot":"prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\": task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).Set\t\n\t","deepseek-chat-lite":"nil {","CodeQwen1.5-7B":" nil {"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != ","infill":").SetToCurrentTime()\n","suffix":"\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n\n\tprometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\": task,\n\t\t},\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"nil {\n        return err\n    }\n    d.scheduleDetail[task][schedule] = id\n","CodeQwen2.5-7B":"nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\tprometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\": task,\n\t\t},\n\t})\n\treturn nil\n}","copilot":").SetToCurrentTime()\n","deepseek-chat-lite":"nil {","CodeQwen1.5-7B":" nil {"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\t","infill":"\n}\n","suffix":"","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"return err\n    }\n    d.scheduleDetail[task][schedule] = id\n    return nil\n}\n\nfunc (d *CronManager) RemoveTask(task string, schedule string) {\n    entryIds, ok := d.scheduleDetail[task]\n   ","CodeQwen2.5-7B":"return err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}","copilot":"\n}\n","deepseek-chat-lite":"return err","CodeQwen1.5-7B":"return err"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\t","infill":"\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}))","suffix":"\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"return err\n    }\n    d.scheduleDetail[task][schedule] = id\n    return nil","CodeQwen2.5-7B":"return err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}","copilot":"\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}))","deepseek-chat-lite":"return err","CodeQwen1.5-7B":"return err"}},{"prefix":"package config\n\nimport (\n\t\"flag\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"os\"\n\n\t\"gopkg.in/yaml.v2\"\n)\n\ntype (\n\tMyConfig struct {\n\t\tAddress       string   `yaml:\"Address\"`       // write data source name. (dsn without database name\n\t\tUser          string   `yaml:\"User\"`          // write data source name. (dsn without database name\n\t\tPasswd        string   `yaml:\"Passwd\"`        // write data source name. (dsn without database name\n\t\tHost          string   `yaml:\"Host\"`          // write data source name. (dsn without database name\n\t\tParams        string   `yaml:\"Params\"`        // DSN params\n\t\tDefaultDbname string   `yaml:\"DefaultDbname\"` // default mysql database name\n\t\tRowLimit      int      `yaml:\"RowLimit\"`      // limit of row numbers in a process\n\t\tSinkDb        []string `yaml:\"SinkDb\"`        // 持久化的数据库\n\t}\n\n\tF10Config struct {\n\t\tUrl                string `yaml:\"Url\"`\n\t\tTopic              string `yaml:\"Topic\"`\n\t\tTimeout            int    `yaml:\"Timeout\"`\n\t\tLimit              int    `yaml:\"Limit\"`\n\t\tFrom               string `yaml:\"From\"`\n\t\tUnderlyingAssetUrl string `yaml:\"UnderlyingAssetUrl\"` // 机构id对应标的代码取数规则\n\t}\n\n\tFUsEtfAdjFactorConfig struct {\n\t\tUrl string `yaml:\"Url\"`\n\t}\n\n\tPgConfig struct {\n\t\tQueryTimeout int `yaml:\"QueryTimeout\"` // time out of pg query\n\t}\n\n\tKafkaConfig struct {\n\t\tBrokers          string `yaml:\"brokers\"`\n\t\tUsername         string `yaml:\"username\"`\n\t\tPassword         string `yaml:\"password\"`\n\t\tConsumerGroup    string `yaml:\"consumer_group\"`\n\t\tSecurityProtocol string `yaml:\"security_protocol\"`\n\t\tSaslMechanism    string `yaml:\"sasl_mechanism\"`\n\t}\n\n\tServiceConfig struct {\n\t\tHttpPort int `yaml:\"HttpPort\"` // http port\n\t}\n\n\tLogConfig struct {\n\t\tLogPath     string `yaml:\"LogPath\"`     // log file path\n\t\tStatLogPath string `yaml:\"StatLogPath\"` // status log file path\n\t\tGinLogPath  string `yaml:\"GinLogPath\"`  // gin log file path\n\t\tLogLevel    string `yaml:\"LogLevel\"`    // log level\n\t}\n\n\tSecurityCodeChangeConfig struct {\n\t\tUrl          string                   `yaml:\"Url\"`\n\t\tTimeout      int                      `yaml:\"Timeout\"`\n\t\tMarketConfig []CodeChangeMarketConfig `yaml:\"MarketConfig\"`\n\t}\n\tCodeChangeMarketConfig struct {\n\t\tMarket   string `yaml:\"Market\"`\n\t\tSinkDb   string `yaml:\"SinkDb\"`\n\t\tCronExpr string `yaml:\"CronExpr\"`\n\t}\n\n\tConfig struct {\n\t\tMysql              MyConfig                 `yaml:\"Mysql\"`      // mysql configure\n\t\tBasicData          string                   `yaml:\"basic_data\"` // 代码表环境变量\n\t\tF10                F10Config                `yaml:\"F10\"`\n\t\tFUsEtfAdjFactor    FUsEtfAdjFactorConfig    `yaml:\"FUsEtfAdjFactor\"`\n\t\tPgsql              PgConfig                 `yaml:\"Pgsql\"` // pgsql configure\n\t\tKafka              KafkaConfig              `yaml:\"Kafka\"`\n\t\tService            ServiceConfig            `yaml:\"Service\"`            // service configure\n\t\tLog                LogConfig                `yaml:\"Log\"`                // log configure\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(confi","infill":"gPath string) ","suffix":"{\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n","relevantFile":"<file_path>app/finance_info/finance_info.go\n\terr = LoadDbInfo(orm)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, init load dbinfo\", err.Error())\n\t}\n\t// 设置热更新函数, 定时更新\n\tcallback := func() {\n\t\terr := LoadDbInfo(orm)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Warn(\"load db-info cron\", zap.String(\"error\", err.Error()))\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()","relevantFileList":[],"filePath":"app/config/config.go","template":"go","multiRes":{"hipilot":"gPath string)","CodeQwen2.5-7B":"gPath string) ","copilot":"gPath string) ","deepseek-chat-lite":"gPath string) ","CodeQwen1.5-7B":" gPath string) <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep> <file_sep>"}},{"prefix":"package main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, def","infill":"ault is /usr/local/conf/conf.yaml","suffix":"\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>app/finance_info/finance_info.go\npackage finance_info\n\nimport (\n\t\"errors\"\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\tmysql_dao \"finance_etl/app/dao/mysql\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"gorm.io/driver/mysql\"\n\t\"gorm.io/gorm\"\n\t\"gorm.io/gorm/logger\"\n\t\"gorm.io/gorm/schema\"\n)\n\ntype (\n\tFinanceInfoManager struct {\n\t\tdbName           string                                // database名称\n\t\tTableBasic       map[string]my_orm.FinanceBasic        // 财务表基础信息\n\t\tTableFieldName   map[string]my_orm.FieldInfo           // key是数据源指标名称\n\t\tMapFieldTable    map[string][]my_orm.FieldInfo         // key是表名\n\t\tMapTaskIndicator map[string]my_orm.ExtractF10Indicator // key是taskid\n\t\tMapTableTask     map[string][]string                   // key是表名，value是taskid列表\n\t}\n)\n\nconst FinanceDbEqual = \"finance_db = ?\"\n\nvar GlobalFinanceInfoManager map[string]FinanceInfoManager\n\nfunc Init() error {\n\t// 初始化连接\n\tmysqlDsn := mysql_dao.MakeDsn(config.GetMysql().Address, config.GetMysql().Params, config.GetMysql().DefaultDbname)\n\tlevellog.Log.Info(\"Init mysql connect\", zap.String(\"mysqlDsn\", mysqlDsn))\n\torm, err := gorm.Open(mysql.Open(mysqlDsn), &gorm.Config{\n\t\tNamingStrategy: schema.NamingStrategy{\n\t\t\tTablePrefix:   \"\",\n\t\t\tSingularTable: true,\n\t\t},\n\t\tLogger: logger.Default.LogMode(logger.Silent),\n\t})\n\tif err != nil {\n\t\treturn errors.New(err.Error() + \", mysqlDsn:\" + mysqlDsn)\n\t}\n\t// 初始化缓存\n\tGlobalFinanceInfoManager = make(map[string]FinanceInfoManager)\n\t// 初始化加载DbInfo\n\terr = LoadDbInfo(orm)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, init load dbinfo\", err.Error())\n\t}\n\t// 设置热更新函数, 定时更新\n\tcallback := func() {\n\t\terr := LoadDbInfo(orm)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Warn(\"load db-info cron\", zap.String(\"error\", err.Error()))\n\t\t}","relevantFileList":[],"filePath":"main.go","template":"go","multiRes":{"hipilot":"ault is conf/conf.yaml","CodeQwen2.5-7B":"ault: /usr/local/conf/conf.yaml\")\n\tflag.StringVar(&version, \"version\", \"0.0.1\", \"Version of the server, default: 0.0.1","copilot":"ault is /usr/local/conf/conf.yaml","deepseek-chat-lite":"ault is /usr/local/conf/conf.yaml","CodeQwen1.5-7B":"ault: /usr/local/conf/conf.yaml"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// print log\n\tlog.Printf(\"add task %s, schedule %s\", task, schedule)\n\n\tmetrics\n\n\tprometheus.Register(p","infill":"rometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime())","suffix":"\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/finance_info/finance_info.go\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n\t\t\tfinanceInfoManager.MapTaskIndicator[v.TaskId] = v\n\t\t\t_, ok := financeInfoManager.MapTableTask[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = make([]string, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = append(financeInfoManager.MapTableTask[v.FinanceTable], v.TaskId)\n\t\t}\n\t\tGlobalFinanceInfoManager[dbname] = financeInfoManager\n\t}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"rometheus.NewGauge(prometheus.GaugeOpts{\n        Name: \"gms_cron_task\",\n        ConstLabels: map[string]string{\n            \"task\":     task,\n            \"schedule\": schedule,\n        },\n    }","CodeQwen2.5-7B":"rometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}))\n\treturn nil\n}","copilot":"rometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime())","deepseek-chat-lite":"rometheus.NewGauge(prometheus.GaugeOpts{","CodeQwen1.5-7B":" rometheus.NewGauge(prometheus.GaugeOpts{"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// print log\n\tlog.Printf(\"add task %s, schedule %s\", task, schedule)\n\n\tmetricsTask := prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",","infill":"ConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n\n\tprometheus.Register(metricsTask)\n\tmetricsTask.SetToCurrentTime()","suffix":"\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>app/finance_info/finance_info.go\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n\t\t\tfinanceInfoManager.MapTaskIndicator[v.TaskId] = v\n\t\t\t_, ok := financeInfoManager.MapTableTask[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = make([]string, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = append(financeInfoManager.MapTableTask[v.FinanceTable], v.TaskId)\n\t\t}\n\t\tGlobalFinanceInfoManager[dbname] = financeInfoManager\n\t}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"\n        ConstLabels: map[string]string{\n            \"task\":     task,\n            \"schedule\": schedule,\n        },\n    })\n    metricsTask.Set(1)","CodeQwen2.5-7B":"\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n\tprometheus.MustRegister(metricsTask)\n\tmetricsTask.Set(1)\n\treturn nil\n}","copilot":"ConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n\n\tprometheus.Register(metricsTask)\n\tmetricsTask.SetToCurrentTime()","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// print log\n\tlog.Printf(\"add task %s, schedule %s\", task, schedule)\n\n\tmetricsTask := prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{","infill":"\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n\n\tprometheus.Register(metricsTask)\n\tmetricsTask.SetToCurrentTime()","suffix":"\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>app/finance_info/finance_info.go\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n\t\t\tfinanceInfoManager.MapTaskIndicator[v.TaskId] = v\n\t\t\t_, ok := financeInfoManager.MapTableTask[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = make([]string, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = append(financeInfoManager.MapTableTask[v.FinanceTable], v.TaskId)\n\t\t}\n\t\tGlobalFinanceInfoManager[dbname] = financeInfoManager\n\t}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"\"task\": task, \"schedule\": schedule},\n    })\n    prometheus.MustRegister(metricsTask)","CodeQwen2.5-7B":"\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n\tmetricsTask.Set(1)\n\tprometheus.MustRegister(metricsTask)\n\treturn nil\n}","copilot":"\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n\n\tprometheus.Register(metricsTask)\n\tmetricsTask.SetToCurrentTime()","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// print log\n\tlog.Printf(\"add task %s, schedule %s\", task, schedule)\n\n\tmetricsTask := prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t}","infill":",\n\t})\n\n\tprometheus.Register(metricsTask)\n\tmetricsTask.SetToCurrentTime()","suffix":"\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>app/finance_info/finance_info.go\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n\t\t\tfinanceInfoManager.MapTaskIndicator[v.TaskId] = v\n\t\t\t_, ok := financeInfoManager.MapTableTask[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = make([]string, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = append(financeInfoManager.MapTableTask[v.FinanceTable], v.TaskId)\n\t\t}\n\t\tGlobalFinanceInfoManager[dbname] = financeInfoManager\n\t}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":",\n    })\n    metricsTask.Set(1)","CodeQwen2.5-7B":",\n\t})\n\tprometheus.MustRegister(metricsTask)\n\tmetricsTask.Set(1)\n\treturn nil\n}","copilot":",\n\t})\n\n\tprometheus.Register(metricsTask)\n\tmetricsTask.SetToCurrentTime()","deepseek-chat-lite":",","CodeQwen1.5-7B":" ,"}},{"prefix":"package main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVe","infill":"rsion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}","suffix":"\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit() {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", \"/usr/local/conf/conf.yaml\", \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)","relevantFileList":[],"filePath":"main.go","template":"go","multiRes":{"hipilot":"cVersion() prometheus.Collector {\n    return promauto.NewGauge(prometheus.GaugeOpts{\n        Name:        \"finance_etl_version\",\n        Help:        \"finance_etl version\",\n        ConstLabels: promethe","CodeQwen2.5-7B":"rsion = promauto.NewGauge(prometheus.GaugeOpts{\n\tName: \"finance_etl_version\",\n\tHelp: \"finance etl version\",\n})","copilot":"rsion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}","deepseek-chat-lite":"rsion() string {","CodeQwen1.5-7B":" rsion = promauto.NewGauge(prometheus.GaugeOpts{"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(","infill":"cronTaskMetrics","suffix":")\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\n\t\tprometheus.\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/extract/automatic/update_kafka.go\npackage automatic\n\nimport (\n\t\"context\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype KafkaConsumer struct {\n\tConfig        kafka_dao.Config\n\tTask          KafkaTaskInfo\n\tconsumer      kafka_dao.Consumer\n\tconsumerGroup sarama.ConsumerGroup\n\tctx           context.Context\n\tcancel        context.CancelFunc\n\tchErr         chan error\n}\n\nfunc (d KafkaConsumer) Start() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\tlevellog.Log.Info(\"No need to start kafka consume\")\n\t\treturn 0, nil\n\t}\n\tlevellog.Log.Info(\"start init kafka consumer\", zap.Strings(\"brokers\", d.Config.Brokers),\n\t\tzap.String(\"username\", d.Config.Username), zap.String(\"password\", d.Config.Password),\n\t\tzap.String(\"consumer_group\", d.Config.ConsumerGroup), zap.String(\"SecurityProtocol\", d.Config.SecurityProtocol),\n\t\tzap.String(\"SaslMechanism\", d.Config.SaslMechanism), zap.String(\"topic\", d.Task.GetTopic()))\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n<file_path>app/metrics/metrics.go\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// qps\n\t// trigger: cron kafka\n\t// source: pg, f10\n\tqpsCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_qps\",\n\t\t\tHelp: \"request received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// 处理耗时分布\n\t// trigger: cron manual kafka\n\t// stage: extract proccess load\n\t// export: all bbrq rtime real code\n\t// source: pg, f10\n\tcostReqBucketVec = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"gms_etl_cost_histogram\",\n\t\t\tHelp:    \"cost of time for each request.\",\n\t\t\tBuckets: []float64{100, 1000, 5000, 10000, 50000, 120000, 600000},\n\t\t},\n\t\t[]string{\"db\", \"table\", \"stage\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// label_name 面板显示时公有显示\n\tnameCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_label_name\",\n\t\t\tHelp: \"show label name\",\n\t\t},\n\t\t[]string{\"db\", \"table\"},\n\t)\n\n\t// 错误数 记录接口错误,内部错误等\n\terrorCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_error\",\n\t\t\tHelp: \"errors\",\n\t\t},\n\t\t[]string{\"status\"},\n\t)\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"cronTaskMetrics","CodeQwen2.5-7B":"cronTaskMetrics<|file_sep|><|fim_prefix|>/README.md\npackage cron","copilot":"cronTaskMetrics","deepseek-chat-lite":"cronTaskMetrics","CodeQwen1.5-7B":" cronTaskMetrics"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \\","infill":"such as package, data flow, etc.\n\tkafkaConsumerMetrics = prometheus.NewGaugeVec(\n\t\t\n","suffix":"\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n","relevantFile":"<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n<file_path>app/cron/cron.go\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n<file_path>app/extract_manager/extract_manager.go\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"\"real\",  //实时","CodeQwen2.5-7B":"\"real\\\", //实时导出\n}","copilot":"such as package, data flow, etc.\n\tkafkaConsumerMetrics = prometheus.NewGaugeVec(\n\t\t\n","deepseek-chat-lite":"","CodeQwen1.5-7B":" \"real\\\", //实时导出"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的","infill":")\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaSkipTask","suffix":"\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"差异","CodeQwen2.5-7B":"指标\nvar (\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"rx_traffic\",\n\t\t\tHelp: \"流量指标\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)","copilot":")\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaSkipTask","deepseek-chat-lite":"区别","CodeQwen1.5-7B":" 触发方式"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出","infill":".Itoa(int(partition)), taskId).Inc()","suffix":"\n}\n","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/cron/cron.go\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/extract/manual/request_f10.go\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n\tto := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.EndTime/10000, d.ReqParam.EndTime%10000/100, d.ReqParam.EndTime%100)\n\trequestBody := my_orm.F10Request{\n\t\tCurrency: financeTable.Currency,\n\t\tCategory: \"indicator\",\n\t\tStrategy: d.ReqParam.Strategy,\n\t\tFrom:     from,\n\t\tTo:       to,\n\t}\n\tmarket := financeTable.Market\n\tcodes, err := GetCodelist(d.FinanceDb, d.FinanceTable)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"req codelist failed\", zap.Error(err))\n\t\treturn 0, err\n\t}\n\trecordTotal := 0\n\t// 由于存在全量请求的情况，数据量较大，所以每次只请求一支代码\n\tfor _, code := range codes {\n\t\tlevellog.Log.Debug(\"code info\", zap.Int(\"submarket\", code.Submarket), zap.String(\"code\", code.Code))\n\t\tif strings.Contains(code.Code, \"_\") { // 过滤掉含有特殊符号的代码\n\t\t\tcontinue\n\t\t}\n\t\tsubjects := fmt.Sprintf(\"%s-%s\", market, code.Code)\n\t\trequestBody.Subjects = subjects\n\t\tfor _, taskId := range tableF10Task {\n\t\t\trequestBody.Ids = dbInfo.MapTaskIndicator[taskId].Ids\n\t\t\tval, _ := query.Values(requestBody)\n\t\t\treq.URL.RawQuery = val.Encode()\n\t\t\tresp, err := httpGetWithRetry(client, req, 3)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Warn(\"manual request f10 data, request failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\trecordCnt, err := d.ProcessF10RequestData(resp, market, requestBody.Ids, dbInfo.MapTaskIndicator[taskId].HadReport, dbInfo.MapTaskIndicator[taskId].DateType)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Info(\"manual request F10 data, process failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trecordTotal = recordTotal + recordCnt\n\t\t}\n\t}\n\tmetrics.TrafficMetricsAdd(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10, recordTotal)\n\t// f10持久化耗时记录\n\tmetrics.CostBucketMetricsObserve(d.FinanceDb, d.FinanceTable, metrics.StageAll, triggerType, exportType, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\tlevellog.Log.Info(\"f10 sinkdata affected:\", zap.String(\"db\", d.FinanceDb), zap.String(\"table\", d.FinanceTable), zap.Int(\"rows:\", recordTotal))\n\treturn recordTotal, nil\n}\n\nfunc (d DataRequestF10) ProcessF10RequestData(resp *http.Response, market string, ids string, hadReport int, date string) (int, error) {","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n}","copilot":".Itoa(int(partition)), taskId).Inc()","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\\","infill":"\n\nfun","suffix":"\n\nfunc (d *CronMetrics) AddTask(Name string, schedule string) {\n\t_, ok := d.TaskExecAt[Name]\n\tif !ok {\n\t\td.TaskExecAt[Name] = make(map[string]prometheus.Gauge)\n\t}\n\td.TaskExecAt[Name][schedule] = prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"cron_task_exec_at\",\n\t\tHelp: \"cron task exec at\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     Name,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n}\n","relevantFile":"<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"/     d.cron.Remove(d.scheduleDetail[task])","CodeQwen2.5-7B":"\nfunc (d *CronManager) RemoveTaskSchedule(task string, schedule string) {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif !ok {\n\t\treturn\n\t}\n\td.cron.Remove(d.scheduleDetail[task][schedule])\n\tdelete(d.scheduleDetail[task], schedule)\n}","copilot":"\n\nfun","deepseek-chat-lite":"","CodeQwen1.5-7B":"}"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\\","infill":"_, ok := d.TaskExecAt[Name]\n\tif !ok {\n\t\td.TaskExecAt[Name] = make(map[string]prometheus.Gauge)\n\t}\n\td.TaskExecAt[Name][schedule] = prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"cron_task_exec_at\",\n\t\tHelp: \"cron task exec at\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     Name,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})","suffix":"\n}\n","relevantFile":"<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"/    for _, id := range d.scheduleDetail[task] {","CodeQwen2.5-7B":"\n}","copilot":"_, ok := d.TaskExecAt[Name]\n\tif !ok {\n\t\td.TaskExecAt[Name] = make(map[string]prometheus.Gauge)\n\t}\n\td.TaskExecAt[Name][schedule] = prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"cron_task_exec_at\",\n\t\tHelp: \"cron task exec at\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     Name,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})","deepseek-chat-lite":"","CodeQwen1.5-7B":"\\"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\\","infill":"func (d *CronMetrics) SetTaskExecAt(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].SetToCurrentTime()\n}\n\nfunc (d *CronMetrics) RemoveTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].Delete()\n\tdelete(d.TaskExecAt[Name], schedule)\n}","suffix":"\n","relevantFile":"<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n}","copilot":"func (d *CronMetrics) SetTaskExecAt(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].SetToCurrentTime()\n}\n\nfunc (d *CronMetrics) RemoveTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].Delete()\n\tdelete(d.TaskExecAt[Name], schedule)\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":"\\"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\\","infill":"d.TaskExecAt[Name][schedule].SetToCurrentTime()","suffix":"\n}\n\nfunc (d *CronMetrics) RemoveTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].Delete()\n\tdelete(d.TaskExecAt[Name], schedule)\n}\n","relevantFile":"<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"/    for _, v := range d.scheduleDetail[task] {","CodeQwen2.5-7B":"\n}","copilot":"d.TaskExecAt[Name][schedule].SetToCurrentTime()","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.sch","infill":"\n\tdelete(d.TaskExecAt[Name], schedule)\n}","suffix":"\n","relevantFile":"<file_path>app/server/http.go\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"eduleDetail[task][schedule] = id","CodeQwen2.5-7B":"eduleDetail[task][schedule] = id\n\treturn nil\n}","copilot":"\n\tdelete(d.TaskExecAt[Name], schedule)\n}","deepseek-chat-lite":"eduleDetail[task][schedule] = id","CodeQwen1.5-7B":" eduleDetail[task][schedule] = id"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.sch","infill":"\n\tdelet","suffix":"e(d.TaskExecAt[Name], schedule)\n}\n","relevantFile":"<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/server/http.go\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"eduleDetail[task][schedule] = id\n    return nil\n}\n\n// 移除任务\nfunc (d *CronManager) RemoveTask(task string, schedule string) {\n    d.cron.Remove(d.scheduleDetail[task][schedule])\n","CodeQwen2.5-7B":"eduleDetail[task][schedule] = id\n\treturn nil\n}","copilot":"\n\tdelet","deepseek-chat-lite":"eduleDetail[task][schedule] = id","CodeQwen1.5-7B":" eduleDetail[task][schedule] = id"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.sch","infill":"[Name][schedule].Set(0)","suffix":"\n\tdelete(d.TaskExecAt[Name], schedule)\n}\n","relevantFile":"<file_path>app/server/http.go\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"eduleDetail[task][schedule] = id","CodeQwen2.5-7B":"eduleDetail[task][schedule] = id\n\treturn nil\n}","copilot":"[Name][schedule].Set(0)","deepseek-chat-lite":"eduleDetail[task][schedule] = id","CodeQwen1.5-7B":" eduleDetail[task][schedule] = id"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != ","infill":"Name: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\": task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).\n\t","suffix":"\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"nil {\n        return err\n    }\n    d.scheduleDetail[task][schedule] = id\n","CodeQwen2.5-7B":"nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}","copilot":"Name: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\": task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).\n\t","deepseek-chat-lite":"nil {","CodeQwen1.5-7B":" nil {"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != ","infill":"ConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime()\n","suffix":"\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"nil {\n        return err\n    }\n    d.scheduleDetail[task][schedule] = id\n","CodeQwen2.5-7B":"nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}","copilot":"ConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime()\n","deepseek-chat-lite":"nil {","CodeQwen1.5-7B":" nil {"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != ","infill":"\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime()\n","suffix":"\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"nil {\n        return err\n    }\n    d.scheduleDetail[task][schedule] = id\n","CodeQwen2.5-7B":"nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}","copilot":"\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime()\n","deepseek-chat-lite":"nil {","CodeQwen1.5-7B":" nil {"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != ","infill":"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime()\n","suffix":"\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n\n\tprometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"nil {\n        return err\n    }\n    d.scheduleDetail[task][schedule] = id\n","CodeQwen2.5-7B":"nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}","copilot":"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime()\n","deepseek-chat-lite":"nil {","CodeQwen1.5-7B":"nil {"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != ","infill":",\n\t}).SetToCurrentTime()\n","suffix":"\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n\n\tprometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\t\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"nil {\n        return err\n    }\n    d.scheduleDetail[task][schedule] = id\n","CodeQwen2.5-7B":"nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}","copilot":",\n\t}).SetToCurrentTime()\n","deepseek-chat-lite":"nil {","CodeQwen1.5-7B":"nil {"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/common/log\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// print log\n\tl","infill":"og.Infof(\"add task %s, schedule %s\", task, schedule)","suffix":"\n\tprometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>app/dao/mysql/db.go\npackage mysql_dao\n\nimport (\n\t\"database/sql\"\n)\n\n/*newDbConn\n * @Description: 创建新的mysql连接\n * @param dsn\n * @return db\n * @return err\n */\nfunc NewDbConn(dsn string) (db *sql.DB, err error) {\n\tdb, err = sql.Open(\"mysql\", dsn)\n\treturn\n}\n\nfunc MakeDsn(address string, params string, dbName string) string {\n\treturn address + dbName + \"?\" + params\n}\n<file_path>app/finance_info/finance_info.go\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n\t\t\tfinanceInfoManager.MapTaskIndicator[v.TaskId] = v\n\t\t\t_, ok := financeInfoManager.MapTableTask[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = make([]string, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = append(financeInfoManager.MapTableTask[v.FinanceTable], v.TaskId)\n\t\t}\n\t\tGlobalFinanceInfoManager[dbname] = financeInfoManager\n\t}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"log.Infof(\"cron task [%s] add success, next schedule time: %s\", task, schedule)","CodeQwen2.5-7B":"og.Infof(\"add task %s, schedule %s, id %d\", task, schedule, id)<|file_sep|><|fim_prefix|>/README.md\npackage cron","copilot":"og.Infof(\"add task %s, schedule %s\", task, schedule)","deepseek-chat-lite":"og.Infof(\"add cron task: %s, schedule: %s\", task, schedule)","CodeQwen1.5-7B":" og.Infof(\"add task %s, schedule %s\", task, schedule)"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// print log\n\tlog.Printf(\"add task %s, schedule %s\", task, schedule)\n\n\tmetricsTask(task, schedule)\n\n\tprometheus.Register(prometheus.NewGauge(prometheus.GaugeOpts{","infill":"Name: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime())","suffix":"\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>app/finance_info/finance_info.go\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n\t\t\tfinanceInfoManager.MapTaskIndicator[v.TaskId] = v\n\t\t\t_, ok := financeInfoManager.MapTableTask[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = make([]string, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = append(financeInfoManager.MapTableTask[v.FinanceTable], v.TaskId)\n\t\t}\n\t\tGlobalFinanceInfoManager[dbname] = financeInfoManager\n\t}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"\n        Name: \"gms_cron_task\",\n        ConstLabels: map[string]string{\n            \"task\":     task,\n            \"schedule\": schedule,\n        },\n    }))","CodeQwen2.5-7B":"\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}))\n\treturn nil\n}","copilot":"Name: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime())","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// print log\n\tlog.Printf(\"add task %s, schedule %s\", task, schedule)\n\n\tmetricsTask := \n\n\tprometheus.Register(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{","infill":"\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime())","suffix":"\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>app/finance_info/finance_info.go\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n\t\t\tfinanceInfoManager.MapTaskIndicator[v.TaskId] = v\n\t\t\t_, ok := financeInfoManager.MapTableTask[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = make([]string, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = append(financeInfoManager.MapTableTask[v.FinanceTable], v.TaskId)\n\t\t}\n\t\tGlobalFinanceInfoManager[dbname] = financeInfoManager\n\t}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"\"task\": task, \"schedule\": schedule},\n    }))","CodeQwen2.5-7B":"\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}))\n\treturn nil\n}","copilot":"\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime())","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// print log\n\tlog.Printf(\"add task %s, schedule %s\", task, schedule)\n\n\tmetricsTask := prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\n\n\tprometheus.Register(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,","infill":"\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime())","suffix":"\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>app/finance_info/finance_info.go\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n\t\t\tfinanceInfoManager.MapTaskIndicator[v.TaskId] = v\n\t\t\t_, ok := financeInfoManager.MapTableTask[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = make([]string, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = append(financeInfoManager.MapTableTask[v.FinanceTable], v.TaskId)\n\t\t}\n\t\tGlobalFinanceInfoManager[dbname] = financeInfoManager\n\t}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"schedule,","CodeQwen2.5-7B":"\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}))\n\tmetricsTask.Set(1)\n\treturn nil\n}","copilot":"\"schedule\": schedule,\n\t\t},\n\t}).SetToCurrentTime())","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// print log\n\tlog.Printf(\"add task %s, schedule %s\", task, schedule)\n\n\tmetricsTask := prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\n\n\tprometheus.Register(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t}","infill":",\n\t}).SetToCurrentTime())","suffix":"\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>app/finance_info/finance_info.go\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n\t\t\tfinanceInfoManager.MapTaskIndicator[v.TaskId] = v\n\t\t\t_, ok := financeInfoManager.MapTableTask[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = make([]string, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = append(financeInfoManager.MapTableTask[v.FinanceTable], v.TaskId)\n\t\t}\n\t\tGlobalFinanceInfoManager[dbname] = financeInfoManager\n\t}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":",","CodeQwen2.5-7B":",\n\t}))\n\treturn nil\n}","copilot":",\n\t}).SetToCurrentTime())","deepseek-chat-lite":",","CodeQwen1.5-7B":","}},{"prefix":"package main\n\nimport (\n\t\"fmt\"\n\t\"net/http\"\n\n\t\"github.com/gin-gonic/gin\"\n\tgin \"github.com/gin-gonic/gin\"\n)\n\n//","infill":"\n)\n\n//","suffix":"\n)\n\n// 定义处理器函数\nfunc handler(w http.ResponseWriter, r *http.Request) {\n\tfmt.Fprintf(w, \"Hello, you've requested: %s\\n\", r.URL.Path)\n}\n\nfunc main() {\n\t// 将处理器函数绑定到路由\n\thttp.HandleFunc(\"/\", handler)\n\n\t// 启动服务器\n\thttp.ListenAndServe(\":8080\", nil)\n\n\tr := gin.Default()\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(200, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t}\n\n\tr.\n}\n","relevantFile":"","relevantFileList":[],"filePath":"main.go","template":"go","multiRes":{"hipilot":" 定义路由","CodeQwen2.5-7B":" 定义处理器函数\nfunc handler(c *gin.Context) {\n\tc.JSON(200, gin.H{\n\t\t\"message\": \"Hello, World!\",\n\t})\n}","copilot":"\n)\n\n//","deepseek-chat-lite":" 定义处理器函数","CodeQwen1.5-7B":" 引入net/http包"}},{"prefix":"package main\n\nimport (\n\t\"fmt\"\n\t\"net/http\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\n// 定义处理器函数\nfunc handler(w http.ResponseWriter, r *http.Request) {\n\tfmt.Fprintf(w, \"Hello, you've requested: %s\\n\", r.URL.Path)\n}\n\nfunc main() {\n\t// // 将处理器函数绑定到路由\n\t// http.HandleFunc(\"/\", handler)\n\n\t// // 启动服务器\n\t// http.ListenAndServe(\":8080\", nil)\n\n\tr := gin.Default()\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(200, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.","infill":"Run() // listen and serve on","suffix":"\n}\n","relevantFile":"","relevantFileList":[],"filePath":"main.go","template":"go","multiRes":{"hipilot":"Run() // 监听并在 0.0.0.0:8080 上启动服务","CodeQwen2.5-7B":"Run(\":8080\")","copilot":"Run() // listen and serve on","deepseek-chat-lite":"Run() // listen and serve on 0.0.0.0:8080","CodeQwen1.5-7B":"Run() // listen and serve on 0.0.0.0:8080 (for windows \"localhost:8080\")"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects)\n\treturn\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期","infill":"查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的","suffix":"\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\tdateStr, ok := jsonF10Rsp.Data.Data[0].Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 {\n\t\td.ProcessInvalidData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t// 如果 validInt=1, 则更新相应的指标数据\n\t// 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tif taskInfo.Currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(ids, subjects, taskInfo.Currency)\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tsearchSql := fmt.Sprintf(\"SELECT isvalid FROM `%s` WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\trow, _ := load.FetchOneRowData(taskInfo.DbName, searchSql)\n\tvar isvalidField int // mysql数据库中isvalid字段\n\terr := row.Scan(&isvalidField)\n\tif err == nil {\n\t\tloadSql := fmt.Sprintf(\"INSERT INTO `%s` (zqdm,unix_time) VALUES (%s, %d) ON DUPLICATE KEY UPDATE isvalid = 0\", taskInfo.TableName, codeStr, unixTime)\n\t\t_, err := load.SinkData(taskInfo.DbName, loadSql)\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"search isvalid in local mysql, searchSql:%s, loadSql:%s, err:%s, msg:%s\", searchSql, loadSql, err.Error())\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t\t}\n\t}\n\treturn\n}\n\n// kafka 推送数据为原生数据，这里再请求一次指标数据，\nfunc (d F10KafkaTaskInfo) RequestF10Data(ids string, subjects string, currency string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      ids,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"查询，日期查询结果为空时，不处理","CodeQwen2.5-7B":"查询一次数据\n// 3. 如果是新增操作，直接插入数据\n// 4. 如果是删除操作，将数据库中这条数据的isvalid字段置为0\n// 5. 如果是更新操作，将数据库中这条数据的isvalid字段置为1\n// 6. 如果是更新操作，将数据库中这条数据的isvalid字段置为1\n// 7. 如果是更新操作，将数据库中这条数据的isvalid字段置为1\n// 8. 如果是更新操作，将数据库中这条数据的isvalid字段置为1\n// 9. 如果是更新操作，将数据库中这条数据的isvalid字段置为1\n// 10. 如果是更新操作，将数据库中这条数据的isvalid字段置为1\n// 11. 如果是更新操作，将数据库中这条数据的isvalid字段置为1\n// 12. 如果是更新操作，将数据库中这条数据的isvalid字段置为1\n// 13. 如果是更新操作，将数据库中这条数据的isvalid字段置为1\n// 14. 如果是更新操作，将数据库中这条数据的isvalid字段置为1\n// 15. 如果是更新操作，将数据库中这条数据的isvalid字段置为1\n// 16. 如果是更新操作，将数据库中","copilot":"查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的","deepseek-chat-lite":"查询一次","CodeQwen1.5-7B":" 筛选出对应的数据"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects)\n\treturn\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次","infill":"查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新e","suffix":"\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\tdateStr, ok := jsonF10Rsp.Data.Data[0].Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 {\n\t\td.ProcessInvalidData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t// 如果 validInt=1, 则更新相应的指标数据\n\t// 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tif taskInfo.Currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(ids, subjects, taskInfo.Currency)\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tsearchSql := fmt.Sprintf(\"SELECT isvalid FROM `%s` WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\trow, _ := load.FetchOneRowData(taskInfo.DbName, searchSql)\n\tvar isvalidField int // mysql数据库中isvalid字段\n\terr := row.Scan(&isvalidField)\n\tif err == nil {\n\t\tloadSql := fmt.Sprintf(\"INSERT INTO `%s` (zqdm,unix_time) VALUES (%s, %d) ON DUPLICATE KEY UPDATE isvalid = 0\", taskInfo.TableName, codeStr, unixTime)\n\t\t_, err := load.SinkData(taskInfo.DbName, loadSql)\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"search isvalid in local mysql, searchSql:%s, loadSql:%s, err:%s, msg:%s\", searchSql, loadSql, err.Error())\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t\t}\n\t}\n\treturn\n}\n\n// kafka 推送数据为原生数据，这里再请求一次指标数据，\nfunc (d F10KafkaTaskInfo) RequestF10Data(ids string, subjects string, currency string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      ids,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"查询的日期和kafka推送的日期不一致，则先删除第一次查询的数据，再插入kafka推送的消息","CodeQwen2.5-7B":"查询的日期和kafka推送的日期不一致，则需要先查询一次数据，再查询一次数据，然后合并两次查询的结果\n// 5. 如果第二次查询的日期和kafka推送的日期一致，则直接使用第二次查询的数据更新数据库\n// 6. 如果第二次查询的日期和kafka推送的日期不一致，则需要先查询一次数据，再查询一次数据，然后合并两次查询的结果\n// 7. 如果第二次查询的日期和kafka推送的日期不一致，则需要先查询一次数据，再查询一次数据，然后合并两次查询的结果\n// 8. 如果第二次查询的日期和kafka推送的日期不一致，则需要先查询一次数据，再查询一次数据，然后合并两次查询的结果\n// 9. 如果第二次查询的日期和kafka推送的日期不一致，则需要先查询一次数据，再查询一次数据，然后合并两次查询的结果\n// 10. 如果第二次查询的日期和kafka推送的日期不一致，则需要先查询一次数据，再查询一次数据，然后合并两次查询的结果\n// 11. 如果第二次查询的日期和kafka推送的日期不一致，则需要先查询一次数据，再查询一次数据，然后合并两次查询的结果\n// 12. 如果第二次查询的日期和kafka推送的日期不一致，则需要先查询一次数据，再查询一次数据，然后合并两次查询","copilot":"查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新e","deepseek-chat-lite":"查询的日期和kafka推送的日期不一致，则需要先按最新状态查询一次数据，再按日期查询一次数据，然后更新数据库","CodeQwen1.5-7B":" 查询的日期和kafka推送的日期不一致，则先查询一次日期，再更新数据库"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]","infill":"yaml:\"name\" json:\"name\" binding:\"required\"`","suffix":"\n\tSchedule string `yaml:\"schedule\" json:\"schedule\" binding:\"required\"`\n}\n\ntype CronMetrics struct {\n\t\n}\n","relevantFile":"<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"; if !ok {\n        return\n    }\n    for _, id := range taskDetail {\n        d.cron.Remove(id)\n    }","CodeQwen2.5-7B":"\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, id := range taskDetail {\n\t\td.cron.Remove(id)\n\t}\n\tdelete(d.scheduleDetail, task)\n}","copilot":"yaml:\"name\" json:\"name\" binding:\"required\"`","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\\","infill":"`yaml:\"schedule\" json:\"schedule\" binding:\"required\"`","suffix":"\n}\n\ntype CronMetrics struct {\n\t\n}\n","relevantFile":"<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"return","CodeQwen2.5-7B":"\n\t\treturn\n\t}\n\tfor _, id := range taskDetail {\n\t\td.cron.Remove(id)\n\t}\n\tdelete(d.scheduleDetail, task)","copilot":"`yaml:\"schedule\" json:\"schedule\" binding:\"required\"`","deepseek-chat-lite":"","CodeQwen1.5-7B":" \\"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\\","infill":"(d *CronMetrics) AddTask(Name string , schedule string) {\n\t\n}\n","suffix":"","relevantFile":"<file_path>app/config/config.go\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n\nfunc GetSecurityCodeChangeConfig() SecurityCodeChangeConfig {\n\treturn cfg.SecurityCodeChange\n}\n\nfunc GetCodeChangeMarketConfigMap() map[string]CodeChangeMarketConfig {\n\tconfigMap := map[string]CodeChangeMarketConfig{}\n\tcfg := GetSecurityCodeChangeConfig()\n\tfor _, v := range cfg.MarketConfig {\n\t\tconfigMap[v.Market] = v\n\t}\n\treturn configMap\n}\n<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"return}\n    for _, id := range taskDetail {\n        d.cron.Remove(id)\n    }\n}\n\nfunc init() {\n    GCronManager.Start()\n}\n","CodeQwen2.5-7B":"\n\t\treturn\n\t}\n\tfor _, id := range taskDetail {\n\t\td.cron.Remove(id)\n\t}\n\tdelete(d.scheduleDetail, task)\n}","copilot":"(d *CronMetrics) AddTask(Name string , schedule string) {\n\t\n}\n","deepseek-chat-lite":"","CodeQwen1.5-7B":" r\\n"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\ntype CronFuncWithMetrics struct {\n\tCronFunc func()\n\tN","infill":"ame     string\n\tSchedule string","suffix":"\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, CronFuncWithMetrics{\n\t\tCronFunc: cronFunc,\n\t\tName:     task,\n\t\t\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n\ntype CronMetrics struct {\n\tTaskExecAt map[string]map[string]prometheus.Gauge\n}\n\nfunc (d *CronMetrics) AddTask(Name string, schedule string) {\n\t_, ok := d.TaskExecAt[Name]\n\tif !ok {\n\t\td.TaskExecAt[Name] = make(map[string]prometheus.Gauge)\n\t}\n\td.TaskExecAt[Name][schedule] = prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"cron_task_exec_at\",\n\t\tHelp: \"cron task exec at\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     Name,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n}\n\nfunc (d *CronMetrics) SetTaskExecAt(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].SetToCurrentTime()\n}\n\nfunc (d *CronMetrics) RemoveTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].Set(0)\n\tdelete(d.TaskExecAt[Name], schedule)\n}\n","relevantFile":"<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n<file_path>cron.go\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\tc.remove <- id\n\t} else {\n\t\tc.removeEntry(id)\n\t}\n}\n\n// Start the cron scheduler in its own goroutine, or no-op if already started.\nfunc (c *Cron) Start() {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treturn\n\t}\n\tc.running = true\n\tgo c.run()\n}\n\n// Run the cron scheduler, or no-op if already running.\nfunc (c *Cron) Run() {\n\tc.runningMu.Lock()\n\tif c.running {\n\t\tc.runningMu.Unlock()\n\t\treturn\n\t}\n\tc.running = true\n\tc.runningMu.Unlock()\n\tc.run()\n}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"ame     string","CodeQwen2.5-7B":"ame     string\n}","copilot":"ame     string\n\tSchedule string","deepseek-chat-lite":"ame     string","CodeQwen1.5-7B":" ame     string"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\ntype CronFuncWithMetrics struct {\n\tCronFunc func()\n\tName     string","infill":"Schedule string","suffix":"\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, CronFuncWithMetrics{\n\t\tCronFunc: cronFunc,\n\t\tName:     task,\n\t\tSchedule: schedule,\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n\ntype CronMetrics struct {\n\tTaskExecAt map[string]map[string]prometheus.Gauge\n}\n\nfunc (d *CronMetrics) AddTask(Name string, schedule string) {\n\t_, ok := d.TaskExecAt[Name]\n\tif !ok {\n\t\td.TaskExecAt[Name] = make(map[string]prometheus.Gauge)\n\t}\n\td.TaskExecAt[Name][schedule] = prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"cron_task_exec_at\",\n\t\tHelp: \"cron task exec at\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     Name,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n}\n\nfunc (d *CronMetrics) SetTaskExecAt(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].SetToCurrentTime()\n}\n\nfunc (d *CronMetrics) RemoveTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].Set(0)\n\tdelete(d.TaskExecAt[Name], schedule)\n}\n","relevantFile":"<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n<file_path>cron.go\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\tc.remove <- id\n\t} else {\n\t\tc.removeEntry(id)\n\t}\n}\n\n// Start the cron scheduler in its own goroutine, or no-op if already started.\nfunc (c *Cron) Start() {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treturn\n\t}\n\tc.running = true\n\tgo c.run()\n}\n\n// Run the cron scheduler, or no-op if already running.\nfunc (c *Cron) Run() {\n\tc.runningMu.Lock()\n\tif c.running {\n\t\tc.runningMu.Unlock()\n\t\treturn\n\t}\n\tc.running = true\n\tc.runningMu.Unlock()\n\tc.run()\n}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"// 任务名\n    Schedule string    // 任务调度","CodeQwen2.5-7B":"\n\tSchedule string\n}","copilot":"Schedule string","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\ntype CronFuncWithMetrics struct {\n\tCronFunc func()\n\tName     string\n\tSchedule string\n}\n\nfunc (d CronFuncWithMetrics) CronFunc() {\n\tGCronMetrics.SetTaskExecAt(d.Name, d.Schedule)\n\n\t\t\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif","infill":"{\n\t\tCronFunc: cronFunc,\n\t\tName:     task,\n\t\tSchedule: schedule,\n\t}.CronFunc)","suffix":"\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n\ntype CronMetrics struct {\n\tTaskExecAt map[string]map[string]prometheus.Gauge\n}\n\nfunc (d *CronMetrics) AddTask(Name string, schedule string) {\n\t_, ok := d.TaskExecAt[Name]\n\tif !ok {\n\t\td.TaskExecAt[Name] = make(map[string]prometheus.Gauge)\n\t}\n\td.TaskExecAt[Name][schedule] = prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"cron_task_exec_at\",\n\t\tHelp: \"cron task exec at\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     Name,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n}\n\nfunc (d *CronMetrics) SetTaskExecAt(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].SetToCurrentTime()\n}\n\nfunc (d *CronMetrics) RemoveTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].Set(0)\n\tdelete(d.TaskExecAt[Name], schedule)\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"ok {\n        d.RemoveTask(task)\n    }\n    _, ok = d.scheduleDetail[task]\n    if !ok {\n        d.scheduleDetail[task] = make(CronEntryInfo)\n    }\n    id, err := d.c","CodeQwen2.5-7B":" ok {\n\t\treturn nil\n\t}\n\td.scheduleDetail[task] = make(CronEntryInfo)\n\tid, err := d.cron.AddFunc(schedule, cronFunc)","copilot":"{\n\t\tCronFunc: cronFunc,\n\t\tName:     task,\n\t\tSchedule: schedule,\n\t}.CronFunc)","deepseek-chat-lite":" !ok {","CodeQwen1.5-7B":"  ok {"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\ntype CronFuncWithMetrics struct {\n\tFunc     func()\n\tName     string\n\tSchedule string\n\t\n}\n\nfunc (d *CronFuncWithMetrics) Func() {","infill":"CronMetrics.SetTaskExecAt(d.Name, d.Schedule)\n\n\td.CronFunc()\n}","suffix":"\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, CronFuncWithMetrics{\n\t\tCronFunc: cronFunc,\n\t\tName:     task,\n\t\tSchedule: schedule,\n\t}.CronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n\ntype CronMetrics struct {\n\tTaskExecAt map[string]map[string]prometheus.Gauge\n}\n\nfunc (d *CronMetrics) AddTask(Name string, schedule string) {\n\t_, ok := d.TaskExecAt[Name]\n\tif !ok {\n\t\td.TaskExecAt[Name] = make(map[string]prometheus.Gauge)\n\t}\n\td.TaskExecAt[Name][schedule] = prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"cron_task_exec_at\",\n\t\tHelp: \"cron task exec at\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     Name,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n}\n\nfunc (d *CronMetrics) SetTaskExecAt(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].SetToCurrentTime()\n}\n\nfunc (d *CronMetrics) RemoveTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].Set(0)\n\tdelete(d.TaskExecAt[Name], schedule)\n}\n","relevantFile":"<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n<file_path>cron.go\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\tc.remove <- id\n\t} else {\n\t\tc.removeEntry(id)\n\t}\n}\n\n// Start the cron scheduler in its own goroutine, or no-op if already started.\nfunc (c *Cron) Start() {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"\n    defer func() {\n        if err := recover(); err != nil {\n            \n        }\n    }()\n    d.Func()\n}","CodeQwen2.5-7B":"\n\td.Func()\n\tGCronManager.SetTaskExecAt(d.Name, d.Schedule)\n}","copilot":"CronMetrics.SetTaskExecAt(d.Name, d.Schedule)\n\n\td.CronFunc()\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\ntype CronFuncWithMetrics struct {\n\tFunc     func()\n\tName     string\n\tSchedule string\n}\n\nfunc (d *CronFuncWithMetrics) CronFunc() {\n\tCronMetrics.SetTaskExecAt(d.Name, d.Schedule)","infill":"d.CronFunc()\n}","suffix":"\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, CronFuncWithMetrics{\n\t\tCronFunc: cronFunc,\n\t\tName:     task,\n\t\tSchedule: schedule,\n\t}.CronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor _, v := range taskDetail {\n\t\td.cron.Remove(v)\n\t}\n\tdelete(d.scheduleDetail, task)\n}\n\ntype CronMetrics struct {\n\tTaskExecAt map[string]map[string]prometheus.Gauge\n}\n\nfunc (d *CronMetrics) AddTask(Name string, schedule string) {\n\t_, ok := d.TaskExecAt[Name]\n\tif !ok {\n\t\td.TaskExecAt[Name] = make(map[string]prometheus.Gauge)\n\t}\n\td.TaskExecAt[Name][schedule] = prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"cron_task_exec_at\",\n\t\tHelp: \"cron task exec at\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     Name,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n}\n\nfunc (d *CronMetrics) SetTaskExecAt(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].SetToCurrentTime()\n}\n\nfunc (d *CronMetrics) RemoveTask(Name string, schedule string) {\n\td.TaskExecAt[Name][schedule].Set(0)\n\tdelete(d.TaskExecAt[Name], schedule)\n}\n","relevantFile":"<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n<file_path>cron.go\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\tc.remove <- id\n\t} else {\n\t\tc.removeEntry(id)\n\t}\n}\n\n// Start the cron scheduler in its own goroutine, or no-op if already started.\nfunc (c *Cron) Start() {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"\n    defer CronMetrics.RemoveTask(d.Name, d.Schedule)\n    d.Func()\n}","CodeQwen2.5-7B":"\n\td.Func()\n\tCronMetrics.SetTaskExecAt(d.Name, d.Schedule)\n}","copilot":"d.CronFunc()\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\t","infill":"prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}))","suffix":"\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"return err\n    }\n    d.scheduleDetail[task][schedule] = id\n    return nil","CodeQwen2.5-7B":"return err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}","copilot":"prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}))","deepseek-chat-lite":"return err","CodeQwen1.5-7B":"return err"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\t","infill":"Name: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}))","suffix":"\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron map[string]extract.Extract\n\tF10Kafka  extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>cron.go\nfunc (c *Cron) AddJob(spec string, cmd Job) (EntryID, error) {\n\tschedule, err := c.parser.Parse(spec)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn c.Schedule(schedule, cmd), nil\n}\n\n// Schedule adds a Job to the Cron to be run on the given schedule.\n// The job is wrapped with the configured Chain.\nfunc (c *Cron) Schedule(schedule Schedule, cmd Job) EntryID {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tc.nextID++\n\tentry := &Entry{\n\t\tID:         c.nextID,\n\t\tSchedule:   schedule,\n\t\tWrappedJob: c.chain.Then(cmd),\n\t\tJob:        cmd,\n\t}\n\tif !c.running {\n\t\tc.entries = append(c.entries, entry)\n\t} else {\n\t\tc.add <- entry\n\t}\n\treturn entry.ID\n}\n\n// Entries returns a snapshot of the cron entries.\nfunc (c *Cron) Entries() []Entry {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n\t\treplyChan := make(chan []Entry, 1)\n\t\tc.snapshot <- replyChan\n\t\treturn <-replyChan\n\t}\n\treturn c.entrySnapshot()\n}\n\n// Location gets the time zone location\nfunc (c *Cron) Location() *time.Location {\n\treturn c.location\n}\n\n// Entry returns a snapshot of the given entry, or nil if it couldn't be found.\nfunc (c *Cron) Entry(id EntryID) Entry {\n\tfor _, entry := range c.Entries() {\n\t\tif id == entry.ID {\n\t\t\treturn entry\n\t\t}\n\t}\n\treturn Entry{}\n}\n\n// Remove an entry from being run in the future.\nfunc (c *Cron) Remove(id EntryID) {\n\tc.runningMu.Lock()\n\tdefer c.runningMu.Unlock()\n\tif c.running {\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"return err\n    }\n    d.scheduleDetail[task][schedule] = id\n    return nil","CodeQwen2.5-7B":"return err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\treturn nil\n}","copilot":"Name: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}))","deepseek-chat-lite":"return err","CodeQwen1.5-7B":"return err"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(","infill":"prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},","suffix":"\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.\n\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// metrics\n\tmetricsTask := prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n\tprometheus.Register(metricsTask)\n\tmetricsTask.SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/extract/automatic/update_kafka.go\npackage automatic\n\nimport (\n\t\"context\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype KafkaConsumer struct {\n\tConfig        kafka_dao.Config\n\tTask          KafkaTaskInfo\n\tconsumer      kafka_dao.Consumer\n\tconsumerGroup sarama.ConsumerGroup\n\tctx           context.Context\n\tcancel        context.CancelFunc\n\tchErr         chan error\n}\n\nfunc (d KafkaConsumer) Start() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\tlevellog.Log.Info(\"No need to start kafka consume\")\n\t\treturn 0, nil\n\t}\n\tlevellog.Log.Info(\"start init kafka consumer\", zap.Strings(\"brokers\", d.Config.Brokers),\n\t\tzap.String(\"username\", d.Config.Username), zap.String(\"password\", d.Config.Password),\n\t\tzap.String(\"consumer_group\", d.Config.ConsumerGroup), zap.String(\"SecurityProtocol\", d.Config.SecurityProtocol),\n\t\tzap.String(\"SaslMechanism\", d.Config.SaslMechanism), zap.String(\"topic\", d.Task.GetTopic()))\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"prometheus.GaugeOpts{\n    Name: \"gms_cron_task_total\",\n    Help: \"The total number of cron tasks.\",\n}, []string{\"task\", \"schedule\"})","CodeQwen2.5-7B":"\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},","copilot":"prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",","infill":"Help: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},","suffix":"\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.Mus\n\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// metrics\n\tmetricsTask := prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n\tprometheus.Register(metricsTask)\n\tmetricsTask.SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n\t\tHelp: \"cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},","copilot":"Help: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string","infill":"var configPath string","suffix":"\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"./conf/conf.yaml\", \"Path to the configuration file\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit()\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\n\tserver.Init()\n}\n","relevantFile":"<file_path>app/config/config.go\npackage config\n\nimport (\n\t\"flag\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"os\"\n\n\t\"gopkg.in/yaml.v2\"\n)\n\ntype (\n\tMyConfig struct {\n\t\tAddress       string   `yaml:\"Address\"`       // write data source name. (dsn without database name\n\t\tUser          string   `yaml:\"User\"`          // write data source name. (dsn without database name\n\t\tPasswd        string   `yaml:\"Passwd\"`        // write data source name. (dsn without database name\n\t\tHost          string   `yaml:\"Host\"`          // write data source name. (dsn without database name\n\t\tParams        string   `yaml:\"Params\"`        // DSN params\n\t\tDefaultDbname string   `yaml:\"DefaultDbname\"` // default mysql database name\n\t\tRowLimit      int      `yaml:\"RowLimit\"`      // limit of row numbers in a process\n\t\tSinkDb        []string `yaml:\"SinkDb\"`        // 持久化的数据库\n\t}\n\n\tF10Config struct {\n\t\tUrl                string `yaml:\"Url\"`\n\t\tTopic              string `yaml:\"Topic\"`\n\t\tTimeout            int    `yaml:\"Timeout\"`\n\t\tLimit              int    `yaml:\"Limit\"`\n\t\tFrom               string `yaml:\"From\"`\n\t\tUnderlyingAssetUrl string `yaml:\"UnderlyingAssetUrl\"` // 机构id对应标的代码取数规则\n\t}\n\n\tFUsEtfAdjFactorConfig struct {\n\t\tUrl                string `yaml:\"Url\"`\n\t}\n\n\tPgConfig struct {\n\t\tQueryTimeout int `yaml:\"QueryTimeout\"` // time out of pg query\n\t}\n\n\tKafkaConfig struct {\n\t\tBrokers          string `yaml:\"brokers\"`\n\t\tUsername         string `yaml:\"username\"`\n\t\tPassword         string `yaml:\"password\"`\n\t\tConsumerGroup    string `yaml:\"consumer_group\"`\n\t\tSecurityProtocol string `yaml:\"security_protocol\"`\n\t\tSaslMechanism    string `yaml:\"sasl_mechanism\"`\n\t}\n\n\tServiceConfig struct {\n\t\tHttpPort int `yaml:\"HttpPort\"` // http port\n\t}\n\n\tLogConfig struct {\n\t\tLogPath     string `yaml:\"LogPath\"`     // log file path\n\t\tStatLogPath string `yaml:\"StatLogPath\"` // status log file path\n\t\tGinLogPath  string `yaml:\"GinLogPath\"`  // gin log file path\n\t\tLogLevel    string `yaml:\"LogLevel\"`    // log level\n\t}\n<file_path>app/finance_info/finance_info.go\npackage finance_info\n\nimport (\n\t\"errors\"\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\tmysql_dao \"finance_etl/app/dao/mysql\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"gorm.io/driver/mysql\"\n\t\"gorm.io/gorm\"\n\t\"gorm.io/gorm/logger\"\n\t\"gorm.io/gorm/schema\"\n)\n\ntype (\n\tFinanceInfoManager struct {\n\t\tdbName           string                                // database名称\n\t\tTableBasic       map[string]my_orm.FinanceBasic        // 财务表基础信息\n\t\tTableFieldName   map[string]my_orm.FieldInfo           // key是数据源指标名称\n\t\tMapFieldTable    map[string][]my_orm.FieldInfo         // key是表名\n\t\tMapTaskIndicator map[string]my_orm.ExtractF10Indicator // key是taskid\n\t\tMapTableTask     map[string][]string                   // key是表名，value是taskid列表\n\t}\n)\n\nconst FinanceDbEqual = \"finance_db = ?\"\n\nvar GlobalFinanceInfoManager map[string]FinanceInfoManager\n\nfunc Init() error {\n\t// 初始化连接\n\tmysqlDsn := mysql_dao.MakeDsn(config.GetMysql().Address, config.GetMysql().Params, config.GetMysql().DefaultDbname)\n\tlevellog.Log.Info(\"Init mysql connect\", zap.String(\"mysqlDsn\", mysqlDsn))\n\torm, err := gorm.Open(mysql.Open(mysqlDsn), &gorm.Config{\n\t\tNamingStrategy: schema.NamingStrategy{\n\t\t\tTablePrefix:   \"\",\n\t\t\tSingularTable: true,\n\t\t},\n\t\tLogger: logger.Default.LogMode(logger.Silent),\n\t})\n\tif err != nil {\n\t\treturn errors.New(err.Error() + \", mysqlDsn:\" + mysqlDsn)\n\t}\n\t// 初始化缓存\n\tGlobalFinanceInfoManager = make(map[string]FinanceInfoManager)\n\t// 初始化加载DbInfo\n\terr = LoadDbInfo(orm)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, init load dbinfo\", err.Error())\n\t}\n\t// 设置热更新函数, 定时更新\n\tcallback := func() {\n\t\terr := LoadDbInfo(orm)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Warn(\"load db-info cron\", zap.String(\"error\", err.Error()))\n\t\t}","relevantFileList":[],"filePath":"main.go","template":"go","multiRes":{"hipilot":" = \"v1.0.0\"\n\nvar configPath string","CodeQwen2.5-7B":"\nvar configPath string","copilot":"var configPath string","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \\","infill":"nsumer, such as package, data flow, etc.\n\t// kafkaConsumerMetrics = prometheus.NewGaugeVec(\n\t\t","suffix":"\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n","relevantFile":"<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n<file_path>app/cron/cron.go\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n<file_path>app/extract_manager/extract_manager.go\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"nal,  //实时数据","CodeQwen2.5-7B":"\"real\\\", //实时导出\n}","copilot":"nsumer, such as package, data flow, etc.\n\t// kafkaConsumerMetrics = prometheus.NewGaugeVec(\n\t\t","deepseek-chat-lite":"","CodeQwen1.5-7B":" \"real\\\", //实时导出"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个","infill":"Help: \"data recv package\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task},\n","suffix":"\n\n\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"个指标","CodeQwen2.5-7B":"指标\nvar (\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"rx_traffic\",\n\t\t\tHelp: \"流量统计\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)","copilot":"Help: \"data recv package\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task},\n","deepseek-chat-lite":"触发方式","CodeQwen1.5-7B":" 枚举"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的","infill":"kafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaSkipTaskID","suffix":"\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"表","CodeQwen2.5-7B":"指标\nvar (\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"rx_traffic\",\n\t\t\tHelp: \"流量指标\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)","copilot":"kafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaSkipTaskID","deepseek-chat-lite":"区别","CodeQwen1.5-7B":" 触发方式"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的","infill":"[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_task_id\",\n\t\t\tHelp: \"skip task id\",\n\t\t\t","suffix":"\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"表","CodeQwen2.5-7B":"指标\nvar (\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"rx_traffic\",\n\t\t\tHelp: \"流量指标\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)","copilot":"[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_task_id\",\n\t\t\tHelp: \"skip task id\",\n\t\t\t","deepseek-chat-lite":"区别","CodeQwen1.5-7B":" 触发方式"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更","infill":"// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition string, taskId string, flow float64) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, partition, taskId).Set(flow)\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition string, taskId string, skipNum float64) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, partition, taskId).Set(skipNum)\n}\n","suffix":"","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/cron/cron.go\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/extract/manual/request_f10.go\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n\tto := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.EndTime/10000, d.ReqParam.EndTime%10000/100, d.ReqParam.EndTime%100)\n\trequestBody := my_orm.F10Request{\n\t\tCurrency: financeTable.Currency,\n\t\tCategory: \"indicator\",\n\t\tStrategy: d.ReqParam.Strategy,\n\t\tFrom:     from,\n\t\tTo:       to,\n\t}\n\tmarket := financeTable.Market\n\tcodes, err := GetCodelist(d.FinanceDb, d.FinanceTable)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"req codelist failed\", zap.Error(err))\n\t\treturn 0, err\n\t}\n\trecordTotal := 0\n\t// 由于存在全量请求的情况，数据量较大，所以每次只请求一支代码\n\tfor _, code := range codes {\n\t\tlevellog.Log.Debug(\"code info\", zap.Int(\"submarket\", code.Submarket), zap.String(\"code\", code.Code))\n\t\tif strings.Contains(code.Code, \"_\") { // 过滤掉含有特殊符号的代码\n\t\t\tcontinue\n\t\t}\n\t\tsubjects := fmt.Sprintf(\"%s-%s\", market, code.Code)\n\t\trequestBody.Subjects = subjects\n\t\tfor _, taskId := range tableF10Task {\n\t\t\trequestBody.Ids = dbInfo.MapTaskIndicator[taskId].Ids\n\t\t\tval, _ := query.Values(requestBody)\n\t\t\treq.URL.RawQuery = val.Encode()\n\t\t\tresp, err := httpGetWithRetry(client, req, 3)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Warn(\"manual request f10 data, request failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\trecordCnt, err := d.ProcessF10RequestData(resp, market, requestBody.Ids, dbInfo.MapTaskIndicator[taskId].HadReport, dbInfo.MapTaskIndicator[taskId].DateType)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Info(\"manual request F10 data, process failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trecordTotal = recordTotal + recordCnt\n\t\t}\n\t}\n\tmetrics.TrafficMetricsAdd(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10, recordTotal)\n\t// f10持久化耗时记录\n\tmetrics.CostBucketMetricsObserve(d.FinanceDb, d.FinanceTable, metrics.StageAll, triggerType, exportType, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\tlevellog.Log.Info(\"f10 sinkdata affected:\", zap.String(\"db\", d.FinanceDb), zap.String(\"table\", d.FinanceTable), zap.Int(\"rows:\", recordTotal))\n\treturn recordTotal, nil\n}\n\nfunc (d DataRequestF10) ProcessF10RequestData(resp *http.Response, market string, ids string, hadReport int, date string) (int, error) {","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"新写入","CodeQwen2.5-7B":"新导出\nvar exportTypeDict2 = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出","copilot":"// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition string, taskId string, flow float64) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, partition, taskId).Set(flow)\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition string, taskId string, skipNum float64) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, partition, taskId).Set(skipNum)\n}\n","deepseek-chat-lite":"新方式","CodeQwen1.5-7B":" 新"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更","infill":"kafkaRecvSkipTaskId.WithLabelValues(topic, partition, taskId).Set(skipNum)\n}\n","suffix":"","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/cron/cron.go\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/extract/manual/request_f10.go\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n\tto := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.EndTime/10000, d.ReqParam.EndTime%10000/100, d.ReqParam.EndTime%100)\n\trequestBody := my_orm.F10Request{\n\t\tCurrency: financeTable.Currency,\n\t\tCategory: \"indicator\",\n\t\tStrategy: d.ReqParam.Strategy,\n\t\tFrom:     from,\n\t\tTo:       to,\n\t}\n\tmarket := financeTable.Market\n\tcodes, err := GetCodelist(d.FinanceDb, d.FinanceTable)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"req codelist failed\", zap.Error(err))\n\t\treturn 0, err\n\t}\n\trecordTotal := 0\n\t// 由于存在全量请求的情况，数据量较大，所以每次只请求一支代码\n\tfor _, code := range codes {\n\t\tlevellog.Log.Debug(\"code info\", zap.Int(\"submarket\", code.Submarket), zap.String(\"code\", code.Code))\n\t\tif strings.Contains(code.Code, \"_\") { // 过滤掉含有特殊符号的代码\n\t\t\tcontinue\n\t\t}\n\t\tsubjects := fmt.Sprintf(\"%s-%s\", market, code.Code)\n\t\trequestBody.Subjects = subjects\n\t\tfor _, taskId := range tableF10Task {\n\t\t\trequestBody.Ids = dbInfo.MapTaskIndicator[taskId].Ids\n\t\t\tval, _ := query.Values(requestBody)\n\t\t\treq.URL.RawQuery = val.Encode()\n\t\t\tresp, err := httpGetWithRetry(client, req, 3)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Warn(\"manual request f10 data, request failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\trecordCnt, err := d.ProcessF10RequestData(resp, market, requestBody.Ids, dbInfo.MapTaskIndicator[taskId].HadReport, dbInfo.MapTaskIndicator[taskId].DateType)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Info(\"manual request F10 data, process failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trecordTotal = recordTotal + recordCnt\n\t\t}\n\t}\n\tmetrics.TrafficMetricsAdd(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10, recordTotal)\n\t// f10持久化耗时记录\n\tmetrics.CostBucketMetricsObserve(d.FinanceDb, d.FinanceTable, metrics.StageAll, triggerType, exportType, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\tlevellog.Log.Info(\"f10 sinkdata affected:\", zap.String(\"db\", d.FinanceDb), zap.String(\"table\", d.FinanceTable), zap.Int(\"rows:\", recordTotal))\n\treturn recordTotal, nil\n}\n\nfunc (d DataRequestF10) ProcessF10RequestData(resp *http.Response, market string, ids string, hadReport int, date string) (int, error) {","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"新指标\nvar (\n    TotalUpdate = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Namespace: \"finance_etl\",\n            Subsystem: \"update\",\n            Name:      \"total_count\",\n            Help:      \"total","CodeQwen2.5-7B":"新导出\nvar exportTypeDict2 = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出","copilot":"kafkaRecvSkipTaskId.WithLabelValues(topic, partition, taskId).Set(skipNum)\n}\n","deepseek-chat-lite":"新方式","CodeQwen1.5-7B":" 新"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出","infill":"rconv.Itoa(int(partition)), taskId).Add(flow)\n}","suffix":"\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Inc()\n}\n","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/cron/cron.go\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/extract/manual/request_f10.go\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n\tto := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.EndTime/10000, d.ReqParam.EndTime%10000/100, d.ReqParam.EndTime%100)\n\trequestBody := my_orm.F10Request{\n\t\tCurrency: financeTable.Currency,\n\t\tCategory: \"indicator\",\n\t\tStrategy: d.ReqParam.Strategy,\n\t\tFrom:     from,\n\t\tTo:       to,\n\t}\n\tmarket := financeTable.Market\n\tcodes, err := GetCodelist(d.FinanceDb, d.FinanceTable)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"req codelist failed\", zap.Error(err))\n\t\treturn 0, err\n\t}\n\trecordTotal := 0\n\t// 由于存在全量请求的情况，数据量较大，所以每次只请求一支代码\n\tfor _, code := range codes {\n\t\tlevellog.Log.Debug(\"code info\", zap.Int(\"submarket\", code.Submarket), zap.String(\"code\", code.Code))\n\t\tif strings.Contains(code.Code, \"_\") { // 过滤掉含有特殊符号的代码\n\t\t\tcontinue\n\t\t}\n\t\tsubjects := fmt.Sprintf(\"%s-%s\", market, code.Code)\n\t\trequestBody.Subjects = subjects\n\t\tfor _, taskId := range tableF10Task {\n\t\t\trequestBody.Ids = dbInfo.MapTaskIndicator[taskId].Ids\n\t\t\tval, _ := query.Values(requestBody)\n\t\t\treq.URL.RawQuery = val.Encode()\n\t\t\tresp, err := httpGetWithRetry(client, req, 3)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Warn(\"manual request f10 data, request failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\trecordCnt, err := d.ProcessF10RequestData(resp, market, requestBody.Ids, dbInfo.MapTaskIndicator[taskId].HadReport, dbInfo.MapTaskIndicator[taskId].DateType)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Info(\"manual request F10 data, process failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trecordTotal = recordTotal + recordCnt\n\t\t}\n\t}\n\tmetrics.TrafficMetricsAdd(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10, recordTotal)\n\t// f10持久化耗时记录\n\tmetrics.CostBucketMetricsObserve(d.FinanceDb, d.FinanceTable, metrics.StageAll, triggerType, exportType, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\tlevellog.Log.Info(\"f10 sinkdata affected:\", zap.String(\"db\", d.FinanceDb), zap.String(\"table\", d.FinanceTable), zap.Int(\"rows:\", recordTotal))\n\treturn recordTotal, nil\n}\n\nfunc (d DataRequestF10) ProcessF10RequestData(resp *http.Response, market string, ids string, hadReport int, date string) (int, error) {","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n}","copilot":"rconv.Itoa(int(partition)), taskId).Add(flow)\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出","infill":"strconv.Itoa(int(partition)), taskId).Add(packageNum)","suffix":"\n}\n\n// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition int32, taskId string, flow float64) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(flow)\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Inc()\n}\n","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/cron/cron.go\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/extract/manual/request_f10.go\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n\tto := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.EndTime/10000, d.ReqParam.EndTime%10000/100, d.ReqParam.EndTime%100)\n\trequestBody := my_orm.F10Request{\n\t\tCurrency: financeTable.Currency,\n\t\tCategory: \"indicator\",\n\t\tStrategy: d.ReqParam.Strategy,\n\t\tFrom:     from,\n\t\tTo:       to,\n\t}\n\tmarket := financeTable.Market\n\tcodes, err := GetCodelist(d.FinanceDb, d.FinanceTable)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"req codelist failed\", zap.Error(err))\n\t\treturn 0, err\n\t}\n\trecordTotal := 0\n\t// 由于存在全量请求的情况，数据量较大，所以每次只请求一支代码\n\tfor _, code := range codes {\n\t\tlevellog.Log.Debug(\"code info\", zap.Int(\"submarket\", code.Submarket), zap.String(\"code\", code.Code))\n\t\tif strings.Contains(code.Code, \"_\") { // 过滤掉含有特殊符号的代码\n\t\t\tcontinue\n\t\t}\n\t\tsubjects := fmt.Sprintf(\"%s-%s\", market, code.Code)\n\t\trequestBody.Subjects = subjects\n\t\tfor _, taskId := range tableF10Task {\n\t\t\trequestBody.Ids = dbInfo.MapTaskIndicator[taskId].Ids\n\t\t\tval, _ := query.Values(requestBody)\n\t\t\treq.URL.RawQuery = val.Encode()\n\t\t\tresp, err := httpGetWithRetry(client, req, 3)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Warn(\"manual request f10 data, request failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\trecordCnt, err := d.ProcessF10RequestData(resp, market, requestBody.Ids, dbInfo.MapTaskIndicator[taskId].HadReport, dbInfo.MapTaskIndicator[taskId].DateType)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Info(\"manual request F10 data, process failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trecordTotal = recordTotal + recordCnt\n\t\t}\n\t}\n\tmetrics.TrafficMetricsAdd(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10, recordTotal)\n\t// f10持久化耗时记录\n\tmetrics.CostBucketMetricsObserve(d.FinanceDb, d.FinanceTable, metrics.StageAll, triggerType, exportType, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\tlevellog.Log.Info(\"f10 sinkdata affected:\", zap.String(\"db\", d.FinanceDb), zap.String(\"table\", d.FinanceTable), zap.Int(\"rows:\", recordTotal))\n\treturn recordTotal, nil\n}\n\nfunc (d DataRequestF10) ProcessF10RequestData(resp *http.Response, market string, ids string, hadReport int, date string) (int, error) {","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n}","copilot":"strconv.Itoa(int(partition)), taskId).Add(packageNum)","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// print log\n\tlog.Printf(\"add task %s, schedule %s\", task, schedule)\n\tprometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     \"task\",\n\t\t\t\"schedule\": \"","infill":"schedule","suffix":"\",\n\t\t},\n\t}).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n\t\tlog.Fatalf(\"解析config.yaml出错: %v\", err)\n\t}\n\tif cfg.Mysql.Host != \"\" {\n\t\tserver := os.Getenv(cfg.Mysql.Host)\n\t\tcfg.Mysql.Address = fmt.Sprintf(\"%s:%s@tcp(%s)/\", cfg.Mysql.User, cfg.Mysql.Passwd, server)\n\t}\n\n\t// 代码变更相关的市场配置信息初始化\n\tCodeChangeMarketConfigMap = GetCodeChangeMarketConfigMap()\n}\n\nfunc GetMysql() MyConfig {\n\treturn cfg.Mysql\n}\n\nfunc GetF10() F10Config {\n\treturn cfg.F10\n}\n\nfunc GetFUsEtfAdjFactor() FUsEtfAdjFactorConfig {\n\treturn cfg.FUsEtfAdjFactor\n}\n\nfunc GetPgsql() PgConfig {\n\treturn cfg.Pgsql\n}\n\nfunc GetKafka() KafkaConfig {\n\treturn cfg.Kafka\n}\n\nfunc GetService() ServiceConfig {\n\treturn cfg.Service\n}\n\nfunc GetLog() LogConfig {\n\treturn cfg.Log\n}\n\nfunc GetBasicDataAddr() string {\n\treturn cfg.BasicData\n}\n<file_path>app/dao/mysql/db.go\npackage mysql_dao\n\nimport (\n\t\"database/sql\"\n)\n\n/*newDbConn\n * @Description: 创建新的mysql连接\n * @param dsn\n * @return db\n * @return err\n */\nfunc NewDbConn(dsn string) (db *sql.DB, err error) {\n\tdb, err = sql.Open(\"mysql\", dsn)\n\treturn\n}\n\nfunc MakeDsn(address string, params string, dbName string) string {\n\treturn address + dbName + \"?\" + params\n}\n<file_path>app/finance_info/finance_info.go\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n\t\t\tfinanceInfoManager.MapTaskIndicator[v.TaskId] = v\n\t\t\t_, ok := financeInfoManager.MapTableTask[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = make([]string, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = append(financeInfoManager.MapTableTask[v.FinanceTable], v.TaskId)\n\t\t}\n\t\tGlobalFinanceInfoManager[dbname] = financeInfoManager\n\t}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"schedule","CodeQwen2.5-7B":"schedule<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"schedule","deepseek-chat-lite":"schedule","CodeQwen1.5-7B":" schedule\","}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule","infill":"prometheus.Register(metricsTask)\n\tmetricsTask.SetToCurrentTime()","suffix":"\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tQueryTimeout int `yaml:\"QueryTimeout\"` // time out of pg query\n\t}\n\n\tKafkaConfig struct {\n\t\tBrokers          string `yaml:\"brokers\"`\n\t\tUsername         string `yaml:\"username\"`\n\t\tPassword         string `yaml:\"password\"`\n\t\tConsumerGroup    string `yaml:\"consumer_group\"`\n\t\tSecurityProtocol string `yaml:\"security_protocol\"`\n\t\tSaslMechanism    string `yaml:\"sasl_mechanism\"`\n\t}\n\n\tServiceConfig struct {\n\t\tHttpPort int `yaml:\"HttpPort\"` // http port\n\t}\n\n\tLogConfig struct {\n\t\tLogPath     string `yaml:\"LogPath\"`     // log file path\n\t\tStatLogPath string `yaml:\"StatLogPath\"` // status log file path\n\t\tGinLogPath  string `yaml:\"GinLogPath\"`  // gin log file path\n\t\tLogLevel    string `yaml:\"LogLevel\"`    // log level\n\t}\n\n\tSecurityCodeChangeConfig struct {\n\t\tUrl          string                   `yaml:\"Url\"`\n\t\tTimeout      int                      `yaml:\"Timeout\"`\n\t\tMarketConfig []CodeChangeMarketConfig `yaml:\"MarketConfig\"`\n\t}\n\tCodeChangeMarketConfig struct {\n\t\tMarket   string `yaml:\"Market\"`\n\t\tSinkDb   string `yaml:\"SinkDb\"`\n\t\tCronExpr string `yaml:\"CronExpr\"`\n\t}\n\n\tConfig struct {\n\t\tMysql              MyConfig                 `yaml:\"Mysql\"`      // mysql configure\n\t\tBasicData          string                   `yaml:\"basic_data\"` // 代码表环境变量\n\t\tF10                F10Config                `yaml:\"F10\"`\n\t\tFUsEtfAdjFactor    FUsEtfAdjFactorConfig    `yaml:\"FUsEtfAdjFactor\"`\n\t\tPgsql              PgConfig                 `yaml:\"Pgsql\"` // pgsql configure\n\t\tKafka              KafkaConfig              `yaml:\"Kafka\"`\n\t\tService            ServiceConfig            `yaml:\"Service\"`            // service configure\n\t\tLog                LogConfig                `yaml:\"Log\"`                // log configure\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n<file_path>app/finance_info/finance_info.go\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n\t\t\tfinanceInfoManager.MapTaskIndicator[v.TaskId] = v\n\t\t\t_, ok := financeInfoManager.MapTableTask[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = make([]string, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = append(financeInfoManager.MapTableTask[v.FinanceTable], v.TaskId)\n\t\t}\n\t\tGlobalFinanceInfoManager[dbname] = financeInfoManager\n\t}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"schedule, cronFunc)\n    if err != nil {\n        return err\n    }\n    d.scheduleDetail[task][schedule] = id\n\n    prometheus.Register(prometheus.NewGauge(prometheus.GaugeOpts{\n       ","CodeQwen2.5-7B":", cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\tprometheus.MustRegister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}))<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"prometheus.Register(metricsTask)\n\tmetricsTask.SetToCurrentTime()","deepseek-chat-lite":", cronFunc)","CodeQwen1.5-7B":" , cronFunc)"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nfunc (d *CronManager) Start() {\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule","infill":"metricsTask.SetToCurrentTime()","suffix":"\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/config/config.go\n\t\tQueryTimeout int `yaml:\"QueryTimeout\"` // time out of pg query\n\t}\n\n\tKafkaConfig struct {\n\t\tBrokers          string `yaml:\"brokers\"`\n\t\tUsername         string `yaml:\"username\"`\n\t\tPassword         string `yaml:\"password\"`\n\t\tConsumerGroup    string `yaml:\"consumer_group\"`\n\t\tSecurityProtocol string `yaml:\"security_protocol\"`\n\t\tSaslMechanism    string `yaml:\"sasl_mechanism\"`\n\t}\n\n\tServiceConfig struct {\n\t\tHttpPort int `yaml:\"HttpPort\"` // http port\n\t}\n\n\tLogConfig struct {\n\t\tLogPath     string `yaml:\"LogPath\"`     // log file path\n\t\tStatLogPath string `yaml:\"StatLogPath\"` // status log file path\n\t\tGinLogPath  string `yaml:\"GinLogPath\"`  // gin log file path\n\t\tLogLevel    string `yaml:\"LogLevel\"`    // log level\n\t}\n\n\tSecurityCodeChangeConfig struct {\n\t\tUrl          string                   `yaml:\"Url\"`\n\t\tTimeout      int                      `yaml:\"Timeout\"`\n\t\tMarketConfig []CodeChangeMarketConfig `yaml:\"MarketConfig\"`\n\t}\n\tCodeChangeMarketConfig struct {\n\t\tMarket   string `yaml:\"Market\"`\n\t\tSinkDb   string `yaml:\"SinkDb\"`\n\t\tCronExpr string `yaml:\"CronExpr\"`\n\t}\n\n\tConfig struct {\n\t\tMysql              MyConfig                 `yaml:\"Mysql\"`      // mysql configure\n\t\tBasicData          string                   `yaml:\"basic_data\"` // 代码表环境变量\n\t\tF10                F10Config                `yaml:\"F10\"`\n\t\tFUsEtfAdjFactor    FUsEtfAdjFactorConfig    `yaml:\"FUsEtfAdjFactor\"`\n\t\tPgsql              PgConfig                 `yaml:\"Pgsql\"` // pgsql configure\n\t\tKafka              KafkaConfig              `yaml:\"Kafka\"`\n\t\tService            ServiceConfig            `yaml:\"Service\"`            // service configure\n\t\tLog                LogConfig                `yaml:\"Log\"`                // log configure\n\t\tSecurityCodeChange SecurityCodeChangeConfig `yaml:\"SecurityCodeChange\"` // 证券代码变更配置\n\t}\n)\n\nvar (\n\tcfg                       Config\n\tCodeChangeMarketConfigMap map[string]CodeChangeMarketConfig // 代码变更相关的市场配置信息\n)\n\nfunc ConfigureInit(configPath string) {\n\t// 线上环境配置,提交时注意解除注释\n\tcfgFile := flag.String(\"f\", configPath, \"config file path\")\n\t// 本地环境,提交时注意注释掉\n\t//cfgFile := flag.String(\"f\", \"./conf/conf.yaml\", \"config file path\")\n\tflag.Parse()\n\tcontent, err := ioutil.ReadFile(*cfgFile)\n\tif yaml.Unmarshal(content, &cfg) != nil {\n<file_path>app/finance_info/finance_info.go\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = append(financeInfoManager.MapFieldTable[v.FinanceTable], v)\n\t\t}\n\t\t// 获取F10任务id和指标映射关系\n\t\tvar extractF10Indicator []my_orm.ExtractF10Indicator\n\t\torm.Table(\"ExtractF10Indicator\").Where(FinanceDbEqual, dbname).Find(&extractF10Indicator)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range extractF10Indicator {\n\t\t\tfinanceInfoManager.MapTaskIndicator[v.TaskId] = v\n\t\t\t_, ok := financeInfoManager.MapTableTask[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = make([]string, 0)\n\t\t\t}\n\t\t\tfinanceInfoManager.MapTableTask[v.FinanceTable] = append(financeInfoManager.MapTableTask[v.FinanceTable], v.TaskId)\n\t\t}\n\t\tGlobalFinanceInfoManager[dbname] = financeInfoManager\n\t}\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":"schedule, cronFunc())\n    if err != nil {\n        return err\n    }\n    d.scheduleDetail[task][schedule] = id","CodeQwen2.5-7B":", cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\tprometheus.MustRegister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t}))<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"metricsTask.SetToCurrentTime()","deepseek-chat-lite":", cronFunc)","CodeQwen1.5-7B":" , cronFunc)"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll: true,\n\t\tFilters: filters.Args{\n\t\t\tfilters:","infill":" map[string]map[string]bool{\n\t\t\t\t\"name\": {\n\t\t\t\t\t\"(_|-)dev\": true,\n\t\t\t\t},\n\t\t\t},","suffix":"\n\t\t}})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n<file_path>parse.go\nmultiple values.\n*/\npackage filters // import \"github.com/docker/docker/api/types/filters\"\n\nimport (\n\t\"encoding/json\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// Args stores a mapping of keys to a set of multiple values.\ntype Args struct {\n\tfields map[string]map[string]bool\n}\n\n// KeyValuePair are used to initialize a new Args\ntype KeyValuePair struct {\n\tKey   string\n\tValue string\n}\n\n// Arg creates a new KeyValuePair for initializing Args\nfunc Arg(key, value string) KeyValuePair {\n\treturn KeyValuePair{Key: key, Value: value}\n}\n\n// NewArgs returns a new Args populated with the initial args\nfunc NewArgs(initialArgs ...KeyValuePair) Args {\n\targs := Args{fields: map[string]map[string]bool{}}\n\tfor _, arg := range initialArgs {\n\t\targs.Add(arg.Key, arg.Value)\n\t}\n\treturn args\n}\n\n// Keys returns all the keys in list of Args\nfunc (args Args) Keys() []string {\n\tkeys := make([]string, 0, len(args.fields))\n\tfor k := range args.fields {\n\t\tkeys = append(keys, k)\n\t}\n\treturn keys\n}\n\n// MarshalJSON returns a JSON byte representation of the Args\nfunc (args Args) MarshalJSON() ([]byte, error) {\n\tif len(args.fields) == 0 {\n\t\treturn []byte(\"{}\"), nil\n\t}\n\treturn json.Marshal(args.fields)\n}\n\n// ToJSON returns the Args as a JSON encoded string\nfunc ToJSON(a Args) (string, error) {\n\tif a.Len() == 0 {\n\t\treturn \"\", nil\n\t}\n\tbuf, err := json.Marshal(a)","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"map[string][]string{\n                \"label\": {\"org.opencontainers.image.title=devcontainer\"},\n            },","CodeQwen2.5-7B":": {\n\t\t\t\t\"name\": []string{\"(_|-)dev$\"},\n\t\t\t},\n\t\t},\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}","copilot":" map[string]map[string]bool{\n\t\t\t\t\"name\": {\n\t\t\t\t\t\"(_|-)dev\": true,\n\t\t\t\t},\n\t\t\t},","deepseek-chat-lite":" []string{","CodeQwen1.5-7B":" map[string][]string{"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc ","infill":":= Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: container.State,\n\t\t}","suffix":"\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tc.Ports += port.IP + \":\" + port.PublicPort + \"->\" + port.PrivatePort + \"/\" + port.Type + \" \"\n\t\t}\n\n\t\tcontainerList = append(containerList, Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: container.State,\n\t\t})\n\t}\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>port.go\npackage types\n\n// This file was generated by the swagger tool.\n// Editing this file might prove futile when you re-run the swagger generate command\n\n// Port An open port on a container\n// swagger:model Port\ntype Port struct {\n\n\t// Host IP address that the container's port is mapped to\n\tIP string `json:\"IP,omitempty\"`\n\n\t// Port on the container\n\t// Required: true\n\tPrivatePort uint16 `json:\"PrivatePort\"`\n\n\t// Port exposed on the host\n\tPublicPort uint16 `json:\"PublicPort,omitempty\"`\n\n\t// type\n\t// Required: true\n\tType string `json:\"Type\"`\n}\n<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":":= Container{}","CodeQwen2.5-7B":":= Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: container.State,\n\t\t}","copilot":":= Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: container.State,\n\t\t}","deepseek-chat-lite":":= Container{}","CodeQwen1.5-7B":" := Container{"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],","infill":"Image:  container.Image,\n\t\t\tStatus: container.State,\n\t\t}","suffix":"\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tc.Ports += port.IP + \":\" + port.PublicPort + \"->\" + port.PrivatePort + \"/\" + port.Type + \" \"\n\t\t}\n\n\t\tcontainerList = append(containerList, Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: container.State,\n\t\t})\n\t}\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>port.go\npackage types\n\n// This file was generated by the swagger tool.\n// Editing this file might prove futile when you re-run the swagger generate command\n\n// Port An open port on a container\n// swagger:model Port\ntype Port struct {\n\n\t// Host IP address that the container's port is mapped to\n\tIP string `json:\"IP,omitempty\"`\n\n\t// Port on the container\n\t// Required: true\n\tPrivatePort uint16 `json:\"PrivatePort\"`\n\n\t// Port exposed on the host\n\tPublicPort uint16 `json:\"PublicPort,omitempty\"`\n\n\t// type\n\t// Required: true\n\tType string `json:\"Type\"`\n}\n<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n\t\t\tImage:  container.Image,\n\t\t\tStatus: container.State,\n\t\t}","copilot":"Image:  container.Image,\n\t\t\tStatus: container.State,\n\t\t}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: container.State,\n\t\t}","infill":"\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tc.Ports += port.IP + \":\" + port.PublicPort + \"->\" + port.PrivatePort + \"/\" + port.Type + \" \"\n\t\t}\n","suffix":"\n\t\tcontainerList = append(containerList, Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: container.State,\n\t\t})\n\t}\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>port.go\npackage types\n\n// This file was generated by the swagger tool.\n// Editing this file might prove futile when you re-run the swagger generate command\n\n// Port An open port on a container\n// swagger:model Port\ntype Port struct {\n\n\t// Host IP address that the container's port is mapped to\n\tIP string `json:\"IP,omitempty\"`\n\n\t// Port on the container\n\t// Required: true\n\tPrivatePort uint16 `json:\"PrivatePort\"`\n\n\t// Port exposed on the host\n\tPublicPort uint16 `json:\"PublicPort,omitempty\"`\n\n\t// type\n\t// Required: true\n\tType string `json:\"Type\"`\n}\n<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n\t\tlog.Println(c)","copilot":"\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tc.Ports += port.IP + \":\" + port.PublicPort + \"->\" + port.PrivatePort + \"/\" + port.Type + \" \"\n\t\t}\n","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: container.State,\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {","infill":"c.Ports += port.IP + \":\" + strconv.i port.PublicPort + \"->\" + port.PrivatePort + \"/\" + port.Type + \" \"\n\t\t}","suffix":"\n\n\t\tcontainerList = append(containerList, Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: container.State,\n\t\t})\n\t}\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>port.go\npackage types\n\n// This file was generated by the swagger tool.\n// Editing this file might prove futile when you re-run the swagger generate command\n\n// Port An open port on a container\n// swagger:model Port\ntype Port struct {\n\n\t// Host IP address that the container's port is mapped to\n\tIP string `json:\"IP,omitempty\"`\n\n\t// Port on the container\n\t// Required: true\n\tPrivatePort uint16 `json:\"PrivatePort\"`\n\n\t// Port exposed on the host\n\tPublicPort uint16 `json:\"PublicPort,omitempty\"`\n\n\t// type\n\t// Required: true\n\tType string `json:\"Type\"`\n}\n<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":" // Not implemented for now","CodeQwen2.5-7B":"\n\t\t\tc.Ports += strconv.Itoa(int(port.PublicPort)) + \":\" + strconv.Itoa(int(port.PrivatePort)) + \" \"\n\t\t}","copilot":"c.Ports += port.IP + \":\" + strconv.i port.PublicPort + \"->\" + port.PrivatePort + \"/\" + port.Type + \" \"\n\t\t}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: container.State,\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tc.Ports += port.IP + \":\" + strconv.Itoa(int(port.PublicPort)) + \"->\" + strconv.Itoa(int(port.PrivatePort)) + \"/\" + port.Type + \" \"\n\t\t}\n\n\t\tcontainerList = append(containerList, c","infill":")","suffix":"\n\t}\n\n\treturn containerList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>itoa.go\n\npackage strconv\n\nimport \"math/bits\"\n\nconst fastSmalls = true // enable fast path for small integers\n\n// FormatUint returns the string representation of i in the given base,\n// for 2 <= base <= 36. The result uses the lower-case letters 'a' to 'z'\n// for digit values >= 10.\nfunc FormatUint(i uint64, base int) string {\n\tif fastSmalls && i < nSmalls && base == 10 {\n\t\treturn small(int(i))\n\t}\n\t_, s := formatBits(nil, i, base, false, false)\n\treturn s\n}\n\n// FormatInt returns the string representation of i in the given base,\n// for 2 <= base <= 36. The result uses the lower-case letters 'a' to 'z'\n// for digit values >= 10.\nfunc FormatInt(i int64, base int) string {\n\tif fastSmalls && 0 <= i && i < nSmalls && base == 10 {\n\t\treturn small(int(i))\n\t}\n\t_, s := formatBits(nil, uint64(i), base, i < 0, false)\n\treturn s\n}\n\n// Itoa is equivalent to FormatInt(int64(i), 10).\nfunc Itoa(i int) string {\n\treturn FormatInt(int64(i), 10)\n}\n\n// AppendInt appends the string form of the integer i,\n// as generated by FormatInt, to dst and returns the extended buffer.\nfunc AppendInt(dst []byte, i int64, base int) []byte {\n\tif fastSmalls && 0 <= i && i < nSmalls && base == 10 {\n\t\treturn append(dst, small(int(i))...)\n\t}\n\tdst, _ = formatBits(dst, uint64(i), base, i < 0, true)\n\treturn dst\n}\n\n// AppendUint appends the string form of the unsigned integer i,\n// as generated by FormatUint, to dst and returns the extended buffer.\nfunc AppendUint(dst []byte, i uint64, base int) []byte {\n\tif fastSmalls && i < nSmalls && base == 10 {\n\t\treturn append(dst, small(int(i))...)\n\t}\n\tdst, _ = formatBits(dst, i, base, false, true)\n\treturn dst\n}\n\n// small returns the string for an i with 0 <= i < nSmalls.\nfunc small(i int) string {\n\tif i < 10 {\n\t\treturn digits[i : i+1]\n\t}\n\treturn smallsString[i*2 : i*2+2]\n<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":")","CodeQwen2.5-7B":")","copilot":")","deepseek-chat-lite":")","CodeQwen1.5-7B":" )<file_sep> /README.md"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = ","infill":"strconv.Itoa(int(port.PublicPort))","suffix":"\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"strconv.Itoa(int(port.PublicPort))","CodeQwen2.5-7B":"port.PublicPort + \" (SSH)\"<|cursor|>","copilot":"strconv.Itoa(int(port.PublicPort))","deepseek-chat-lite":"\"SSH\"","CodeQwen1.5-7B":" c.Ports + \"SSH \""}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`","infill":"VirtualSize string `json:\"virtualSize\"`\n}","suffix":"\n\nfunc GetImagesFromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timg := Image{\n\t\t\tID:          image.ID,\n\t\t\tRepository:  image.RepoTags[0],\n\t\t\tTag:         image.RepoTags[1],\n\t\t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),\n\t\t\tSize:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t\t\tVirtualSize: strconv.Itoa(int(image.VirtualSize / 1024 / 1024)),\n\t\t}\n\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":" // size in MB\n    VirtualSize string `json:\"virtualSize\"`","CodeQwen2.5-7B":"\n\tVirtualSize string `json:\"virtualSize\"`\n}","copilot":"VirtualSize string `json:\"virtualSize\"`\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}\n\nfunc GetImagesFromDocker() ([]Image, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"re","infill":"ference\", \"ubuntu\")","suffix":"\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(images)\n\n\timageList := []Image{}\n\t// for _, image := range images {\n\t// \timg := Image{\n\t// \t\tID:          image.ID,\n\t// \t\tRepository:  image.RepoTags[0],\n\t// \t\tTag:         image.RepoTags[1],\n\t// \t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),\n\t// \t\tSize:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t// \t\tVirtualSize: strconv.Itoa(int(image.VirtualSize / 1024 / 1024)),\n\t// \t}\n\n\t// \timageList = append(imageList, img)\n\t// }\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"reference\", \"dev-*\")","CodeQwen2.5-7B":"pository\", \"(_|-)dev$\")","copilot":"ference\", \"ubuntu\")","deepseek-chat-lite":"ferences\", \"dev\")","CodeQwen1.5-7B":"pository\", \"dev\")"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects)\n\treturn\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直","infill":"接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致","suffix":"\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\tdateStr, ok := jsonF10Rsp.Data.Data[0].Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 {\n\t\td.ProcessInvalidData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t// 如果 validInt=1, 则更新相应的指标数据\n\t// 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tif taskInfo.Currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(ids, subjects, taskInfo.Currency)\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tsearchSql := fmt.Sprintf(\"SELECT isvalid FROM `%s` WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\trow, _ := load.FetchOneRowData(taskInfo.DbName, searchSql)\n\tvar isvalidField int // mysql数据库中isvalid字段\n\terr := row.Scan(&isvalidField)\n\tif err == nil {\n\t\tloadSql := fmt.Sprintf(\"INSERT INTO `%s` (zqdm,unix_time) VALUES (%s, %d) ON DUPLICATE KEY UPDATE isvalid = 0\", taskInfo.TableName, codeStr, unixTime)\n\t\t_, err := load.SinkData(taskInfo.DbName, loadSql)\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"search isvalid in local mysql, searchSql:%s, loadSql:%s, err:%s, msg:%s\", searchSql, loadSql, err.Error())\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t\t}\n\t}\n\treturn\n}\n\n// kafka 推送数据为原生数据，这里再请求一次指标数据，\nfunc (d F10KafkaTaskInfo) RequestF10Data(ids string, subjects string, currency string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      ids,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"更新数据，否则需要先删除在新增","CodeQwen2.5-7B":"接更新数据\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则先查询一次数据，再查询一次数据，如果第二次查询的日期和kafka推送的日期一致，则直接更新数据\n// 5. 如果第二次查询的日期和kafka推送的日期不一致，则先查询一次数据，再查询一次数据，如果第三次查询的日期和kafka推送的日期一致，则直接更新数据\n// 6. 如果第三次查询的日期和kafka推送的日期不一致，则先查询一次数据，再查询一次数据，如果第四次查询的日期和kafka推送的日期一致，则直接更新数据\n// 7. 如果第四次查询的日期和kafka推送的日期不一致，则先查询一次数据，再查询一次数据，如果第五次查询的日期和kafka推送的日期一致，则直接更新数据\n// 8. 如果第五次查询的日期和kafka推送的日期不一致，则先查询一次数据，再查询一次数据，如果第六次查询的日期和kafka推送的日期一致，则直接更新数据\n// 9. 如果第六次查询的日期和kafka推送的日期不一致，则先查询一次数据，再查询一次数据，如果第七次查询的日期和kafka推送的日期一致，则直接更新数据\n// 10. 如果第七次查询的日期和kafka推送的日期不一致","copilot":"接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致","deepseek-chat-lite":"接使用第一次查询的结果","CodeQwen1.5-7B":" 接更新数据库"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects)\n\treturn\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\tdateStr, ok := jsonF10Rsp.Data.Data[0].Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 {\n\t\td.ProcessInvalidData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t// 如果 validInt=1, 则更新相应的指标数据\n\t// 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tif taskInfo.Currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(ids, subjects, taskInfo.Currency)\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\terr = fmt.Errorf(\"update isvalid in local mysql, sql:%s, err:%s\", updateIsValidSql, err.Error())\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid in local mysql success\", zap.String(\"t","infill":"able\", taskInfo.TableName), zap.String(\"code\", codeStr), \n\t\tzap.String(\"date\", dateStr), zap.Int(\"valid\", 0)","suffix":"\n\t}\n\treturn\n}\n\n// kafka 推送数据为原生数据，这里再请求一次指标数据，\nfunc (d F10KafkaTaskInfo) RequestF10Data(ids string, subjects string, currency string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      ids,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"able\", taskInfo.TableName))","CodeQwen2.5-7B":"able\", taskInfo.TableName), zap.String(\"code\", codeStr), zap.String(\"date\", dateStr), zap.Int(\"valid\", validInt))\n\t}\n}","copilot":"able\", taskInfo.TableName), zap.String(\"code\", codeStr), \n\t\tzap.String(\"date\", dateStr), zap.Int(\"valid\", 0)","deepseek-chat-lite":"ask\", f10TaskId), zap.String(\"subjects\", subjects))","CodeQwen1.5-7B":"ask\", f10TaskId), zap.String(\"subjects\", subjects))"}},{"prefix":"package cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t}","infill":",\n\t[]string{\"task\", \"schedule\"},\n","suffix":")\n\nfunc (d *CronManager) Start() {\n\tprometheus.Must\n\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\t// metrics\n\tmetricsTask := prometheus.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"task\":     task,\n\t\t\t\"schedule\": schedule,\n\t\t},\n\t})\n\tprometheus.Register(metricsTask)\n\tmetricsTask.SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n\n\t\tprometheus.Unregister(prometheus.NewGauge(prometheus.GaugeOpts{\n\t\t\tName: \"gms_cron_task\",\n\t\t\tConstLabels: map[string]string{\n\t\t\t\t\"task\":     task,\n\t\t\t\t\"schedule\": s,\n\t\t\t},\n\t\t}))\n\t}\n\tdelete(d.scheduleDetail, task)\n\n}\n","relevantFile":"<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))","relevantFileList":[],"filePath":"app/cron/cron.go","template":"go","multiRes":{"hipilot":", []string{\"task\", \"schedule\"}","CodeQwen2.5-7B":",\n\t[]string{\"task\", \"schedule\"},\n","copilot":",\n\t[]string{\"task\", \"schedule\"},\n","deepseek-chat-lite":",","CodeQwen1.5-7B":" ,"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个","infill":"ow, etc.\n\tkafkaConsumerMetrics = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"gms_kafka_consumer\",\n\t\t\t\n","suffix":"\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"个prometheus.CounterVec的初始化","CodeQwen2.5-7B":"指标\nvar (\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"rx_traffic\",\n\t\t\tHelp: \"流量统计\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)","copilot":"ow, etc.\n\tkafkaConsumerMetrics = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"gms_kafka_consumer\",\n\t\t\t\n","deepseek-chat-lite":"触发方式","CodeQwen1.5-7B":" 枚举"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个","infill":"prometheus.GaugeOpts{\n\t\t\tName: \"gms_kafka_consumer\",\n\t\t\t","suffix":"\n\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"个指标","CodeQwen2.5-7B":"指标\nvar (\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"rx_traffic\",\n\t\t\tHelp: \"流量统计\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)","copilot":"prometheus.GaugeOpts{\n\t\t\tName: \"gms_kafka_consumer\",\n\t\t\t","deepseek-chat-lite":"触发方式","CodeQwen1.5-7B":" 枚举"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (\n\tStageAll       = \"all\"       //完整阶段\n\tStageExtract   = \"extract\"   //抽取\n\tStageTransform = \"transform\" //转换\n\tStageLoad      = \"load\"      //持久化\n)\n\n// 数据源source\nconst (\n\tSourcePg              = \"pg\"              // pg数据库\n\tSourceF10             = \"f10\"             // F10\n\tSourceFUsEtfAdjFactor = \"FUsEtfAdjFactor\" //美股ETF复权因子\n)\n\nvar (\n\t// 接收数据条数\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_rx_traffic\",\n\t\t\tHelp: \"count received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// qps\n\t// trigger: cron kafka\n\t// source: pg, f10\n\tqpsCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_qps\",\n\t\t\tHelp: \"request received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// 处理耗时分布\n\t// trigger: cron manual kafka\n\t// stage: extract proccess load\n\t// export: all bbrq rtime real code\n\t// source: pg, f10\n\tcostReqBucketVec = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"gms_etl_cost_histogram\",\n\t\t\tHelp:    \"cost of time for each request.\",\n\t\t\tBuckets: []float64{100, 1000, 5000, 10000, 50000, 120000, 600000},\n\t\t},\n\t\t[]string{\"db\", \"table\", \"stage\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// label_name 面板显示时公有显示\n\tnameCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_label_name\",\n\t\t\tHelp: \"show label name\",\n\t\t},\n\t\t[]string{\"db\", \"table\"},\n\t)\n\n\t// 错误数 记录接口错误,内部错误等\n\terrorCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_error\",\n\t\t\tHelp: \"errors\",\n\t\t},\n\t\t[]string{\"status\"},\n\t)\n\n\t// some metrics of kafka consumer, such as package, data flow, etc.\n\tkafkaConsumerMetrics = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{","infill":"Name: \"data\",\n","suffix":"\n\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n","relevantFile":"","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"Name: \"kafka_consumer_monitor\", Help: \"kafka consumer metrics\"},\n        []string{\"topic\", \"partition\"},\n    )","CodeQwen2.5-7B":"\n\t\t\tName: \"gms_etl_kafka_consumer\",\n\t\t\tHelp: \"kafka consumer metrics\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"type\"},\n\t)<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"Name: \"data\",\n","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个","infill":",\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\n","suffix":"\n\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"个别指标","CodeQwen2.5-7B":"指标\nvar (\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"rx_traffic\",\n\t\t\tHelp: \"流量统计\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)","copilot":",\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\n","deepseek-chat-lite":"触发方式","CodeQwen1.5-7B":" 枚举"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个","infill":"rtition\", \"task_id\"},\n\t)\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n","suffix":"\n\n\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"个指标\nvar trigTypeDict = map[int]string{\n    my_orm.TriggerAll:    \"all\",     my_orm.TriggerIncrement: \"inc\", //增量更新\n    my_orm.TriggerUpdate:   \"update\",  // ","CodeQwen2.5-7B":"指标\nvar (\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"rx_traffic\",\n\t\t\tHelp: \"流量统计\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)","copilot":"rtition\", \"task_id\"},\n\t)\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n","deepseek-chat-lite":"触发方式","CodeQwen1.5-7B":" 枚举"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (\n\tStageAll       = \"all\"       //完整阶段\n\tStageExtract   = \"extract\"   //抽取\n\tStageTransform = \"transform\" //转换\n\tStageLoad      = \"load\"      //持久化\n)\n\n// 数据源source\nconst (\n\tSourcePg              = \"pg\"              // pg数据库\n\tSourceF10             = \"f10\"             // F10\n\tSourceFUsEtfAdjFactor = \"FUsEtfAdjFactor\" //美股ETF复权因子\n)\n\nvar (\n\t// 接收数据条数\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_rx_traffic\",\n\t\t\tHelp: \"count received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// qps\n\t// trigger: cron kafka\n\t// source: pg, f10\n\tqpsCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_qps\",\n\t\t\tHelp: \"request received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// 处理耗时分布\n\t// trigger: cron manual kafka\n\t// stage: extract proccess load\n\t// export: all bbrq rtime real code\n\t// source: pg, f10\n\tcostReqBucketVec = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"gms_etl_cost_histogram\",\n\t\t\tHelp:    \"cost of time for each request.\",\n\t\t\tBuckets: []float64{100, 1000, 5000, 10000, 50000, 120000, 600000},\n\t\t},\n\t\t[]string{\"db\", \"table\", \"stage\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// label_name 面板显示时公有显示\n\tnameCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_label_name\",\n\t\t\tHelp: \"show label name\",\n\t\t},\n\t\t[]string{\"db\", \"table\"},\n\t)\n\n\t// 错误数 记录接口错误,内部错误等\n\terrorCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_error\",\n\t\t\tHelp: \"errors\",\n\t\t},\n\t\t[]string{\"status\"},\n\t)\n\n\t// some metrics of kafka consumer, such as package, data flow, etc.\n\tkafkaDataRecvPackage = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_package\",\n\t\t\tHelp: \"data recv package\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{","infill":"Name: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\t","suffix":"\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"Name: \"data_recv_flow\",\n            Help: \"data recv flow\",\n        },\n        []string{\"topic\", \"partition\", \"task_id\"},\n    )","CodeQwen2.5-7B":"\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"Name: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\t","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的","infill":",\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_task_id\",\n\t\t\tHelp: \"skip task id\",\n","suffix":"\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"Metrics","CodeQwen2.5-7B":"指标\nvar (\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"rx_traffic\",\n\t\t\tHelp: \"流量指标\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)","copilot":",\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_task_id\",\n\t\t\tHelp: \"skip task id\",\n","deepseek-chat-lite":"区别","CodeQwen1.5-7B":" 触发方式"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的","infill":"= prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_recv_task_id\",\n\t\t\tHelp: \"skip recv task id\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n","suffix":"\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n\n\t\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"指标","CodeQwen2.5-7B":"指标\nvar (\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"rx_traffic\",\n\t\t\tHelp: \"流量指标\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)","copilot":"= prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_recv_task_id\",\n\t\t\tHelp: \"skip recv task id\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n","deepseek-chat-lite":"区别","CodeQwen1.5-7B":" 触发器"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的","infill":"Help: \"skip recv task id\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)","suffix":"\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n\n\tprometheus.MustRegister(kafkaDataRecvPackage)\n\tprometheus.MustRegister(kafkaDataRecvFlow)\n\tprometheus.MustRegister(kafkaRecvSkipTaskId)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n// KafkaDataRecvPackage kafka数据包接收统计\n\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/extract/manual/request_f10.go\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\ntype DataRequestF10 struct {\n\tDatasource   int              // 数据源类型， 0：f10\n\tUrl          string           // 数据源地址\n\tFinanceTable string           // 财务数据表\n\tFinanceDb    string           // 财务数据库\n\tTriggerType  int              // 更新数据方式,manual,cron,kafka,方便指标接入\n\tReqParam     extract.ReqParam // http传入参数\n}\n\nfunc (d DataRequestF10) RequestData() (int, error) {\n\tclient := &http.Client{}\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\n\tif d.Datasource == my_orm.RequestHttpF10 {\n\t\treturn d.RequestF10Data(client, req)\n\t}\n\treturn 0, errors.New(DatasourceErr)\n}\n\nfunc (d DataRequestF10) RequestF10Data(client *http.Client, req *http.Request) (int, error) {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn 0, errors.New(DbErr)\n\t}\n\tfinanceTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"处理","CodeQwen2.5-7B":"指标\nvar (\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"rx_traffic\",\n\t\t\tHelp: \"流量统计\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)","copilot":"Help: \"skip recv task id\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)","deepseek-chat-lite":"触发方式","CodeQwen1.5-7B":" 触发器"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (","infill":"prometheus.MustRegister(kafkaDataRecvPackage)\n\tprometheus.MustRegister(kafkaDataRecvFlow)\n\tprometheus.MustRegister(kafkaRecvSkipTaskId)","suffix":"\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n// KafkaDataRecvPackage kafka数据包接收统计\nfunc KafkaDataRecvPackage(topic string, partition string, taskId string, packageNum float64) {\n\tkafkaDataRecvPackage.WithLabelValues(topic, partition, taskId).Set(packageNum)\n}\n\n\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"\n    StagePreProcess  = \"pre_process\"  // 预处理\n    StageHandleData = \"handle_data\" // 处理数据\n    StageSaveData      = \"save_data\"      // 保存数据\n    StagePush2Client     = \"push2client\"    ","CodeQwen2.5-7B":"\n\tStageInit = \"init\" // 初始化阶段\n\tStageMain = \"main\" // 主要处理阶段\n\tStageEnd  = \"end\"  // 结束阶段\n)","copilot":"prometheus.MustRegister(kafkaDataRecvPackage)\n\tprometheus.MustRegister(kafkaDataRecvFlow)\n\tprometheus.MustRegister(kafkaRecvSkipTaskId)","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (","infill":"prometheus.MustRegister(kafkaRecvSkipTaskId)","suffix":"\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n// KafkaDataRecvPackage kafka数据包接收统计\nfunc KafkaDataRecvPackage(topic string, partition string, taskId string, packageNum float64) {\n\tkafkaDataRecvPackage.WithLabelValues(topic, partition, taskId).Set(packageNum)\n}\n\n// KafkaDataRecvFlow kafka数据流量接收统计\n\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/cron/cron.go\npackage cron\n\nimport (\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/robfig/cron/v3\"\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"\n    StagePreprocess  = \"preprocess\"\n    StageHandle      = \"handle\"\n    StageSink         = \"sink\"\n    StageAfterprocess = \"afterprocess\"","CodeQwen2.5-7B":"\n\tStageInit      = \"init\"      // 初始化\n\tStageParse     = \"parse\"     // 解析\n\tStageTransform = \"transform\" // 转换\n\tStageSink      = \"sink\"      // 写入\n)","copilot":"prometheus.MustRegister(kafkaRecvSkipTaskId)","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"/","infill":"Topic, message.Partition, jsonF10.TaskId)","suffix":"\n\t\treturn\n\t}\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\tdateStr, ok := jsonF10Rsp.Data.Data[0].Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 {\n\t\td.ProcessInvalidData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t// 如果 validInt=1, 则更新相应的指标数据\n\t// 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tif taskInfo.Currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(ids, subjects, taskInfo.Currency)\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tsearchSql := fmt.Sprintf(\"SELECT isvalid FROM `%s` WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\trow, _ := load.FetchOneRowData(taskInfo.DbName, searchSql)\n\tvar isvalidField int // mysql数据库中isvalid字段\n\terr := row.Scan(&isvalidField)\n\tif err == nil {\n\t\tloadSql := fmt.Sprintf(\"INSERT INTO `%s` (zqdm,unix_time) VALUES (%s, %d) ON DUPLICATE KEY UPDATE isvalid = 0\", taskInfo.TableName, codeStr, unixTime)\n\t\t_, err := load.SinkData(taskInfo.DbName, loadSql)\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"search isvalid in local mysql, searchSql:%s, loadSql:%s, err:%s, msg:%s\", searchSql, loadSql, err.Error())\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t\t}\n\t}\n\treturn\n}\n\n// kafka 推送数据为原生数据，这里再请求一次指标数据，\nfunc (d F10KafkaTaskInfo) RequestF10Data(ids string, subjects string, currency string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      ids,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %","relevantFile":"<file_path>app/finance_info/finance_info.go\n\t\t\tSingularTable: true,\n\t\t},\n\t\tLogger: logger.Default.LogMode(logger.Silent),\n\t})\n\tif err != nil {\n\t\treturn errors.New(err.Error() + \", mysqlDsn:\" + mysqlDsn)\n\t}\n\t// 初始化缓存\n\tGlobalFinanceInfoManager = make(map[string]FinanceInfoManager)\n\t// 初始化加载DbInfo\n\terr = LoadDbInfo(orm)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, init load dbinfo\", err.Error())\n\t}\n\t// 设置热更新函数, 定时更新\n\tcallback := func() {\n\t\terr := LoadDbInfo(orm)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Warn(\"load db-info cron\", zap.String(\"error\", err.Error()))\n\t\t}\n\t}\n\terr = cron.GCronManager.AddTask(\"load_dbinfo_cron\", \"* * * * *\", callback)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s, add task failed, fun:load dbinfo\", err.Error())\n\t}\n\treturn nil\n}\n\n// LoadDbInfo 从mysql中读取配置,独立为函数方便字段热更新\nfunc LoadDbInfo(orm *gorm.DB) error {\n\tfor _, dbname := range config.GetMysql().SinkDb {\n\t\tfinanceInfoManager := FinanceInfoManager{\n\t\t\tdbName:           dbname,\n\t\t\tTableBasic:       make(map[string]my_orm.FinanceBasic),\n\t\t\tTableFieldName:   make(map[string]my_orm.FieldInfo),\n\t\t\tMapFieldTable:    make(map[string][]my_orm.FieldInfo),\n\t\t\tMapTaskIndicator: make(map[string]my_orm.ExtractF10Indicator),\n\t\t\tMapTableTask:     make(map[string][]string),\n\t\t}\n\t\t// 获取财务文件基础信息\n\t\tvar financeInfo []my_orm.FinanceBasic\n\t\torm.Table(\"FinanceBasic\").Where(FinanceDbEqual, dbname).Find(&financeInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range financeInfo {\n\t\t\tfinanceInfoManager.TableBasic[v.FinanceTable] = v\n\t\t\tmetrics.NameMetricsInc(dbname, v.FinanceTable) // 记录程序中所有的库表,用来看板上方选择\n\t\t}\n\t\t// 获取财务文件字段描述\n\t\tvar fieldInfo []my_orm.FieldInfo\n\t\torm.Table(\"FieldInfo\").Where(FinanceDbEqual, dbname).Find(&fieldInfo)\n\t\tif orm.Error != nil {\n\t\t\treturn orm.Error\n\t\t}\n\t\tfor _, v := range fieldInfo {\n\t\t\tfinanceInfoManager.TableFieldName[v.FieldSrcName] = v\n\t\t\t_, ok := financeInfoManager.MapFieldTable[v.FinanceTable]\n\t\t\tif !ok {\n\t\t\t\tfinanceInfoManager.MapFieldTable[v.FinanceTable] = make([]my_orm.FieldInfo, 0)\n<file_path>app/extract/automatic/update_kafka.go\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype KafkaConsumer struct {\n\tConfig        kafka_dao.Config\n\tTask          KafkaTaskInfo\n\tconsumer      kafka_dao.Consumer\n\tconsumerGroup sarama.ConsumerGroup\n\tctx           context.Context\n\tcancel        context.CancelFunc\n\tchErr         chan error\n}\n\nfunc (d KafkaConsumer) Start() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\tlevellog.Log.Info(\"No need to start kafka consume\")\n\t\treturn 0, nil\n\t}\n\tlevellog.Log.Info(\"start init kafka consumer\", zap.Strings(\"brokers\", d.Config.Brokers),\n\t\tzap.String(\"username\", d.Config.Username), zap.String(\"password\", d.Config.Password),\n\t\tzap.String(\"consumer_group\", d.Config.ConsumerGroup), zap.String(\"SecurityProtocol\", d.Config.SecurityProtocol),\n\t\tzap.String(\"SaslMechanism\", d.Config.SaslMechanism), zap.String(\"topic\", d.Task.GetTopic()))\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) Stop() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\treturn 0, nil\n\t}\n\td.cancel()\n\treturn 0, nil\n}\n<file_path>app/extract_manager/extract_manager.go\n\t}\n\t// 任务配置\n\tF10Task := automatic.F10KafkaTaskInfo{\n\t\tTopic:              config.GetF10().Topic,\n\t\tUrl:                config.GetF10().Url,\n\t\tUnderlyingAssetUrl: config.GetF10().UnderlyingAssetUrl,\n\t}\n\tF10Task.MapTaskProcess = make(map[string]automatic.F10KafkaTask)\n\tF10Task.MapTaskIdIndicators = make(map[string]string)\n\tfor database, detail := range finance_info.GlobalFinanceInfoManager {\n\t\tfor k, v := range detail.TableBasic {\n\t\t\tlevellog.Log.Info(\"Init f10 kafka task\", zap.String(\"database\", database),\n\t\t\t\tzap.String(\"task\", v.TaskIds), zap.String(\"table\", k))\n\n\t\t\tmarket := detail.TableBasic[k].Market\n\t\t\ttaskIds := strings.Split(v.TaskIds, \",\") // taskids 有多个 taskid，每个 taskid 对应多个指标\n\t\t\tfor _, taskId := range taskIds {\n\t\t\t\tif taskId == \"\" {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\ttaskIndicators, ok := detail.MapTaskIndicator[taskId]\n\t\t\t\tif ok == false {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tmapOtherName := make(map[string]string)\n\t\t\t\tidList := strings.Split(taskIndicators.Ids, \",\")\n\t\t\t\tmapTmp := detail.TableFieldName\n\t\t\t\tfor _, val := range idList {\n\t\t\t\t\tdestName, ok := mapTmp[val]\n\t\t\t\t\tif ok == false {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tmapOtherName[val] = destName.FieldName // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\t\t\t\t\tlevellog.Log.Info(\"Init F10 kafka field transfer\", zap.String(\"taskid\", v.TaskIds),\n\t\t\t\t\t\tzap.String(\"src_field\", val), zap.String(\"dest_field\", destName.FieldName))\n\t\t\t\t}\n\n\t\t\t\tF10Task.MapTaskProcess[taskId+\"_\"+market] = automatic.F10KafkaTask{\n\t\t\t\t\tTaskName:    taskId,\n\t\t\t\t\tDbName:      database,\n\t\t\t\t\tTableName:   k,\n\t\t\t\t\tCurrency:    detail.TableBasic[k].Currency,\n\t\t\t\t\tMarketType:  detail.TableBasic[k].MarketType,\n\t\t\t\t\tMapItemName: mapOtherName,\n\t\t\t\t\tDate:        taskIndicators.DateType,\n\t\t\t\t\tHadReport:   taskIndicators.HadReport,\n\t\t\t\t}\n\n\t\t\t\t// 对于多个市场使用同一个 taskid的情况下，这里只需要取第一个市场的 taskid，因为同一个 taskid 对应的指标是相同的；\n\t\t\t\tif _, ok := F10Task.MapTaskIdIndicators[taskId]; !ok {\n\t\t\t\t\tF10Task.MapTaskIdIndicators[taskId] = taskIndicators.Ids\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif len(F10Task.MapTaskProcess) == 0 {\n\t\tF10Task.Topic = \"\"\n\t}\n\t// 生成消费者\n<file_path>app/extract/manual/request_f10.go\n\tvar jsonF10Rsp my_orm.F10Responses\n\t// 解析应答\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"read f10 body failed, err: %s, data: %s\", err.Error(), string(body))\n\t\treturn 0, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"unmarshal f10 failed, err: %s, data: %s\", err.Error(), string(body))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn 0, err\n\t}\n\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\terr = fmt.Errorf(\"rsp body is invalid in manual request f10, data: %s\", string(body))\n\t\treturn 0, err\n\t}\n\n\t// 这里将代码转换为同花顺代码，便于后续处理\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\trecordCnt, err := d.ParseF10ResponseData(thsCode, ids, hadReport, date, jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"parse f10 rsp body failed\", zap.Error(err))\n\t\treturn 0, err\n\t}\n\n\treturn recordCnt, nil\n}","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"/ 功能: 更新数据库kafka f10的数据","CodeQwen2.5-7B":"/ Copyright 2021 The Fuchsia Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the LICENSE file.","copilot":"Topic, message.Partition, jsonF10.TaskId)","deepseek-chat-lite":"/ 获取同花顺代码","CodeQwen1.5-7B":" *"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更","infill":"flow)","suffix":"\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, partition, taskId).Inc()\n}\n","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/cron/cron.go\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/extract/manual/request_f10.go\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n\tto := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.EndTime/10000, d.ReqParam.EndTime%10000/100, d.ReqParam.EndTime%100)\n\trequestBody := my_orm.F10Request{\n\t\tCurrency: financeTable.Currency,\n\t\tCategory: \"indicator\",\n\t\tStrategy: d.ReqParam.Strategy,\n\t\tFrom:     from,\n\t\tTo:       to,\n\t}\n\tmarket := financeTable.Market\n\tcodes, err := GetCodelist(d.FinanceDb, d.FinanceTable)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"req codelist failed\", zap.Error(err))\n\t\treturn 0, err\n\t}\n\trecordTotal := 0\n\t// 由于存在全量请求的情况，数据量较大，所以每次只请求一支代码\n\tfor _, code := range codes {\n\t\tlevellog.Log.Debug(\"code info\", zap.Int(\"submarket\", code.Submarket), zap.String(\"code\", code.Code))\n\t\tif strings.Contains(code.Code, \"_\") { // 过滤掉含有特殊符号的代码\n\t\t\tcontinue\n\t\t}\n\t\tsubjects := fmt.Sprintf(\"%s-%s\", market, code.Code)\n\t\trequestBody.Subjects = subjects\n\t\tfor _, taskId := range tableF10Task {\n\t\t\trequestBody.Ids = dbInfo.MapTaskIndicator[taskId].Ids\n\t\t\tval, _ := query.Values(requestBody)\n\t\t\treq.URL.RawQuery = val.Encode()\n\t\t\tresp, err := httpGetWithRetry(client, req, 3)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Warn(\"manual request f10 data, request failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\trecordCnt, err := d.ProcessF10RequestData(resp, market, requestBody.Ids, dbInfo.MapTaskIndicator[taskId].HadReport, dbInfo.MapTaskIndicator[taskId].DateType)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Info(\"manual request F10 data, process failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trecordTotal = recordTotal + recordCnt\n\t\t}\n\t}\n\tmetrics.TrafficMetricsAdd(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10, recordTotal)\n\t// f10持久化耗时记录\n\tmetrics.CostBucketMetricsObserve(d.FinanceDb, d.FinanceTable, metrics.StageAll, triggerType, exportType, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\tlevellog.Log.Info(\"f10 sinkdata affected:\", zap.String(\"db\", d.FinanceDb), zap.String(\"table\", d.FinanceTable), zap.Int(\"rows:\", recordTotal))\n\treturn recordTotal, nil\n}\n\nfunc (d DataRequestF10) ProcessF10RequestData(resp *http.Response, market string, ids string, hadReport int, date string) (int, error) {","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"新数据同步的股票数量gauge","CodeQwen2.5-7B":"新导出\nvar exportTypeDict2 = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n}","copilot":"flow)","deepseek-chat-lite":"新","CodeQwen1.5-7B":" 新任务统计"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更","infill":"packageNum)","suffix":"\n}\n\n// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition int32, taskId string, flow float64) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, partition, taskId).Add(flow)\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, \n}\n","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/cron/cron.go\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/extract/manual/request_f10.go\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n\tto := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.EndTime/10000, d.ReqParam.EndTime%10000/100, d.ReqParam.EndTime%100)\n\trequestBody := my_orm.F10Request{\n\t\tCurrency: financeTable.Currency,\n\t\tCategory: \"indicator\",\n\t\tStrategy: d.ReqParam.Strategy,\n\t\tFrom:     from,\n\t\tTo:       to,\n\t}\n\tmarket := financeTable.Market\n\tcodes, err := GetCodelist(d.FinanceDb, d.FinanceTable)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"req codelist failed\", zap.Error(err))\n\t\treturn 0, err\n\t}\n\trecordTotal := 0\n\t// 由于存在全量请求的情况，数据量较大，所以每次只请求一支代码\n\tfor _, code := range codes {\n\t\tlevellog.Log.Debug(\"code info\", zap.Int(\"submarket\", code.Submarket), zap.String(\"code\", code.Code))\n\t\tif strings.Contains(code.Code, \"_\") { // 过滤掉含有特殊符号的代码\n\t\t\tcontinue\n\t\t}\n\t\tsubjects := fmt.Sprintf(\"%s-%s\", market, code.Code)\n\t\trequestBody.Subjects = subjects\n\t\tfor _, taskId := range tableF10Task {\n\t\t\trequestBody.Ids = dbInfo.MapTaskIndicator[taskId].Ids\n\t\t\tval, _ := query.Values(requestBody)\n\t\t\treq.URL.RawQuery = val.Encode()\n\t\t\tresp, err := httpGetWithRetry(client, req, 3)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Warn(\"manual request f10 data, request failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\trecordCnt, err := d.ProcessF10RequestData(resp, market, requestBody.Ids, dbInfo.MapTaskIndicator[taskId].HadReport, dbInfo.MapTaskIndicator[taskId].DateType)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Info(\"manual request F10 data, process failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trecordTotal = recordTotal + recordCnt\n\t\t}\n\t}\n\tmetrics.TrafficMetricsAdd(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10, recordTotal)\n\t// f10持久化耗时记录\n\tmetrics.CostBucketMetricsObserve(d.FinanceDb, d.FinanceTable, metrics.StageAll, triggerType, exportType, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\tlevellog.Log.Info(\"f10 sinkdata affected:\", zap.String(\"db\", d.FinanceDb), zap.String(\"table\", d.FinanceTable), zap.Int(\"rows:\", recordTotal))\n\treturn recordTotal, nil\n}\n\nfunc (d DataRequestF10) ProcessF10RequestData(resp *http.Response, market string, ids string, hadReport int, date string) (int, error) {","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"新数据源数据同步状态","CodeQwen2.5-7B":"新导出\nvar exportTypeDict2 = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出","copilot":"packageNum)","deepseek-chat-lite":"新方式","CodeQwen1.5-7B":" 新数据统计"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出","infill":"float64(flow))","suffix":"\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Inc()\n}\n","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/cron/cron.go\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/extract/manual/request_f10.go\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n\tto := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.EndTime/10000, d.ReqParam.EndTime%10000/100, d.ReqParam.EndTime%100)\n\trequestBody := my_orm.F10Request{\n\t\tCurrency: financeTable.Currency,\n\t\tCategory: \"indicator\",\n\t\tStrategy: d.ReqParam.Strategy,\n\t\tFrom:     from,\n\t\tTo:       to,\n\t}\n\tmarket := financeTable.Market\n\tcodes, err := GetCodelist(d.FinanceDb, d.FinanceTable)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"req codelist failed\", zap.Error(err))\n\t\treturn 0, err\n\t}\n\trecordTotal := 0\n\t// 由于存在全量请求的情况，数据量较大，所以每次只请求一支代码\n\tfor _, code := range codes {\n\t\tlevellog.Log.Debug(\"code info\", zap.Int(\"submarket\", code.Submarket), zap.String(\"code\", code.Code))\n\t\tif strings.Contains(code.Code, \"_\") { // 过滤掉含有特殊符号的代码\n\t\t\tcontinue\n\t\t}\n\t\tsubjects := fmt.Sprintf(\"%s-%s\", market, code.Code)\n\t\trequestBody.Subjects = subjects\n\t\tfor _, taskId := range tableF10Task {\n\t\t\trequestBody.Ids = dbInfo.MapTaskIndicator[taskId].Ids\n\t\t\tval, _ := query.Values(requestBody)\n\t\t\treq.URL.RawQuery = val.Encode()\n\t\t\tresp, err := httpGetWithRetry(client, req, 3)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Warn(\"manual request f10 data, request failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\trecordCnt, err := d.ProcessF10RequestData(resp, market, requestBody.Ids, dbInfo.MapTaskIndicator[taskId].HadReport, dbInfo.MapTaskIndicator[taskId].DateType)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Info(\"manual request F10 data, process failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trecordTotal = recordTotal + recordCnt\n\t\t}\n\t}\n\tmetrics.TrafficMetricsAdd(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10, recordTotal)\n\t// f10持久化耗时记录\n\tmetrics.CostBucketMetricsObserve(d.FinanceDb, d.FinanceTable, metrics.StageAll, triggerType, exportType, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\tlevellog.Log.Info(\"f10 sinkdata affected:\", zap.String(\"db\", d.FinanceDb), zap.String(\"table\", d.FinanceTable), zap.Int(\"rows:\", recordTotal))\n\treturn recordTotal, nil\n}\n\nfunc (d DataRequestF10) ProcessF10RequestData(resp *http.Response, market string, ids string, hadReport int, date string) (int, error) {","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n}","copilot":"float64(flow))","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出","infill":"float64(packageNum))","suffix":"\n}\n\n// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition int32, taskId string, flow int) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(flow))\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Inc()\n}\n","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n<file_path>app/cron/cron.go\n)\n\n// 本接口非线程安全，注意使用场景\n// 全局定时任务\nvar GCronManager CronManager\n\ntype CronEntryInfo map[string]cron.EntryID\n\ntype CronManager struct {\n\tcron           *cron.Cron\n\tscheduleDetail map[string]CronEntryInfo\n}\n\nvar cronTaskMetrics = prometheus.NewGaugeVec(\n\tprometheus.GaugeOpts{\n\t\tName: \"gms_cron_task\",\n\t\tHelp: \"Cron task\",\n\t},\n\t[]string{\"task\", \"schedule\"},\n)\n\nfunc (d *CronManager) Start() {\n\tprometheus.MustRegister(cronTaskMetrics)\n\n\tloc, _ := time.LoadLocation(\"Asia/Shanghai\")\n\td.cron = cron.New(cron.WithLocation(loc))\n\td.scheduleDetail = make(map[string]CronEntryInfo)\n\td.cron.Start()\n}\n\nfunc (d *CronManager) Stop() {\n\td.cron.Stop()\n}\n\nfunc (d *CronManager) AddTask(task string, schedule string, cronFunc func()) error {\n\t_, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\td.scheduleDetail[task] = make(CronEntryInfo)\n\t}\n\t_, ok = d.scheduleDetail[task][schedule]\n\tif ok {\n\t\treturn nil\n\t}\n\tid, err := d.cron.AddFunc(schedule, cronFunc)\n\tif err != nil {\n\t\treturn err\n\t}\n\td.scheduleDetail[task][schedule] = id\n\n\tcronTaskMetrics.WithLabelValues(task, schedule).SetToCurrentTime()\n\treturn nil\n}\n\nfunc (d *CronManager) RemoveTask(task string) {\n\ttaskDetail, ok := d.scheduleDetail[task]\n\tif !ok {\n\t\treturn\n\t}\n\tfor s, v := range taskDetail {\n\t\td.cron.Remove(v)\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\n<file_path>app/extract/manual/request_f10.go\n\ttableF10Task, ok := dbInfo.MapTableTask[d.FinanceTable]\n\tif !ok {\n\t\treturn 0, errors.New(TableErr)\n\t}\n\n\texportType := metrics.AllExport // 这种情况\n\ttriggerType := metrics.GetTriggerType(d.TriggerType)\n\tmetrics.QpsMetricsInc(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10)\n\tstartExtractTime := time.Now()\n\n\tfrom := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.StartTime/10000, d.ReqParam.StartTime%10000/100, d.ReqParam.StartTime%100)\n\tto := fmt.Sprintf(\"%02d-%02d-%02d\", d.ReqParam.EndTime/10000, d.ReqParam.EndTime%10000/100, d.ReqParam.EndTime%100)\n\trequestBody := my_orm.F10Request{\n\t\tCurrency: financeTable.Currency,\n\t\tCategory: \"indicator\",\n\t\tStrategy: d.ReqParam.Strategy,\n\t\tFrom:     from,\n\t\tTo:       to,\n\t}\n\tmarket := financeTable.Market\n\tcodes, err := GetCodelist(d.FinanceDb, d.FinanceTable)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"req codelist failed\", zap.Error(err))\n\t\treturn 0, err\n\t}\n\trecordTotal := 0\n\t// 由于存在全量请求的情况，数据量较大，所以每次只请求一支代码\n\tfor _, code := range codes {\n\t\tlevellog.Log.Debug(\"code info\", zap.Int(\"submarket\", code.Submarket), zap.String(\"code\", code.Code))\n\t\tif strings.Contains(code.Code, \"_\") { // 过滤掉含有特殊符号的代码\n\t\t\tcontinue\n\t\t}\n\t\tsubjects := fmt.Sprintf(\"%s-%s\", market, code.Code)\n\t\trequestBody.Subjects = subjects\n\t\tfor _, taskId := range tableF10Task {\n\t\t\trequestBody.Ids = dbInfo.MapTaskIndicator[taskId].Ids\n\t\t\tval, _ := query.Values(requestBody)\n\t\t\treq.URL.RawQuery = val.Encode()\n\t\t\tresp, err := httpGetWithRetry(client, req, 3)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Warn(\"manual request f10 data, request failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\trecordCnt, err := d.ProcessF10RequestData(resp, market, requestBody.Ids, dbInfo.MapTaskIndicator[taskId].HadReport, dbInfo.MapTaskIndicator[taskId].DateType)\n\t\t\tif err != nil {\n\t\t\t\tlevellog.Log.Info(\"manual request F10 data, process failed\", zap.String(\"task\", taskId), zap.String(\"subjects\", subjects), zap.String(\"ReqArg\", req.URL.RawQuery), zap.Error(err))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trecordTotal = recordTotal + recordCnt\n\t\t}\n\t}\n\tmetrics.TrafficMetricsAdd(d.FinanceDb, d.FinanceTable, triggerType, exportType, metrics.SourceF10, recordTotal)\n\t// f10持久化耗时记录\n\tmetrics.CostBucketMetricsObserve(d.FinanceDb, d.FinanceTable, metrics.StageAll, triggerType, exportType, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\tlevellog.Log.Info(\"f10 sinkdata affected:\", zap.String(\"db\", d.FinanceDb), zap.String(\"table\", d.FinanceTable), zap.Int(\"rows:\", recordTotal))\n\treturn recordTotal, nil\n}\n\nfunc (d DataRequestF10) ProcessF10RequestData(resp *http.Response, market string, ids string, hadReport int, date string) (int, error) {","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n}","copilot":"float64(packageNum))","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) GetF10DataByRequest(f10TaskId string, indicatorId string, subjects string, date string) (my_orm.F10Responses, error) {\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorId, subjects, currency)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn jsonF10Rsp, err\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn jsonF10Rsp, nil\n\t}\n\n\t// 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorId, subjects, currency)\n\t}\n\n\treturn jsonF10Rsp, nil\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, currency)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(ids, subjects, currency)\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\tdateStr, ok := jsonF10Rsp.Data.Data[0].Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// kafka 推送数据为原生数据，这里再请求一次指标数据，\n// indicatorIDs: 指标ID列表\n// subjects: 股票。\n// currency: 一个字符串，表示货币类型。\n// date: 一个字符串，表示日期。\n// 该函数返回一个F10Responses对象和一个错误。如果请求成功，错误将为nil。\n// 如果请求失败，错误将包含失败的详细信息。\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs []string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, er","infill":"iod\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date","suffix":"\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"err := http.NewRequest(http.MethodGet, d.Url, nil)\n    if err != nil {\n        levellog.Log.Error(\"new http req f10 failed in kafka thread\", zap.Error(err))\n        metrics.ErrorMetricsIn","CodeQwen2.5-7B":"r := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"new http req f10 failed in kafka thread\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\trequestBody := map[string]interface{}{\n\t\t\"indicator_ids\": indicatorIDs,\n\t\t\"subjects\":      []string{subjects},\n\t\t\"currency\":      currency,\n\t\t\"date\":          date,","copilot":"iod\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date","deepseek-chat-lite":"r := http.NewRequest(\"GET\", d.Url, nil)","CodeQwen1.5-7B":" r := http.NewRequest(\"GET\", d.Url, nil)"}},{"prefix":"package main\n\nimport (\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"os/exec\"\n\t\"strings\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\t// Execute 'docker ps -a' command\n\tout, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Split the output into lines\n\tlines := strings.Split(string(out), \"\\n\")\n\n\tlog.Println(","infill":"lines","suffix":")\n\n\t// Parse the lines into a list of Container objects\n\tcontainers := []Container{}\n\tfor _, line := range lines[1:] { // Skip the header line\n\t\tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t\t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t\t})\n\t\tlog.Println(fields)\n\t\tif len(fields) > 0 {\n\t\t\tcontainers = append(containers, Container{\n\t\t\t\tID:     fields[0],\n\t\t\t\tImage:  fields[1],\n\t\t\t\tName:   fields[2],\n\t\t\t\tStatus: strings.ToUpper(fields[3]),\n\t\t\t\t// Ports:   fields[4],\n\t\t\t\tLastStarted: fields[4],\n\t\t\t})\n\t\t}\n\t}\n\treturn containers, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"lines","CodeQwen2.5-7B":"lines","copilot":"lines","deepseek-chat-lite":"lines","CodeQwen1.5-7B":"string(out)"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCu","infill":"rrency  string           `json:\"currency\"`","suffix":"\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\t\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsV","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"rrency   string           `json:\"currency\"`","CodeQwen2.5-7B":"rrency  string           `json:\"currency\"`\n\tIndicator string           `json:\"indicator\"`\n\tSubject   my_orm.Subject   `json:\"subject\"`\n\tData      []map[string]any `json:\"data\"`\n}","copilot":"rrency  string           `json:\"currency\"`","deepseek-chat-lite":"rrency  string           `json:\"currency\"`","CodeQwen1.5-7B":" rrency  string           `json:\"currency\"`"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}","infill":"// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicat","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"currency = taskInfo.Currency\n    // 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n    if currency != \"CNY\" {\n        jsonF10Rsp, err = d.RequestF","CodeQwen2.5-7B":"","copilot":"// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (IndicatorInfoFromF10, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn IndicatorInfoFromF10{}, err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn IndicatorInfoFromF10{\n\t\t\tMarket:    market,\n\t\t\tCode:      thsCode,\n\t\t\tCurrency:  currency,\n\t\t\tDate:      assetData.Data[0][taskInfo.Date],","infill":"AssetData: assetData,","suffix":"\n\t\t}, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixti","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":" // 这只是本地数据库中配置的日期字段,并不是f10返回的日期字段, f10返回的日期字段在map中配置\n            AssetData: assetData,","CodeQwen2.5-7B":"\n\t\t\tAssetData: assetData,\n\t\t}, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}","copilot":"AssetData: assetData,","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (IndicatorInfoFromF10, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn IndicatorInfoFromF10{}, err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\n\tindicatorInfoFromF10 := IndicatorInfoFromF10{\n\t\tMarket:    market,\n\t\tCode:      thsCode,\n\t\tCurrency:  currency,\n\t\tDate:      assetData.Data[0][taskInfo.Date],\n\t\tAssetData: assetData,\n\t}\n\n\tif !ok {\n\t\treturn indicatorInfoFromF10, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn indicatorInfoFromF10, err\n\t\t}\n\n\t}\n\tindicatorInfoFromF10.Currency = currency\n\tindicatorInfoFromF10.AssetData = jsonF10Rsp.Data.Data[0]\n\treturn indicatorInfoFromF10, err\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\tvar indicatorInfoFromF10","infill":" IndicatorInfoFromF10\n\tindicatorInfoFromF10, err = d.GetAssetDataByRequestF10(jsonF10.TaskId, jsonF10.Data[\"indicator_id\"], jsonF10.Data[\"org_id\"])","suffix":"\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Enco","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":" IndicatorInfoFromF10\n\tindicatorInfoFromF10, err = d.GetAssetDataByRequestF10(jsonF10.TaskId, jsonF10.IndicatorIds, jsonF10.Subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", jsonF10.Subjects))\n\t\treturn\n\t}<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":" IndicatorInfoFromF10\n\tindicatorInfoFromF10, err = d.GetAssetDataByRequestF10(jsonF10.TaskId, jsonF10.Data[\"indicator_id\"], jsonF10.Data[\"org_id\"])","deepseek-chat-lite":" IndicatorInfoFromF10","CodeQwen1.5-7B":"  IndicatorInfoFromF10"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (IndicatorInfoFromF10, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn IndicatorInfoFromF10{}, err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\n\tindicatorInfoFromF10 := IndicatorInfoFromF10{\n\t\tMarket:    market,\n\t\tCode:      thsCode,\n\t\tCurrency:  currency,\n\t\tDate:      assetData.Data[0][taskInfo.Date],\n\t\tAssetData: assetData,\n\t}\n\n\tif !ok {\n\t\treturn indicatorInfoFromF10, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn indicatorInfoFromF10, err\n\t\t}\n\n\t}\n\tindicatorInfoFromF10.Currency = currency\n\tindicatorInfoFromF10.AssetData = jsonF10Rsp.Data.Data[0]\n\treturn indicatorInfoFromF10, err\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif or","infill":"gId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok","suffix":" && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\tvar indicatorInfoFromF10 IndicatorInfoFromF10\n\tindicatorInfoFromF10, err = d.GetAssetDataByRequestF10(jsonF10.TaskId, jsonF10.Data[\"ids\"], subjects)\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\\","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"gId, ok := jsonF10.Data[\"orgId\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"secId\"]; ok<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"gId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok","deepseek-chat-lite":"gId, ok := jsonF10.Data[\"orgId\"]; ok && orgId != \"\" {","CodeQwen1.5-7B":" gId, ok := jsonF10.Data[\"orgId\"]; ok && orgId != \"\" {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (IndicatorInfoFromF10, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn IndicatorInfoFromF10{}, err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\n\tindicatorInfoFromF10 := IndicatorInfoFromF10{\n\t\tMarket:    market,\n\t\tCode:      thsCode,\n\t\tCurrency:  currency,\n\t\tDate:      assetData.Data[0][taskInfo.Date],\n\t\tAssetData: assetData,\n\t}\n\n\tif !ok {\n\t\treturn indicatorInfoFromF10, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn indicatorInfoFromF10, err\n\t\t}\n\n\t}\n\tindicatorInfoFromF10.Currency = currency\n\tindicatorInfoFromF10.AssetData = jsonF10Rsp.Data.Data[0]\n\treturn indicatorInfoFromF10, err\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\tvar indicatorInfoFromF10 IndicatorInfoFromF10\n\tindicatorInfoFromF10, err = d.GetAssetDataByRequestF10(jsonF10.TaskId, ","infill":"jsonF10.Data[\"ids\"], subjects)","suffix":"\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"indicatorInfoFromF10.AssetData.Subject.Code, subjects)<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"jsonF10.Data[\"ids\"], subjects)","deepseek-chat-lite":"jsonF10.TaskId, subjects)","CodeQwen1.5-7B":" indicatorIds, subjects)"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\n\ntype ThsMarketCode ","infill":"struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromF10(subjects string) (ThsMarketCode, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(\"\", subjects, currency, \"\")\n\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market","suffix":"\n\n\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (IndicatorInfoFromF10, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn IndicatorInfoFromF10{}, err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\n\tindicatorInfoFromF10 := IndicatorInfoFromF10{\n\t\tMarket:    market,\n\t\tCode:      thsCode,\n\t\tCurrency:  currency,\n\t\tAssetData: assetData,\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn indicatorInfoFromF10, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\tif ()\n\n\n\tindicatorInfoFromF10.Date\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn indicatorInfoFromF10, err\n\t\t}\n\n\t}\n\tindicatorInfoFromF10.Currency = currency\n\tindicatorInfoFromF10.AssetData = jsonF10Rsp.Data.Data[0]\n\treturn indicatorInfoFromF10, err\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"struct {\n    Market string `json:\"market\"`\n    Code   string `json:\"code\"`\n}","CodeQwen2.5-7B":"struct {\n\tMarket string `json:\"market\"`\n\tCode   string `json:\"code\"`\n}","copilot":"struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromF10(subjects string) (ThsMarketCode, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(\"\", subjects, currency, \"\")\n\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market","deepseek-chat-lite":"struct {","CodeQwen1.5-7B":" struct {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\n\ntype ThsMarketCode struct {\n\tMarket string","infill":"Code   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromF10\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr","suffix":" = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskI","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"`json:\"market\"`\n}\n\n// ProcessKafkaMessage 处理kafka消息\nfunc (d F10KafkaTaskInfo) ProcessKafkaMessage(message *sarama.ConsumerMessage) {\n    var jsonF10 transform.JsonF10","CodeQwen2.5-7B":"\n\tCode   string\n}","copilot":"Code   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromF10\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr","deepseek-chat-lite":" `json:\"market\"`","CodeQwen1.5-7B":"  `json:\"market\"`"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string) (ThsMarketCode, error) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn ThsMarketCode{}, \n\t}","infill":"// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, ids, subjects, \"\")\n\n}","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAs","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"thsCode := ThsMarketCode{Market: ids[0], Code: ids[1]}\n    return thsCode, nil","CodeQwen2.5-7B":"\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\t// 获取推送指标的市场\n\tthsCode := \"\"\n\tif market, ok := indicatorInfoFromF10.Data[0][\"market\"]; ok && market != \"\" {\n\t\tthsCode = market\n\t} else {\n\t\tlevellog.Log.Info(\"market not found in kafka push msg\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, errors.New(\"market not found in kafka push msg\")\n\t}\n\treturn ThsMarketCode{Market: thsCode, Code: subjects}, nil\n}<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, ids, subjects, \"\")\n\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string) (ThsMarketCode, error) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn ThsMarketCode{}, \n\t}\n\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data","infill":"(f10TaskId, ids, subjects, \"\")\n\n}","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAs","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"(ids, subjects, \"\", \"\")\n    if err != nil {\n        levellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n        return ThsMarket","CodeQwen2.5-7B":"(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}","copilot":"(f10TaskId, ids, subjects, \"\")\n\n}","deepseek-chat-lite":"(ids, subjects, d.Currency, d.Date)","CodeQwen1.5-7B":" (ids, subjects)"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")","infill":"if err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t","suffix":"\n\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderly","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":" // 这里要请求的是最新数据接口（可能需要根据indicators筛选字段）\n    if err != nil {\n        return ThsMarketCode{}, err\n    }\n    // 从响应结果中筛选出指标对应的证券","CodeQwen2.5-7B":"\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\t// 获取推送指标的市场\n\tindicatorInfoFromF10, ok := jsonF10Rsp.Data.Data[0]\n\tif !ok {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\t// 获取推送指标的市场\n\tthsCode, ok := indicatorInfoFromF10[\"ths_code\"]\n\tif !ok {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\treturn ThsMarketCode{Market: \"SH\", Code: thsCode}, nil\n}","copilot":"if err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif ","infill":"err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}","suffix":"\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGe","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"err != nil {\n        levellog.Log.Info(\"get market and code from f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n        metrics.ErrorMetricsInc(metrics.ErrRequest","CodeQwen2.5-7B":"err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}","deepseek-chat-lite":"err != nil {","CodeQwen1.5-7B":" err != nil {"}},{"prefix":"package main\n\nimport (\n\t\"fmt\"\n\t\"net/http\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\n// 定义处理器函数\nfunc handler(w http.ResponseWriter, r *http.Request) {\n\tfmt.Fprintf(w, \"Hello, you've requested: %s\\n\", r.URL.Path)\n}\n\nfunc main() {\n\t// // 将处理器函数绑定到路由\n\t// http.HandleFunc(\"/\", handler)\n\n\t// // 启动服务器\n\t// http.ListenAndServe(\":8080\", nil)\n\n\tr := gin.Default","infill":"()\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(200, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.Run() // listen and serve on","suffix":"\n}\n","relevantFile":"","relevantFileList":[],"filePath":"main.go","template":"go","multiRes":{"hipilot":"()\n    r.GET(\"/ping\", func(c *gin.Context) {\n        c.JSON(200, gin.H{\n            \"message\": \"pong\",\n        })\n    })\n    r.Run() // 监听并在 0.0.","CodeQwen2.5-7B":"()\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(200, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\tr.Run()","copilot":"()\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(200, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.Run() // listen and serve on","deepseek-chat-lite":"()","CodeQwen1.5-7B":"()"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, )\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFou\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"no","infill":"t_found_task_info","suffix":"\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, recvDate)\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// ","relevantFile":"<file_path>app/metrics/metrics.go\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"noFoundTask","CodeQwen2.5-7B":"t found<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"t_found_task_info","deepseek-chat-lite":"tFound\")","CodeQwen1.5-7B":" t found\")"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, ","infill":"\"NotFoundIndicatorIDFromConfig\"","suffix":")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, recvDate)\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo","relevantFile":"<file_path>app/metrics/metrics.go\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"\"NotFoundTaskIdFromConfig\"","CodeQwen2.5-7B":"\"NotFoundTaskIdFromConfig\"<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"\"NotFoundIndicatorIDFromConfig\"","deepseek-chat-lite":"\"NotFoundIndicatorIdFromConfig\")","CodeQwen1.5-7B":" \"NotFoundIndicatorFromConfig\")"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tif ","infill":"validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\treturn\n\n\t}","suffix":"\n\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, recvDate)\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]str","relevantFile":"<file_path>app/metrics/metrics.go\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\treturn\n\t}<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\treturn\n\n\t}","deepseek-chat-lite":"validInt == 0 {","CodeQwen1.5-7B":" validInt == 0 {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tif validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetInvalidZeroFromRecvMsg(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\n\t}\n\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIDs, subjects, currency, recvDate)\n\tif err != nil {","infill":"levellog.Log.Info(\"request indica\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn","suffix":"\n\t}\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range","relevantFile":"<file_path>app/metrics/metrics.go\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"RequestF10ApiFailed\")\n\t\treturn\n\t}","copilot":"levellog.Log.Info(\"request indica\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tif validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetInvalidZeroFromRecvMsg(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\n\t}\n\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIDs, subjects, currency, recvDate)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request indicator data from f10 b","infill":"zap.String(\"date\", recvDate))","suffix":"\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"RequestF10ApiFailed\")\n\t\treturn\n\t}\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData ","relevantFile":"<file_path>app/metrics/metrics.go\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"y taskid failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"zap.String(\"date\", recvDate))","deepseek-chat-lite":"ase on f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))","CodeQwen1.5-7B":" y taskid failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (\n\tStageAll       = \"all\"       //完整阶段\n\tStageExtract   = \"extract\"   //抽取\n\tStageTransform = \"transform\" //转换\n\tStageLoad      = \"load\"      //持久化\n)\n\n// 数据源source\nconst (\n\tSourcePg              = \"pg\"              // pg数据库\n\tSourceF10             = \"f10\"             // F10\n\tSourceFUsEtfAdjFactor = \"FUsEtfAdjFactor\" //美股ETF复权因子\n)\n\nvar (\n\t// 接收数据条数\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_rx_traffic\",\n\t\t\tHelp: \"count received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// qps\n\t// trigger: cron kafka\n\t// source: pg, f10\n\tqpsCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_qps\",\n\t\t\tHelp: \"request received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// 处理耗时分布\n\t// trigger: cron manual kafka\n\t// stage: extract proccess load\n\t// export: all bbrq rtime real code\n\t// source: pg, f10\n\tcostReqBucketVec = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"gms_etl_cost_histogram\",\n\t\t\tHelp:    \"cost of time for each request.\",\n\t\t\tBuckets: []float64{100, 1000, 5000, 10000, 50000, 120000, 600000},\n\t\t},\n\t\t[]string{\"db\", \"table\", \"stage\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// label_name 面板显示时公有显示\n\tnameCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_label_name\",\n\t\t\tHelp: \"show label name\",\n\t\t},\n\t\t[]string{\"db\", \"table\"},\n\t)\n\n\t// 错误数 记录接口错误,内部错误等\n\terrorCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_error\",\n\t\t\tHelp: \"errors\",\n\t\t},\n\t\t[]string{\"status\"},\n\t)\n\n\t// some metrics of kafka consumer, such as package, data flow, etc.\n\tkafkaDataRecvPackage = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_package\",\n\t\t\tHelp: \"data recv package\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaRecvSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_recv_task_id\",\n\t\t\tHelp: \"skip recv task id\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\", \"error\"},\n\t)\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n\n\tprometheus.MustRegister(kafkaDataRecvPackage)\n\tprometheus.MustRegister(kafkaDataRecvFlow)\n\tprometheus.MustRegister(kafkaRecvSkipTaskId)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n// KafkaDataRecvPackage kafka数据包接收统计\nfunc KafkaDataRecvPackage(topic string, partition int32, taskId string, packageNum int) {\n\tkafkaDataRecvPackage.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(packageNum))\n}\n\n// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition int32, taskId string, flow int) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(flow))\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string, errType string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId, errType).Inc()\n}\n\nfunc SetInvalidZero(taskId","infill":" string, reason string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(\"\", \"\", taskId, reason).Set(0)\n}\n","suffix":"","relevantFile":"","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"string, partition int32) {\n    kafkaRecvSkipTaskId.WithLabelValues(taskId, strconv.Itoa(int(partition)), \"unknown\", \"invalid_zero\").Set(0)\n}\n","CodeQwen2.5-7B":" string) {\n\tkafkaDataRecvPackage.DeleteLabelValues(\"gms_etl\", \"gms_etl\", taskId)\n\tkafkaDataRecvFlow.DeleteLabelValues(\"gms_etl\", \"gms_etl\", taskId)\n\tkafkaRecvSkipTaskId.DeleteLabelValues(\"gms_etl\", \"gms_etl\", taskId, \"kafka_consume_error\")\n}","copilot":" string, reason string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(\"\", \"\", taskId, reason).Set(0)\n}\n","deepseek-chat-lite":" string) {","CodeQwen1.5-7B":" string) {"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (\n\tStageAll       = \"all\"       //完整阶段\n\tStageExtract   = \"extract\"   //抽取\n\tStageTransform = \"transform\" //转换\n\tStageLoad      = \"load\"      //持久化\n)\n\n// 数据源source\nconst (\n\tSourcePg              = \"pg\"              // pg数据库\n\tSourceF10             = \"f10\"             // F10\n\tSourceFUsEtfAdjFactor = \"FUsEtfAdjFactor\" //美股ETF复权因子\n)\n\nvar (\n\t// 接收数据条数\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_rx_traffic\",\n\t\t\tHelp: \"count received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// qps\n\t// trigger: cron kafka\n\t// source: pg, f10\n\tqpsCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_qps\",\n\t\t\tHelp: \"request received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// 处理耗时分布\n\t// trigger: cron manual kafka\n\t// stage: extract proccess load\n\t// export: all bbrq rtime real code\n\t// source: pg, f10\n\tcostReqBucketVec = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"gms_etl_cost_histogram\",\n\t\t\tHelp:    \"cost of time for each request.\",\n\t\t\tBuckets: []float64{100, 1000, 5000, 10000, 50000, 120000, 600000},\n\t\t},\n\t\t[]string{\"db\", \"table\", \"stage\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// label_name 面板显示时公有显示\n\tnameCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_label_name\",\n\t\t\tHelp: \"show label name\",\n\t\t},\n\t\t[]string{\"db\", \"table\"},\n\t)\n\n\t// 错误数 记录接口错误,内部错误等\n\terrorCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_error\",\n\t\t\tHelp: \"errors\",\n\t\t},\n\t\t[]string{\"status\"},\n\t)\n\n\t// some metrics of kafka consumer, such as package, data flow, etc.\n\tkafkaDataRecvPackage = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_package\",\n\t\t\tHelp: \"data recv package\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaRecvSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_recv_task_id\",\n\t\t\tHelp: \"skip recv task id\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\", \"error\"},\n\t)\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n\n\tprometheus.MustRegister(kafkaDataRecvPackage)\n\tprometheus.MustRegister(kafkaDataRecvFlow)\n\tprometheus.MustRegister(kafkaRecvSkipTaskId)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n// KafkaDataRecvPackage kafka数据包接收统计\nfunc KafkaDataRecvPackage(topic string, partition int32, taskId string, packageNum int) {\n\tkafkaDataRecvPackage.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(packageNum))\n}\n\n// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition int32, taskId string, flow int) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(flow))\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string, errType string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId, errType).Inc()\n}\n\nfunc SetInvalidZero(taskId string, reason string","infill":") {\n\tmetricsetInvalidZero.WithLabelValues(\"\", \"\", taskId, reason).Set(0)\n}\n","suffix":"","relevantFile":"","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":") {\n    errorCounterVec.WithLabelValues(taskId + \"_\" + reason).Set(0)\n}\n\nfunc SetKafkaConsumeZero(topic string, partition int32, taskId string) {\n    kafkaRecvSkipTaskId.WithLabel","CodeQwen2.5-7B":") {\n\tkafkaRecvSkipTaskId.WithLabelValues(\"invalid\", \"0\", taskId, reason).Inc()\n}","copilot":") {\n\tmetricsetInvalidZero.WithLabelValues(\"\", \"\", taskId, reason).Set(0)\n}\n","deepseek-chat-lite":") {","CodeQwen1.5-7B":" ) {"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (\n\tStageAll       = \"all\"       //完整阶段\n\tStageExtract   = \"extract\"   //抽取\n\tStageTransform = \"transform\" //转换\n\tStageLoad      = \"load\"      //持久化\n)\n\n// 数据源source\nconst (\n\tSourcePg              = \"pg\"              // pg数据库\n\tSourceF10             = \"f10\"             // F10\n\tSourceFUsEtfAdjFactor = \"FUsEtfAdjFactor\" //美股ETF复权因子\n)\n\nvar (\n\t// 接收数据条数\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_rx_traffic\",\n\t\t\tHelp: \"count received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// qps\n\t// trigger: cron kafka\n\t// source: pg, f10\n\tqpsCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_qps\",\n\t\t\tHelp: \"request received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// 处理耗时分布\n\t// trigger: cron manual kafka\n\t// stage: extract proccess load\n\t// export: all bbrq rtime real code\n\t// source: pg, f10\n\tcostReqBucketVec = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"gms_etl_cost_histogram\",\n\t\t\tHelp:    \"cost of time for each request.\",\n\t\t\tBuckets: []float64{100, 1000, 5000, 10000, 50000, 120000, 600000},\n\t\t},\n\t\t[]string{\"db\", \"table\", \"stage\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// label_name 面板显示时公有显示\n\tnameCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_label_name\",\n\t\t\tHelp: \"show label name\",\n\t\t},\n\t\t[]string{\"db\", \"table\"},\n\t)\n\n\t// 错误数 记录接口错误,内部错误等\n\terrorCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_error\",\n\t\t\tHelp: \"errors\",\n\t\t},\n\t\t[]string{\"status\"},\n\t)\n\n\t// some metrics of kafka consumer, such as package, data flow, etc.\n\tkafkaDataRecvPackage = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_package\",\n\t\t\tHelp: \"data recv package\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaRecvSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_recv_task_id\",\n\t\t\tHelp: \"skip recv task id\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\", \"error\"},\n\t)\n\n\tcountSetInvalidZero ","infill":"= prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"set_invalid_zero\",\n\t\t\tHelp: \"set invalid zero\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"task_id\", \"reason\"},\n\t)","suffix":"\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n\n\tprometheus.MustRegister(kafkaDataRecvPackage)\n\tprometheus.MustRegister(kafkaDataRecvFlow)\n\tprometheus.MustRegister(kafkaRecvSkipTaskId)\n\n\tprometheus.MustRegister(countSetInvalidZero)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n// KafkaDataRecvPackage kafka数据包接收统计\nfunc KafkaDataRecvPackage(topic string, partition int32, taskId string, packageNum int) {\n\tkafkaDataRecvPackage.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(packageNum))\n}\n\n// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition int32, taskId string, flow int) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(flow))\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string, errType string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId, errType).Inc()\n}\n\nfunc SetInvalidZero(taskId string, reason string) {\n\tcountSetInvalidZero.WithLabelValues(\"\", \"\", taskId, reason).Set(0)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"= prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"set_invalid_zero\",\n            Help: \"invalid zero count\",\n        },\n        []string{\"db\", \"table\", \"task_id\", \"reason\"},\n    )","CodeQwen2.5-7B":"= prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"count_set_invalid_zero\",\n\t\t\tHelp: \"count set invalid zero\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"task_id\", \"reason\"},\n\t)","copilot":"= prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"set_invalid_zero\",\n\t\t\tHelp: \"set invalid zero\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"task_id\", \"reason\"},\n\t)","deepseek-chat-lite":"= prometheus.NewGaugeVec(","CodeQwen1.5-7B":" = prometheus.NewGaugeVec("}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (\n\tStageAll       = \"all\"       //完整阶段\n\tStageExtract   = \"extract\"   //抽取\n\tStageTransform = \"transform\" //转换\n\tStageLoad      = \"load\"      //持久化\n)\n\n// 数据源source\nconst (\n\tSourcePg              = \"pg\"              // pg数据库\n\tSourceF10             = \"f10\"             // F10\n\tSourceFUsEtfAdjFactor = \"FUsEtfAdjFactor\" //美股ETF复权因子\n)\n\nvar (\n\t// 接收数据条数\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_rx_traffic\",\n\t\t\tHelp: \"count received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// qps\n\t// trigger: cron kafka\n\t// source: pg, f10\n\tqpsCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_qps\",\n\t\t\tHelp: \"request received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// 处理耗时分布\n\t// trigger: cron manual kafka\n\t// stage: extract proccess load\n\t// export: all bbrq rtime real code\n\t// source: pg, f10\n\tcostReqBucketVec = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"gms_etl_cost_histogram\",\n\t\t\tHelp:    \"cost of time for each request.\",\n\t\t\tBuckets: []float64{100, 1000, 5000, 10000, 50000, 120000, 600000},\n\t\t},\n\t\t[]string{\"db\", \"table\", \"stage\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// label_name 面板显示时公有显示\n\tnameCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_label_name\",\n\t\t\tHelp: \"show label name\",\n\t\t},\n\t\t[]string{\"db\", \"table\"},\n\t)\n\n\t// 错误数 记录接口错误,内部错误等\n\terrorCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_error\",\n\t\t\tHelp: \"errors\",\n\t\t},\n\t\t[]string{\"status\"},\n\t)\n\n\t// some metrics of kafka consumer, such as package, data flow, etc.\n\tkafkaDataRecvPackage = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_package\",\n\t\t\tHelp: \"data recv package\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaRecvSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_recv_task_id\",\n\t\t\tHelp: \"skip recv task id\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\", \"error\"},\n\t)\n\n\tcountSetInvalidZero = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"set_invalid_zero\",","infill":"Help: \"set invalid zero\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"task_id\", \"reason\"},\n\t)","suffix":"\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n\n\tprometheus.MustRegister(kafkaDataRecvPackage)\n\tprometheus.MustRegister(kafkaDataRecvFlow)\n\tprometheus.MustRegister(kafkaRecvSkipTaskId)\n\n\tprometheus.MustRegister(countSetInvalidZero)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n// KafkaDataRecvPackage kafka数据包接收统计\nfunc KafkaDataRecvPackage(topic string, partition int32, taskId string, packageNum int) {\n\tkafkaDataRecvPackage.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(packageNum))\n}\n\n// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition int32, taskId string, flow int) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(flow))\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string, errType string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId, errType).Inc()\n}\n\nfunc SetInvalidZero(taskId string, reason string) {\n\tcountSetInvalidZero.WithLabelValues(\"\", \"\", taskId, reason).Set(0)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":" // 设置无效数字为0\n            Help: \"set_invalid_zero\",\n        },\n        []string{\"namespace\", \"job\", \"task_id\", \"reason\"},\n    )","CodeQwen2.5-7B":"\n\t\t\tHelp: \"set invalid zero\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"task_id\", \"reason\"},\n\t)<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"Help: \"set invalid zero\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"task_id\", \"reason\"},\n\t)","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (\n\tStageAll       = \"all\"       //完整阶段\n\tStageExtract   = \"extract\"   //抽取\n\tStageTransform = \"transform\" //转换\n\tStageLoad      = \"load\"      //持久化\n)\n\n// 数据源source\nconst (\n\tSourcePg              = \"pg\"              // pg数据库\n\tSourceF10             = \"f10\"             // F10\n\tSourceFUsEtfAdjFactor = \"FUsEtfAdjFactor\" //美股ETF复权因子\n)\n\nvar (\n\t// 接收数据条数\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_rx_traffic\",\n\t\t\tHelp: \"count received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// qps\n\t// trigger: cron kafka\n\t// source: pg, f10\n\tqpsCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_qps\",\n\t\t\tHelp: \"request received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// 处理耗时分布\n\t// trigger: cron manual kafka\n\t// stage: extract proccess load\n\t// export: all bbrq rtime real code\n\t// source: pg, f10\n\tcostReqBucketVec = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"gms_etl_cost_histogram\",\n\t\t\tHelp:    \"cost of time for each request.\",\n\t\t\tBuckets: []float64{100, 1000, 5000, 10000, 50000, 120000, 600000},\n\t\t},\n\t\t[]string{\"db\", \"table\", \"stage\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// label_name 面板显示时公有显示\n\tnameCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_label_name\",\n\t\t\tHelp: \"show label name\",\n\t\t},\n\t\t[]string{\"db\", \"table\"},\n\t)\n\n\t// 错误数 记录接口错误,内部错误等\n\terrorCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_error\",\n\t\t\tHelp: \"errors\",\n\t\t},\n\t\t[]string{\"status\"},\n\t)\n\n\t// some metrics of kafka consumer, such as package, data flow, etc.\n\tkafkaDataRecvPackage = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_package\",\n\t\t\tHelp: \"data recv package\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaRecvSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_recv_task_id\",\n\t\t\tHelp: \"skip recv task id\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\", \"error\"},\n\t)\n\n\tcountSetInvalidZero = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"set_invalid_zero\",\n\t\t\tHelp: \"set invalid zero\",\n\t\t}","infill":",\n\t\t[]string{\"db\", \"table\", \"task_id\", \"reason\"},\n\t)\n","suffix":"\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n\n\tprometheus.MustRegister(kafkaDataRecvPackage)\n\tprometheus.MustRegister(kafkaDataRecvFlow)\n\tprometheus.MustRegister(kafkaRecvSkipTaskId)\n\n\tprometheus.MustRegister(countSetInvalidZero)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n// KafkaDataRecvPackage kafka数据包接收统计\nfunc KafkaDataRecvPackage(topic string, partition int32, taskId string, packageNum int) {\n\tkafkaDataRecvPackage.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(packageNum))\n}\n\n// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition int32, taskId string, flow int) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(flow))\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string, errType string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId, errType).Inc()\n}\n\nfunc SetInvalidZero(taskId string, reason string) {\n\tcountSetInvalidZero.WithLabelValues(\"\", \"\", taskId, reason).Set(0)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":",\n        []string{\"task_id\", \"reason\"},\n    )","CodeQwen2.5-7B":",\n\t\t[]string{\"db\", \"table\", \"task_id\", \"reason\"},\n\t)\n<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":",\n\t\t[]string{\"db\", \"table\", \"task_id\", \"reason\"},\n\t)\n","deepseek-chat-lite":",","CodeQwen1.5-7B":","}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tif validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetInvalidZero(","infill":"jsonF10.","suffix":"\n\t\t\t, \"RecvZero\")\n\t\treturn\n\n\t}\n\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIDs, subjects, currency, recvDate)\n\tif err != nil {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetInvalidZero(jsonF10.TaskId, \"RequestEmpty\")\n\t\treturn\n\t}\n\n\td.UpdateUnderlyingAssetData(thsMarketCode.Code, 1, recvDate, jsonF10Rsp, taskInfo)\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"<file_path>app/metrics/metrics.go\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"jsonF10.TaskId, \"RecvZero\")\n\t\treturn\n\t}","copilot":"jsonF10.","deepseek-chat-lite":"jsonF10.TaskId, \"RecvZero\")","CodeQwen1.5-7B":" jsonF10.TaskId, \"RecvZero\")"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects)\n\treturn\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致","infill":"，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据，fou","suffix":"\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\tdateStr, ok := jsonF10Rsp.Data.Data[0].Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 {\n\t\td.ProcessInvalidData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t// 如果 validInt=1, 则更新相应的指标数据\n\t// 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tif taskInfo.Currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(ids, subjects, taskInfo.Currency)\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tsearchSql := fmt.Sprintf(\"SELECT isvalid FROM `%s` WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\trow, _ := load.FetchOneRowData(taskInfo.DbName, searchSql)\n\tvar isvalidField int // mysql数据库中isvalid字段\n\terr := row.Scan(&isvalidField)\n\tif err == nil {\n\t\tloadSql := fmt.Sprintf(\"INSERT INTO `%s` (zqdm,unix_time) VALUES (%s, %d) ON DUPLICATE KEY UPDATE isvalid = 0\", taskInfo.TableName, codeStr, unixTime)\n\t\t_, err := load.SinkData(taskInfo.DbName, loadSql)\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"search isvalid in local mysql, searchSql:%s, loadSql:%s, err:%s, msg:%s\", searchSql, loadSql, err.Error())\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t\t}\n\t}\n\treturn\n}\n\n// kafka 推送数据为原生数据，这里再请求一次指标数据，\nfunc (d F10KafkaTaskInfo) RequestF10Data(ids string, subjects string, currency string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      ids,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"则先将数据库中对应的数据更新为删除状态(isvalid=0)，再新增或更新最新的数据","CodeQwen2.5-7B":"，则需要先查询一次数据，再查询一次数据\n// 5. 如果第二次查询的日期和kafka推送的日期一致，则直接使用第二次查询的数据更新数据库\n// 6. 如果第二次查询的日期和kafka推送的日期不一致，则需要先查询一次数据，再查询一次数据\n// 7. 如果第三次查询的日期和kafka推送的日期一致，则直接使用第三次查询的数据更新数据库\n// 8. 如果第三次查询的日期和kafka推送的日期不一致，则直接使用第一次查询的数据更新数据库\n// 9. 如果第一次查询的日期和kafka推送的日期不一致，则直接使用第一次查询的数据更新数据库\n// 10. 如果第一次查询的日期和kafka推送的日期不一致，则直接使用第一次查询的数据更新数据库\n// 11. 如果第一次查询的日期和kafka推送的日期不一致，则直接使用第一次查询的数据更新数据库\n// 12. 如果第一次查询的日期和kafka推送的日期不一致，则直接使用第一次查询的数据更新数据库\n// 13. 如果第一次查询的日期和kafka推送的日期不一致，则直接使用第一次查询的数据更新数据库\n// 14. 如果第一次查询的日期和kafka推送的日期不一致，则直接使用第一次查询的数据更新数据库\n// 15. 如果第一次查询的日期和kafka推送的日期不一致，则直接使用第一次","copilot":"，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据，fou","deepseek-chat-lite":"，则需要先更新数据库，再更新数据库","CodeQwen1.5-7B":" ，则先查询一次，再查询一次，如果第二次查询的日期和kafka推送的日期一致，则使用第二次查询的数据更新数据库"}},{"prefix":"package main\n\nimport (\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"os/exec\"\n\t\"strings\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/docker/docker/client\"\n\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientW","infill":"ithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif \n","suffix":"\n\t// Execute 'docker ps -a' command\n\tout, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Split the output into lines\n\tlines := strings.Split(string(out), \"\\n\")\n\n\tlog.Println(lines)\n\n\t// Parse the lines into a list of Container objects\n\tcontainers := []Container{}\n\tfor _, line := range lines { // Skip the header line\n\t\tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t\t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t\t})\n\t\tlog.Println(fields)\n\t\tif len(fields) > 0 {\n\t\t\tcontainers = append(containers, Container{\n\t\t\t\tID:     fields[0],\n\t\t\t\tImage:  fields[1],\n\t\t\t\tName:   fields[2],\n\t\t\t\tStatus: strings.ToUpper(fields[3]),\n\t\t\t\t// Ports:   fields[4],\n\t\t\t\tLastStarted: fields[4],\n\t\t\t})\n\t\t}\n\t}\n\treturn containers, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"ithOpts(client.FromEnv, client.WithAPIVersionNegotiation())","CodeQwen2.5-7B":"ithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer client.Close()\n","copilot":"ithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif \n","deepseek-chat-lite":"ithOpts(client.FromEnv, client.WithAPIVersionNegotiation())","CodeQwen1.5-7B":"ithOpts(client.FromEnv, client.WithAPIVersionNegotiation())"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"os/exec\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif ","infill":"err != nil {\n\t\treturn nil, err\n\t}\n\n\tContainers, err := client.ContainerList(context.Background(), container","suffix":"\n\n\t// Execute 'docker ps -a' command\n\tout, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Split the output into lines\n\tlines := strings.Split(string(out), \"\\n\")\n\n\tlog.Println(lines)\n\n\t// Parse the lines into a list of Container objects\n\tcontainers := []Container{}\n\tfor _, line := range lines { // Skip the header line\n\t\tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t\t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t\t})\n\t\tlog.Println(fields)\n\t\tif len(fields) > 0 {\n\t\t\tcontainers = append(containers, Container{\n\t\t\t\tID:     fields[0],\n\t\t\t\tImage:  fields[1],\n\t\t\t\tName:   fields[2],\n\t\t\t\tStatus: strings.ToUpper(fields[3]),\n\t\t\t\t// Ports:   fields[4],\n\t\t\t\tLastStarted: fields[4],\n\t\t\t})\n\t\t}\n\t}\n\treturn containers, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"err != nil {\n        return nil, err\n    }","CodeQwen2.5-7B":"err != nil {\n\t\treturn nil, err\n\t}\n\tdefer client.Close()","copilot":"err != nil {\n\t\treturn nil, err\n\t}\n\n\tContainers, err := client.ContainerList(context.Background(), container","deepseek-chat-lite":"err != nil {","CodeQwen1.5-7B":" err != nil {"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"os/exec\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tContainer","infill":"s, err := client.ContainerList(context.Background(), container.ListOpt","suffix":"\n\n\t// Execute 'docker ps -a' command\n\tout, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Split the output into lines\n\tlines := strings.Split(string(out), \"\\n\")\n\n\tlog.Println(lines)\n\n\t// Parse the lines into a list of Container objects\n\tcontainers := []Container{}\n\tfor _, line := range lines { // Skip the header line\n\t\tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t\t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t\t})\n\t\tlog.Println(fields)\n\t\tif len(fields) > 0 {\n\t\t\tcontainers = append(containers, Container{\n\t\t\t\tID:     fields[0],\n\t\t\t\tImage:  fields[1],\n\t\t\t\tName:   fields[2],\n\t\t\t\tStatus: strings.ToUpper(fields[3]),\n\t\t\t\t// Ports:   fields[4],\n\t\t\t\tLastStarted: fields[4],\n\t\t\t})\n\t\t}\n\t}\n\treturn containers, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"s := []Container{}","CodeQwen2.5-7B":"s, err := client.ContainerList(context.Background(), client.ContainerListOptions{})\n\tif err != nil {\n\t\treturn nil, err\n\t}","copilot":"s, err := client.ContainerList(context.Background(), container.ListOpt","deepseek-chat-lite":"s, err := client.ContainerList(context.Background(), client.ContainerListOptions{})","CodeQwen1.5-7B":" s, err := client.ContainerList(context.Background(), client.ListContainersOptions{All: true})"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"os/exec\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{","infill":"})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)","suffix":"\n\n\t// Execute 'docker ps -a' command\n\tout, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Split the output into lines\n\tlines := strings.Split(string(out), \"\\n\")\n\n\tlog.Println(lines)\n\n\t// Parse the lines into a list of Container objects\n\tcontainers := []Container{}\n\tfor _, line := range lines { // Skip the header line\n\t\tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t\t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t\t})\n\t\tlog.Println(fields)\n\t\tif len(fields) > 0 {\n\t\t\tcontainers = append(containers, Container{\n\t\t\t\tID:     fields[0],\n\t\t\t\tImage:  fields[1],\n\t\t\t\tName:   fields[2],\n\t\t\t\tStatus: strings.ToUpper(fields[3]),\n\t\t\t\t// Ports:   fields[4],\n\t\t\t\tLastStarted: fields[4],\n\t\t\t})\n\t\t}\n\t}\n\treturn containers, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"All: true})","CodeQwen2.5-7B":"All: true})\n\tif err != nil {\n\t\treturn nil, err\n\t}","copilot":"})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)","deepseek-chat-lite":"})","CodeQwen1.5-7B":"})"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil","infill":", nil","suffix":"\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":", nil","CodeQwen2.5-7B":", nil","copilot":", nil","deepseek-chat-lite":", nil","CodeQwen1.5-7B":", nil<file_sep> /README.md"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"","infill":"\"github.com/docker/docker/client\"","suffix":"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"\"github.com/docker/docker/client\"","CodeQwen2.5-7B":"\n\t\"github.com/docker/docker/client\"","copilot":"\"github.com/docker/docker/client\"","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll: ","infill":"true","suffix":",\n\t\tFilters: map[string][]string{\n\t\t\t\"name\": {\"(_|-)dev$\"\t\t},\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"true","CodeQwen2.5-7B":"true,\n\t\t// Filters: map[string][]string{\n\t\t// \t\"name\": {\"(_|-)dev$\"},\n\t\t// }<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"true","deepseek-chat-lite":"true","CodeQwen1.5-7B":" true"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll: true,\n\t\tFilters: ","infill":"map[string][]string{\n\t\t\t\"name\": {\"(_|-)dev$\"}","suffix":",\n\t\t}})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"map[string][]string{\n            \"label\": {`org.opencontainers.image.created=today`},\n            \"status\": {\"running\"},\n            \"name\":  {`dev`, `^_dev`, `-dev$`},\n            \"label\":","CodeQwen2.5-7B":"map[string][]string{\n\t\t\t\"name\": []string{`(_|-)dev$`}<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"map[string][]string{\n\t\t\t\"name\": {\"(_|-)dev$\"}","deepseek-chat-lite":"map[string][]string{","CodeQwen1.5-7B":" container.Filters{"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll: true,\n\t\tFilters: map[string][]string{","infill":"\"name\": {\"(_|-)dev$\"","suffix":"},\n\t\t}})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"\"label\": {\"com.centurylink.devstream.parent=true\"}","CodeQwen2.5-7B":"\n\t\t\t\"name\": []string{`(_|-)dev$`},\n\t\t},\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}","copilot":"\"name\": {\"(_|-)dev$\"","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err !=","infill":"})\n\tif err !=","suffix":"})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n<file_path>parse.go\nmultiple values.\n*/\npackage filters // import \"github.com/docker/docker/api/types/filters\"\n\nimport (\n\t\"encoding/json\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// Args stores a mapping of keys to a set of multiple values.\ntype Args struct {\n\tfields map[string]map[string]bool\n}\n\n// KeyValuePair are used to initialize a new Args\ntype KeyValuePair struct {\n\tKey   string\n\tValue string\n}\n\n// Arg creates a new KeyValuePair for initializing Args\nfunc Arg(key, value string) KeyValuePair {\n\treturn KeyValuePair{Key: key, Value: value}\n}\n\n// NewArgs returns a new Args populated with the initial args\nfunc NewArgs(initialArgs ...KeyValuePair) Args {\n\targs := Args{fields: map[string]map[string]bool{}}\n\tfor _, arg := range initialArgs {\n\t\targs.Add(arg.Key, arg.Value)\n\t}\n\treturn args\n}\n\n// Keys returns all the keys in list of Args\nfunc (args Args) Keys() []string {\n\tkeys := make([]string, 0, len(args.fields))\n\tfor k := range args.fields {\n\t\tkeys = append(keys, k)\n\t}\n\treturn keys\n}\n\n// MarshalJSON returns a JSON byte representation of the Args\nfunc (args Args) MarshalJSON() ([]byte, error) {\n\tif len(args.fields) == 0 {\n\t\treturn []byte(\"{}\"), nil\n\t}\n\treturn json.Marshal(args.fields)\n}\n\n// ToJSON returns the Args as a JSON encoded string\nfunc ToJSON(a Args) (string, error) {\n\tif a.Len() == 0 {\n\t\treturn \"\", nil\n\t}\n\tbuf, err := json.Marshal(a)","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"nil {\n        return nil, err\n    }\n\n    err = client.ContainerStart(context.Background(), containers[0].ID, types.ContainerStartOptions{","CodeQwen2.5-7B":" nil {\n\t\treturn nil, err\n\t}","copilot":"})\n\tif err !=","deepseek-chat-lite":" nil {","CodeQwen1.5-7B":" nil {"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tc.Ports += port.IP + \":\" + strconv.Itoa(int(port.PublicPort)) + \"->\" + strconv.Itoa(int(port.PrivatePort)) + \"/\" + port.Type + \"\\n\"\n\t\t}\n\n\t\tc.LastStarted","infill":" = time.Format(container.State.StartedAt, time.RFC3339Nano)","suffix":"\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>itoa.go\n// Copyright 2009 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\npackage strconv\n\nimport \"math/bits\"\n\nconst fastSmalls = true // enable fast path for small integers\n\n// FormatUint returns the string representation of i in the given base,\n// for 2 <= base <= 36. The result uses the lower-case letters 'a' to 'z'\n// for digit values >= 10.\nfunc FormatUint(i uint64, base int) string {\n\tif fastSmalls && i < nSmalls && base == 10 {\n\t\treturn small(int(i))\n\t}\n\t_, s := formatBits(nil, i, base, false, false)\n\treturn s\n}\n\n// FormatInt returns the string representation of i in the given base,\n// for 2 <= base <= 36. The result uses the lower-case letters 'a' to 'z'\n// for digit values >= 10.\nfunc FormatInt(i int64, base int) string {\n\tif fastSmalls && 0 <= i && i < nSmalls && base == 10 {\n\t\treturn small(int(i))\n\t}\n\t_, s := formatBits(nil, uint64(i), base, i < 0, false)\n\treturn s\n}\n\n// Itoa is equivalent to FormatInt(int64(i), 10).\nfunc Itoa(i int) string {\n\treturn FormatInt(int64(i), 10)\n}\n\n// AppendInt appends the string form of the integer i,\n// as generated by FormatInt, to dst and returns the extended buffer.\nfunc AppendInt(dst []byte, i int64, base int) []byte {\n\tif fastSmalls && 0 <= i && i < nSmalls && base == 10 {\n\t\treturn append(dst, small(int(i))...)\n\t}\n\tdst, _ = formatBits(dst, uint64(i), base, i < 0, true)\n\treturn dst\n}\n\n// AppendUint appends the string form of the unsigned integer i,\n// as generated by FormatUint, to dst and returns the extended buffer.\nfunc AppendUint(dst []byte, i uint64, base int) []byte {\n\tif fastSmalls && i < nSmalls && base == 10 {\n\t\treturn append(dst, small(int(i))...)\n\t}\n\tdst, _ = formatBits(dst, i, base, false, true)\n\treturn dst\n}\n\n// small returns the string for an i with 0 <= i < nSmalls.\nfunc small(i int) string {\n\tif i < 10 {\n<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":" = container.Created","CodeQwen2.5-7B":" = container.Created.String()","copilot":" = time.Format(container.State.StartedAt, time.RFC3339Nano)","deepseek-chat-lite":" = container.Created.String()","CodeQwen1.5-7B":" = container.Created.String()"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tc.Ports += port.IP + \":\" + strconv.Itoa(int(port.PublicPort)) + \"->\" + strconv.Itoa(int(port.PrivatePort)) + \"/\" + port.Type + \"\\n\"\n\t\t}\n\n\t\t// the value of timest","infill":"amp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.RFC3339)","suffix":"\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"amp is in nanoseconds","CodeQwen2.5-7B":"amp is in nanoseconds\n\t\tc.LastStarted = time.Unix(0, container.Created).Format(\"2006-01-02 15:04:05\")","copilot":"amp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.RFC3339)","deepseek-chat-lite":"amp is in nanoseconds, so we need to convert it to seconds","CodeQwen1.5-7B":"amp is in nanoseconds, so we need to divide by 1000000000 to get seconds"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tc.Ports += port.IP + \":\" + strconv.Itoa(int(port.PublicPort)) + \"->\" + strconv.Itoa(int(port.PrivatePort)) + \"/\" + port.Type + \"\\n\"\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time","infill":".Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.RFC3339)","suffix":"\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":".","CodeQwen2.5-7B":"stamp in seconds\n\t\tc.LastStarted = time.Unix(0, int64(container.Created)*1000000000).Format(\"2006-01-02 15:04:05\")","copilot":".Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.RFC3339)","deepseek-chat-lite":".Time","CodeQwen1.5-7B":" stamp"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}\n\nfunc GetImagesFromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{","infill":"\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})","suffix":"\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"\n                \"error\": err.Error(),\n            })\n            return\n        }\n        c.JSON(http.StatusOK, imageList)\n    })","CodeQwen2.5-7B":"\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})","copilot":"\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}\n\nfunc GetImagesFromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t}","infill":")\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n","suffix":"\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":")\n            return\n        }\n        c.JSON(http.StatusOK, imageList)\n    })\n","CodeQwen2.5-7B":")\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n","copilot":")\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n","deepseek-chat-lite":")","CodeQwen1.5-7B":")"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype","infill":" Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}","suffix":"\n\nfunc GetImagesFromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timg := Image{\n\t\t\tID:          image.ID,\n\t\t\tRepository:  image.RepoTags[0],\n\t\t\t\n\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"Image struct {\n    ID         string `json:\"id\"`\n    Repository string `json:\"repository\"`\n    Tag        string `json:\"tag\"`\n    Size       int    `json:\"size\"`","CodeQwen2.5-7B":" Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n}","copilot":" Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}","deepseek-chat-lite":" Image struct {","CodeQwen1.5-7B":"  Image struct {"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`","infill":"Tag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}","suffix":"\n\nfunc GetImagesFromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timg := Image{\n\t\t\tID:          image.ID,\n\t\t\tRepository:  image.RepoTags[0],\n\t\t\tTag:         image.RepoTags[1],\n\t\t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),\n\t\t\tSize:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t\t\t\n\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n}","copilot":"Tag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`","infill":"Size        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}","suffix":"\n\nfunc GetImagesFromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timg := Image{\n\t\t\tID:          image.ID,\n\t\t\tRepository:  image.RepoTags[0],\n\t\t\tTag:         image.RepoTags[1],\n\t\t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),\n\t\t\tSize:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t\t\tVirtualSize: strconv.Itoa(int(image.VirtualSize / 1024 / 1024)),\n\t\t}\n\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}","copilot":"Size        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}\n\nfunc GetImagesFromDocker() ([]Image, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []Image{}\n\tfor _, image := range images {\n\t\timg := Image{\n\t\t\tID:          image.ID,\n\t\t\tRepository:  image.RepoTags[0],\n\t\t\tTag:         image.RepoTags[1],\n\t\t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),\n\t\t\tSize:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t\t\tVirtualSize: strconv.Itoa(int(image.VirtualSize / 1024 / 1024)),\n\t\t}\n\n\t\timageList = append(imageList, ","infill":"img)","suffix":"\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"img)","CodeQwen2.5-7B":"img)","copilot":"img)","deepseek-chat-lite":"img)","CodeQwen1.5-7B":" img)<file_sep> /README.md"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}\n\nfunc GetImagesFromDocker() ([]Image, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []","infill":"Image{}","suffix":"\n\tfor _, image := range images {\n\t\timg := Image{\n\t\t\tID:          image.ID,\n\t\t\tRepository:  image.RepoTags[0],\n\t\t\tTag:         image.RepoTags[1],\n\t\t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),\n\t\t\tSize:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t\t\tVirtualSize: strconv.Itoa(int(image.VirtualSize / 1024 / 1024)),\n\t\t}\n\n\t\timageList = append(imageList, img)\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"Image{}","CodeQwen2.5-7B":"Image{}","copilot":"Image{}","deepseek-chat-lite":"Image{}","CodeQwen1.5-7B":" Image{}"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tif validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetInvalidZero(taskInfo.TableName, jsonF10.TaskId, \"RecvZero\")\n\t\treturn\n\n\t}\n\n<<<<<<< HEAD\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 {\n\t\td.SetIsValidZeroToMysql(thsCode, dateStr, taskInfo)\n=======\n\tjsonF10Rsp, err := d.Request","infill":"est策略。","suffix":"\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"<file_path>app/metrics/metrics.go\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (\n\tStageAll       = \"all\"       //完整阶段\n\tStageExtract   = \"extract\"   //抽取\n\tStageTransform = \"transform\" //转换\n\tStageLoad      = \"load\"      //持久化\n)\n\n// 数据源source\nconst (\n\tSourcePg              = \"pg\"              // pg数据库\n\tSourceF10             = \"f10\"             // F10\n\tSourceFUsEtfAdjFactor = \"FUsEtfAdjFactor\" //美股ETF复权因子\n)\n\nvar (\n\t// 接收数据条数\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_rx_traffic\",\n\t\t\tHelp: \"count received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// qps\n\t// trigger: cron kafka\n\t// source: pg, f10\n\tqpsCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_qps\",\n\t\t\tHelp: \"request received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// 处理耗时分布\n\t// trigger: cron manual kafka\n\t// stage: extract proccess load\n\t// export: all bbrq rtime real code\n\t// source: pg, f10\n\tcostReqBucketVec = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"gms_etl_cost_histogram\",\n\t\t\tHelp:    \"cost of time for each request.\",","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"F10Data(indicatorIDs, subjects, currency, recvDate)\n    if err != nil {\n        levellog.Log.Info(\"request f10 data failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subject","CodeQwen2.5-7B":"F10Data(indicatorIDs, subjects, currency, recvDate)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\treturn\n\t}","copilot":"est策略。","deepseek-chat-lite":"F10Data(indicatorIDs, subjects, currency, recvDate)","CodeQwen1.5-7B":" F10Data(indicatorIDs, subjects, currency, \"\")"}},{"prefix":"package server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.Post(\"/debug/pushmsg","infill":"\", pushMsgHandler)","suffix":"\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\to\n\n\tc.String(200, \"ok\")\n}\n","relevantFile":"<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)","relevantFileList":[],"filePath":"app/server/http.go","template":"go","multiRes":{"hipilot":"\", pushMsgHandler)       r.GET(\"/debug/pushmsg\", pushMsgHandler)","CodeQwen2.5-7B":"\", pushMsgHandler) // 推送消息\n}","copilot":"\", pushMsgHandler)","deepseek-chat-lite":"\", pushMsgHandler) // 推送消息","CodeQwen1.5-7B":"\", pushMsgHandler)"}},{"prefix":"package server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.Post(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}","infill":"// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tmy_orm.F10Update\n\n\tc.String(200, \"ok\")\n}\n","suffix":"","relevantFile":"<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract_manager/extract_manager.go\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\nfunc GetManualExtract(dbName string, tableName string, http extract.ReqParam) extract.Extract {\n\tdb, ok := finance_info.GlobalFinanceInfoManager[dbName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tbasic, ok := db.TableBasic[tableName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif basic.Datasource == \"PG\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestPG{","relevantFileList":[],"filePath":"app/server/http.go","template":"go","multiRes":{"hipilot":"\n\n// pushmsg\nfunc pushMsgHandler(c *gin.Context) {\n    var req my_orm.PushMessage\n    c.ShouldBind(&req)\n    go my_orm.PushMessageSend(req)\n    c.JSON(200, \"success","CodeQwen2.5-7B":"","copilot":"// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tmy_orm.F10Update\n\n\tc.String(200, \"ok\")\n}\n","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.Post(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {","infill":"// todo 推送消息\n\tvar msgmy_orm.F10Update\n\n\tc.String(200, \"ok\")\n}\n","suffix":"","relevantFile":"<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract_manager/extract_manager.go\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\nfunc GetManualExtract(dbName string, tableName string, http extract.ReqParam) extract.Extract {\n\tdb, ok := finance_info.GlobalFinanceInfoManager[dbName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tbasic, ok := db.TableBasic[tableName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif basic.Datasource == \"PG\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestPG{","relevantFileList":[],"filePath":"app/server/http.go","template":"go","multiRes":{"hipilot":"\n    err := my_orm.PushMsgOrm(c)\n    if err != nil {\n        c.String(400, err.Error())\n    }\n    c.String(200, \"success\")\n}\n","CodeQwen2.5-7B":"\n\tmsg := c.PostForm(\"msg\")\n\tif msg == \"\" {\n\t\tc.String(400, \"msg is empty\")\n\t\treturn\n\t}\n\tmy_orm.PushMsg(msg)\n\tc.String(0, \"ok\")\n}","copilot":"// todo 推送消息\n\tvar msgmy_orm.F10Update\n\n\tc.String(200, \"ok\")\n}\n","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.Post(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tvar msg my_orm.F10Update\n\terr := c.BindJSON(&msg)\n\n\tc","infill":"\n\n\tc","suffix":"\n\n\tc.String(200, \"ok\")\n}\n","relevantFile":"<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract_manager/extract_manager.go\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\nfunc GetManualExtract(dbName string, tableName string, http extract.ReqParam) extract.Extract {\n\tdb, ok := finance_info.GlobalFinanceInfoManager[dbName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tbasic, ok := db.TableBasic[tableName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif basic.Datasource == \"PG\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestPG{\n\t\t\t\tDatasource:   my_orm.RequestTcpPg,","relevantFileList":[],"filePath":"app/server/http.go","template":"go","multiRes":{"hipilot":"if err != nil {\n        c.String(400, err.Error())\n        return\n    }","CodeQwen2.5-7B":".JSON(200, err)","copilot":"\n\n\tc","deepseek-chat-lite":".JSON(200, msg)","CodeQwen1.5-7B":" .JSON(200, msg)"}},{"prefix":"package server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"go.uber.org/zap\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.Post(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tvar msg my_orm.F10Update\n\terr := c.BindJSON(&msg)\n\tif err != nil {","infill":"c.String(400, \"bind json failed\")\n\t\treturn\n\t}\n\n\tlevellog.Log.Info(\"push msg\", zap.Any(\"msg\", msg))\n\n\t","suffix":"\n\n\tc.String(200, \"ok\")\n}\n","relevantFile":"<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract_manager/extract_manager.go\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\nfunc GetManualExtract(dbName string, tableName string, http extract.ReqParam) extract.Extract {\n\tdb, ok := finance_info.GlobalFinanceInfoManager[dbName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tbasic, ok := db.TableBasic[tableName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif basic.Datasource == \"PG\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestPG{\n\t\t\t\tDatasource:   my_orm.RequestTcpPg,","relevantFileList":[],"filePath":"app/server/http.go","template":"go","multiRes":{"hipilot":"\n        zap.S().Errorw(\"pushMsgHandler c.BindJSON failed\", \"err\", err)\n        c.String(500, err.Error())\n        return\n    }","CodeQwen2.5-7B":"\n\t\tzap.S().Errorf(\"bind json error: %s\", err.Error())\n\t\tc.String(400, \"bind json error\")\n\t\treturn\n\t}\n\terr = my_orm.PushMsg(msg)\n\tif err != nil {\n\t\tzap.S().Errorf(\"push msg error: %s\", err.Error())\n\t\tc.String(400, \"push msg error\")\n\t\treturn\n\t}","copilot":"c.String(400, \"bind json failed\")\n\t\treturn\n\t}\n\n\tlevellog.Log.Info(\"push msg\", zap.Any(\"msg\", msg))\n\n\t","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"go.uber.org/zap\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.Post(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tvar msg my_orm.F10Update\n\terr := c.BindJSON(&msg)\n\tif err != nil {\n\t\tc.String(400, \"bind json failed\")","infill":"return\n\t}\n\n\tlevellog.Log.Info(\"push msg\", zap.Any(\"msg\", msg))\n\n\t","suffix":"\n\n\tc.String(200, \"ok\")\n}\n","relevantFile":"<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract_manager/extract_manager.go\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\nfunc GetManualExtract(dbName string, tableName string, http extract.ReqParam) extract.Extract {\n\tdb, ok := finance_info.GlobalFinanceInfoManager[dbName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tbasic, ok := db.TableBasic[tableName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif basic.Datasource == \"PG\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestPG{\n\t\t\t\tDatasource:   my_orm.RequestTcpPg,","relevantFileList":[],"filePath":"app/server/http.go","template":"go","multiRes":{"hipilot":"\n        return\n    }","CodeQwen2.5-7B":"\n\t\treturn\n\t}\n\terr = my_orm.PushMsg(&msg)\n\tif err != nil {\n\t\tc.String(400, err.Error())\n\t\treturn\n\t}","copilot":"return\n\t}\n\n\tlevellog.Log.Info(\"push msg\", zap.Any(\"msg\", msg))\n\n\t","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype Kafk","infill":"(d.ctx, &sarama.ConsumerMessage{\n\t\tValue: []byte(f10Update.Data),\n\t})","suffix":"\n}\n\nfunc (d KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\td.chErr <- err\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n\tclose(d.chErr)\n}\n","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n\n\tDebugPushMessage(my_orm.F10Update)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n\nfunc (e AutoExtract) DebugPushMessage(f10Update my_orm.F10Update) {\n\te.Update.DebugPushMessage(f10Update)\n}\n<file_path>app/extract_manager/extract_manager.go\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"go.uber.org/zap\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n<file_path>app/extract/automatic/update_cron_fUsEtfAdjFactor.go\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support table %s\", d.FinanceTable)\n\t}\n\n\treq := manual.DataRequestFUsEtfAdjFactor{\n\t\tDatasource:   my_orm.RequestHTTPFUsEtfAdjFactor,\n\t\tUrl:          config.GetFUsEtfAdjFactor().Url,\n\t\tFinanceTable: d.FinanceTable,\n\t\tFinanceDb:    d.FinanceDb,\n\t\tTriggerType:  my_orm.UpdateByCron,\n\t}\n\n\ttimes := strings.Split(taskTable.Cron, \";\")\n\tfor _, t := range times {\n\t\tif t == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\ttaskName := d.FinanceDb + d.FinanceTable\n\t\t// 定时任务的回调函数\n\t\tcallback := func() {\n\t\t\tretryCnt := 3\n\t\t\tfor i := 0; i < retryCnt; i++ {\n\t\t\t\trows, err := req.RequestData()\n\t\t\t\tif err == nil {\n\t\t\t\t\tlevellog.Log.Info(fmt.Sprintf(\"export data by cron successfully\"), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"affects row\", rows), zap.Int(\"retry\", i))\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif i == retryCnt-1 {\n\t\t\t\t\tlevellog.Log.Error(fmt.Sprintf(\"export data by cron failed: %s\", err.Error()), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t} else {\n\t\t\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"export data failed: %s, start to retry\", err.Error()), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t\ttime.Sleep(time.Duration(5) * time.Second)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr := cron.GCronManager.AddTask(taskName, t, callback)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"add task failed\"), zap.String(\"table\", d.FinanceTable), zap.String(\"db\", d.FinanceDb), zap.String(\"cron\", t))\n\t\t\tcontinue\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (d UpdateDataCronFUsEtfAdjFactor) Start() (int, error) {\n\treturn 0, d.AddTasks()\n}\n\nfunc (d UpdateDataCronFUsEtfAdjFactor) Stop() (int, error) {\n\ttaskName := d.FinanceDb + d.FinanceTable\n\tcron.GCronManager.RemoveTask(taskName)\n\treturn 0, nil\n}\n\nfunc (d UpdateDataCronFUsEtfAdjFactor) DebugPushMessage(f10Update my_orm.F10Update) {\n}\n<file_path>app/extract/automatic/update_cron_pg.go\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support database %s\", d.FinanceDb)\n\t}\n\ttaskTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support table %s\", d.FinanceTable)\n\t}\n\treqPG := manual.DataRequestPG{\n\t\tDatasource:   my_orm.RequestTcpPg,\n\t\tFinanceTable: d.FinanceTable,\n\t\tFinanceDb:    d.FinanceDb,\n\t\tTriggerType:  my_orm.UpdateByCron,\n\t}\n\ttimes := strings.Split(taskTable.Cron, \";\")\n\tfor _, t := range times {\n\t\tif t == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\ttaskName := d.FinanceDb + d.FinanceTable\n\t\t// 定时任务的回调函数\n\t\tcallback := func() {\n\t\t\tfor i := 0; i < retryCnt; i++ {\n\t\t\t\trows, err := reqPG.RequestData()\n\t\t\t\tif err == nil {\n\t\t\t\t\tlevellog.Log.Info(fmt.Sprintf(\"export data by cron successfully\"), zap.String(\"table\", reqPG.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", reqPG.FinanceDb), zap.Int(\"affects row\", rows), zap.Int(\"retry\", i))\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif i == retryCnt-1 {\n\t\t\t\t\tlevellog.Log.Error(fmt.Sprintf(\"export data by cron failed: %s\", err.Error()), zap.String(\"table\", reqPG.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", reqPG.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t} else {\n\t\t\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"export data failed: %s, start to retry\", err.Error()), zap.String(\"table\", reqPG.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", reqPG.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t\ttime.Sleep(time.Duration(5) * time.Second)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr := cron.GCronManager.AddTask(taskName, t, callback)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"add task failed\"), zap.String(\"table\", d.FinanceTable), zap.String(\"db\", d.FinanceDb), zap.String(\"cron\", t))\n\t\t\tcontinue\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (d UpdateDataCronPg) Start() (int, error) {\n\treturn 0, d.AddTasks()\n}\n\nfunc (d UpdateDataCronPg) Stop() (int, error) {\n\ttaskName := d.FinanceDb + d.FinanceTable\n\tcron.GCronManager.RemoveTask(taskName)\n\treturn 0, nil\n}\n\nfunc (d UpdateDataCronPg) DebugPushMessage(f10Update my_orm.F10Update) {\n}","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka.go","template":"go","multiRes":{"hipilot":"aConsumer struct {\n    ctx           context.Context\n    consumerGroup sarama.ConsumerGroup\n    consumer       Consumer\n    Task          KafkaTaskInfo\n    chErr         chan error","CodeQwen2.5-7B":"aConsumer struct {\n\tctx        context.Context\n\tconsumer   sarama.ConsumerGroupSession\n\tconsumerGroup sarama.ConsumerGroup\n\tTask       KafkaTaskInfo\n\tchErr      chan error\n}","copilot":"(d.ctx, &sarama.ConsumerMessage{\n\t\tValue: []byte(f10Update.Data),\n\t})","deepseek-chat-lite":"aConsumer struct {","CodeQwen1.5-7B":"aConsumer struct {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype Kafk","infill":"Value: []byte(f10Update.Data),\n\t})","suffix":"\n}\n\nfunc (d KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\td.chErr <- err\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n\tclose(d.chErr)\n}\n","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n\n\tDebugPushMessage(my_orm.F10Update)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n\nfunc (e AutoExtract) DebugPushMessage(f10Update my_orm.F10Update) {\n\te.Update.DebugPushMessage(f10Update)\n}\n<file_path>app/extract_manager/extract_manager.go\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"go.uber.org/zap\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n<file_path>app/extract/automatic/update_cron_fUsEtfAdjFactor.go\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support table %s\", d.FinanceTable)\n\t}\n\n\treq := manual.DataRequestFUsEtfAdjFactor{\n\t\tDatasource:   my_orm.RequestHTTPFUsEtfAdjFactor,\n\t\tUrl:          config.GetFUsEtfAdjFactor().Url,\n\t\tFinanceTable: d.FinanceTable,\n\t\tFinanceDb:    d.FinanceDb,\n\t\tTriggerType:  my_orm.UpdateByCron,\n\t}\n\n\ttimes := strings.Split(taskTable.Cron, \";\")\n\tfor _, t := range times {\n\t\tif t == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\ttaskName := d.FinanceDb + d.FinanceTable\n\t\t// 定时任务的回调函数\n\t\tcallback := func() {\n\t\t\tretryCnt := 3\n\t\t\tfor i := 0; i < retryCnt; i++ {\n\t\t\t\trows, err := req.RequestData()\n\t\t\t\tif err == nil {\n\t\t\t\t\tlevellog.Log.Info(fmt.Sprintf(\"export data by cron successfully\"), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"affects row\", rows), zap.Int(\"retry\", i))\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif i == retryCnt-1 {\n\t\t\t\t\tlevellog.Log.Error(fmt.Sprintf(\"export data by cron failed: %s\", err.Error()), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t} else {\n\t\t\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"export data failed: %s, start to retry\", err.Error()), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t\ttime.Sleep(time.Duration(5) * time.Second)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr := cron.GCronManager.AddTask(taskName, t, callback)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"add task failed\"), zap.String(\"table\", d.FinanceTable), zap.String(\"db\", d.FinanceDb), zap.String(\"cron\", t))\n\t\t\tcontinue\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (d UpdateDataCronFUsEtfAdjFactor) Start() (int, error) {\n\treturn 0, d.AddTasks()\n}\n\nfunc (d UpdateDataCronFUsEtfAdjFactor) Stop() (int, error) {\n\ttaskName := d.FinanceDb + d.FinanceTable\n\tcron.GCronManager.RemoveTask(taskName)\n\treturn 0, nil\n}\n\nfunc (d UpdateDataCronFUsEtfAdjFactor) DebugPushMessage(f10Update my_orm.F10Update) {\n}\n<file_path>app/extract/automatic/update_cron_pg.go\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support database %s\", d.FinanceDb)\n\t}\n\ttaskTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support table %s\", d.FinanceTable)\n\t}\n\treqPG := manual.DataRequestPG{\n\t\tDatasource:   my_orm.RequestTcpPg,\n\t\tFinanceTable: d.FinanceTable,\n\t\tFinanceDb:    d.FinanceDb,\n\t\tTriggerType:  my_orm.UpdateByCron,\n\t}\n\ttimes := strings.Split(taskTable.Cron, \";\")\n\tfor _, t := range times {\n\t\tif t == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\ttaskName := d.FinanceDb + d.FinanceTable\n\t\t// 定时任务的回调函数\n\t\tcallback := func() {\n\t\t\tfor i := 0; i < retryCnt; i++ {\n\t\t\t\trows, err := reqPG.RequestData()\n\t\t\t\tif err == nil {\n\t\t\t\t\tlevellog.Log.Info(fmt.Sprintf(\"export data by cron successfully\"), zap.String(\"table\", reqPG.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", reqPG.FinanceDb), zap.Int(\"affects row\", rows), zap.Int(\"retry\", i))\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif i == retryCnt-1 {\n\t\t\t\t\tlevellog.Log.Error(fmt.Sprintf(\"export data by cron failed: %s\", err.Error()), zap.String(\"table\", reqPG.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", reqPG.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t} else {\n\t\t\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"export data failed: %s, start to retry\", err.Error()), zap.String(\"table\", reqPG.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", reqPG.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t\ttime.Sleep(time.Duration(5) * time.Second)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr := cron.GCronManager.AddTask(taskName, t, callback)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"add task failed\"), zap.String(\"table\", d.FinanceTable), zap.String(\"db\", d.FinanceDb), zap.String(\"cron\", t))\n\t\t\tcontinue\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (d UpdateDataCronPg) Start() (int, error) {\n\treturn 0, d.AddTasks()\n}\n\nfunc (d UpdateDataCronPg) Stop() (int, error) {\n\ttaskName := d.FinanceDb + d.FinanceTable\n\tcron.GCronManager.RemoveTask(taskName)\n\treturn 0, nil\n}\n\nfunc (d UpdateDataCronPg) DebugPushMessage(f10Update my_orm.F10Update) {\n}","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka.go","template":"go","multiRes":{"hipilot":"aConsumer struct {\n    ctx           context.Context\n    consumerGroup sarama.ConsumerGroup\n    consumer      MyConsumer\n    Task          KafkaTaskInfo\n    chErr         chan error","CodeQwen2.5-7B":"aConsumer struct {\n\tctx        context.Context\n\tconsumer   sarama.ConsumerGroupSession\n\tconsumerGroup sarama.ConsumerGroup\n\tTask       KafkaTaskInfo\n\tchErr      chan error\n}","copilot":"Value: []byte(f10Update.Data),\n\t})","deepseek-chat-lite":"aConsumer struct {","CodeQwen1.5-7B":"aConsumer struct {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) GetF10DataByRequest(f10TaskId string, indicatorId string, subjects string, date string) (my_orm.F10Responses, error) {\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorId, subjects, currency)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn jsonF10Rsp, err\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn jsonF10Rsp, nil\n\t}\n\n\t// 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorId, subjects, currency)\n\t}\n\n\treturn jsonF10Rsp, nil\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, currency)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(ids, subjects, currency)\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\tdateStr, ok := jsonF10Rsp.Data.Data[0].Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// kafka 推送数据为原生数据，这里再请求一次指标数据，\n// indicatorIDs: 指标ID列表\n// subjects: 股票代码列表\n// currency: 货币类型\n// date: 查询日期，mo'ren\n// 该函数返回一个F10Responses对象和一个错误。如果请求成功，错误将为nil。\n// 如果请求失败，错误将包含失败的详细信息。\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs []string, subjects []string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err :=","infill":"s.Join(subjects, \",\"),","suffix":"\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"http.NewRequest(\"GET\", d.Url, nil)\n    requestBody := my_orm.F10Request{\n        IndicatorIDs: indicatorIDs,\n        Subjects:      subjects,","CodeQwen2.5-7B":" http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"new http req failed in kafka thread\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrNewHttpRequestFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\treq.Header.Set(\"Accept\", \"application/json\")\n\treq.Header.Set(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\")\n\treq.Header.Set(\"Referer\", \"https://www.xueqiu.com/\")\n\treq.Header.Set(\"X-Requested-With\", \"XMLHttpRequest\")\n\treq.Header.Set(\"X-Requested-By\", \"xueqiu\")\n\treq.Header.Set(\"X-VERSION\", \"1.0.0\")\n\treq.Header.Set(\"X-APP-VERSION\", \"1.0.0\")\n\treq.Header.Set(\"X-APP-NAME\", \"xueqiu\")\n\treq.Header.Set(\"X-APP-ID\", \"xueqiu\")\n\treq.Header.Set(\"X-APP-SECRET\", \"xueqiu\")\n\treq.Header.Set(\"X-APP-TYPE\", \"web\")\n\treq.Header.Set(\"X-APP-OS\", \"web\")\n\treq.Header.Set(\"X-APP-","copilot":"s.Join(subjects, \",\"),","deepseek-chat-lite":" http.NewRequest(\"GET\", d.Url, nil)","CodeQwen1.5-7B":" http.NewRequest(\"GET\", d.Url, nil)"}},{"prefix":"package server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"go.uber.org/zap\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.POST(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows","infill":" == nil {\n\t\tc.String(400, \"f10 kafka not init\")\n\t\treturn\n\t}","suffix":"\n\n\textract_manager.GlobalExtractManager.F10Kafka.DebugPushMessage(msg)\n\tc.String(200, \"ok\")\n}\n","relevantFile":"<file_path>app/extract/extract_api.go\npackage extract\n\n// Extract 数据抽取接口定义\ntype Extract interface {\n\tGetMarket() string       // 获取抽取器所属市场名\n\tGetTable() string        // 获取抽取器抽取的表名\n\tGetTrigMod() int         // 获取抽取触发类型（主动 or 被动）\n\tDoExtract() (int, error) // 触发数据更新,包括手动触发与自动更新\n\tStop() (int, error)\n\n\tDebugPushMessage([]byte) // 调试接口\n}\n<file_path>app/extract/automatic/update_kafka.go\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) Stop() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\treturn 0, nil\n\t}\n\td.cancel()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) DebugPushMessage(msg []byte) {\n\tmessage := &sarama.ConsumerMessage{\n\t\tValue: msg,\n\t\tTopic: \"om_debug_push_messsage\",\n\t}\n\td.consumer.Handle(d.ctx, message)\n}\n\nfunc (d KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\td.chErr <- err\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n\tclose(d.chErr)\n}\n<file_path>app/extract/automatic/update_cron_fUsEtfAdjFactor.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"go.uber.org/zap\"\n)\n\ntype UpdateDataCronFUsEtfAdjFactor struct {\n\tFinanceDb    string // 财务数据库\n\tFinanceTable string // 财务数据表\n}\n\nfunc (d UpdateDataCronFUsEtfAdjFactor) AddTasks() error {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support database %s\", d.FinanceDb)\n\t}\n\ttaskTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support table %s\", d.FinanceTable)\n\t}\n\n\treq := manual.DataRequestFUsEtfAdjFactor{\n\t\tDatasource:   my_orm.RequestHTTPFUsEtfAdjFactor,\n\t\tUrl:          config.GetFUsEtfAdjFactor().Url,\n\t\tFinanceTable: d.FinanceTable,\n\t\tFinanceDb:    d.FinanceDb,\n\t\tTriggerType:  my_orm.UpdateByCron,\n\t}\n\n\ttimes := strings.Split(taskTable.Cron, \";\")\n\tfor _, t := range times {\n\t\tif t == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\ttaskName := d.FinanceDb + d.FinanceTable\n\t\t// 定时任务的回调函数\n\t\tcallback := func() {\n\t\t\tretryCnt := 3\n\t\t\tfor i := 0; i < retryCnt; i++ {\n\t\t\t\trows, err := req.RequestData()\n\t\t\t\tif err == nil {\n\t\t\t\t\tlevellog.Log.Info(fmt.Sprintf(\"export data by cron successfully\"), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"affects row\", rows), zap.Int(\"retry\", i))\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif i == retryCnt-1 {\n\t\t\t\t\tlevellog.Log.Error(fmt.Sprintf(\"export data by cron failed: %s\", err.Error()), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t} else {\n\t\t\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"export data failed: %s, start to retry\", err.Error()), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"retry\", i))\n<file_path>app/extract_manager/extract_manager.go\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\nfunc GetManualExtract(dbName string, tableName string, http extract.ReqParam) extract.Extract {\n\tdb, ok := finance_info.GlobalFinanceInfoManager[dbName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tbasic, ok := db.TableBasic[tableName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif basic.Datasource == \"PG\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestPG{","relevantFileList":[],"filePath":"app/server/http.go","template":"go","multiRes":{"hipilot":", err := extract.Extract()\n    if err != nil {\n        levellog.Log.Error(\"extract err\", zap.Error(err))\n        c.String(500, err.Error())\n        metrics.ErrorMetricsInc(metrics.","CodeQwen2.5-7B":" := extract.Extract()\n\tif rows == nil {\n\t\tc.String(400, \"Extract data failed\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrExtractDataFailed)\n\t\treturn\n\t}\n\tc.String(200, \"ok\")\n}","copilot":" == nil {\n\t\tc.String(400, \"f10 kafka not init\")\n\t\treturn\n\t}","deepseek-chat-lite":", err := extract.Extract()","CodeQwen1.5-7B":" := extract.Extract()"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket string `json:\"market\"`\n\tCode   string `json:\"code\"`","infill":"Date   string `json:\"date\"`\n\tAss","suffix":"\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indi","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n\tData   []IndicatorInfo `json:\"data\"`\n}","copilot":"Date   string `json:\"date\"`\n\tAss","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket string `json:\"market\"`\n\tCode   string `json:\"code\"`\n\tDate   string ","infill":"`json:\"date\"`\n\t","suffix":"\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indi","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"`json:\"date\"`","CodeQwen2.5-7B":"`json:\"date\"`\n\tData []map[string]string `json:\"data\"`\n}","copilot":"`json:\"date\"`\n\t","deepseek-chat-lite":"`json:\"date\"`","CodeQwen1.5-7B":" `json:\"date\"`"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData ","infill":"my_orm.AssetData `json:\"assetData\"`","suffix":"\n}\n\nfunc \n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"json.RawMessage `json:\"data\"`","CodeQwen2.5-7B":"[]IndicatorInfo `json:\"data\"`\n}","copilot":"my_orm.AssetData `json:\"assetData\"`","deepseek-chat-lite":"[]IndicatorInfo `json:\"assetData\"`","CodeQwen1.5-7B":" []IndicatorData `json:\"data\"`"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")","infill":"if err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicat","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"\n        if err != nil {\n            levellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n            return market, thsCode, assetData, err\n       ","CodeQwen2.5-7B":"\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n}","copilot":"if err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))","infill":"return market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n}","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicat","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"\n            return market, thsCode, assetData, err\n        }\n    }\n    assetData = jsonF10Rsp.Data.Data[0]\n    return market, thsCode, assetData, err","CodeQwen2.5-7B":"\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n}","copilot":"return market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (IndicatorInfoFromF10, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn IndicatorInfoFromF10{}, err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\n\tindicatorInfoFromF10 := IndicatorInfoFromF10{\n\t\tMarket:    market,\n\t\tCode:      thsCode,\n\t\tCurrency:  currency,\n\t\tDate:      assetData.Data[0][taskInfo.Date],\n\t\tAssetData: assetData,\n\t}\n\n\tif !ok {\n\t\treturn indicatorInfoFromF10, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn indicatorInfoFromF10, err\n\t\t}\n\n\t}\n\tindicatorInfoFromF10.Currency","infill":" = currency\n\tindicatorInfoFromF10.AssetData = jsonF10Rsp.Data.Data[0]","suffix":"\n\treturn indicatorInfoFromF10, err\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\n\tIndicatorInfoFromF10\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n/","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"AssetData.Data[0][taskInfo.Date] = assetData.Data[0][taskInfo.Date]","CodeQwen2.5-7B":" = currency\n\tindicatorInfoFromF10.Date = assetData.Data[0][taskInfo.Date]\n\tindicatorInfoFromF10.AssetData = assetData\n","copilot":" = currency\n\tindicatorInfoFromF10.AssetData = jsonF10Rsp.Data.Data[0]","deepseek-chat-lite":" = currency","CodeQwen1.5-7B":" =currency"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (IndicatorInfoFromF10, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn IndicatorInfoFromF10{}, err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\n\tindicatorInfoFromF10 := IndicatorInfoFromF10{\n\t\tMarket:    market,\n\t\tCode:      thsCode,\n\t\tCurrency:  currency,\n\t\tDate:      assetData.Data[0][taskInfo.Date],\n\t\tAssetData: assetData,\n\t}\n\n\tif !ok {\n\t\treturn indicatorInfoFromF10, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn indicatorInfoFromF10, err\n\t\t}\n\n\t}\n\tindicatorInfoFromF10.Currency = currency\n\tindicatorInfoFromF10.AssetData = jsonF10Rsp.Data.Data[0]\n\treturn indicatorInfoFromF10, err\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in l","infill":"r := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)","suffix":"\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"<file_path>app/orm/f10.go\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {\n\t\tSubject SubjectData         `json:\"subject\"`","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"ocal database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}","copilot":"r := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)","deepseek-chat-lite":"ocal database\", zap.String(\"taskid\", f10TaskId))","CodeQwen1.5-7B":" ocal database\", zap.String(\"taskid\", f10TaskId))"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"os/exec\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPI","infill":"VersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tContainers, err := client.ContainerList(context.Background(), types.ContainerListOptions{})\n","suffix":"\n\t// Execute 'docker ps -a' command\n\tout, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Split the output into lines\n\tlines := strings.Split(string(out), \"\\n\")\n\n\tlog.Println(lines)\n\n\t// Parse the lines into a list of Container objects\n\tcontainers := []Container{}\n\tfor _, line := range lines { // Skip the header line\n\t\tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t\t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t\t})\n\t\tlog.Println(fields)\n\t\tif len(fields) > 0 {\n\t\t\tcontainers = append(containers, Container{\n\t\t\t\tID:     fields[0],\n\t\t\t\tImage:  fields[1],\n\t\t\t\tName:   fields[2],\n\t\t\t\tStatus: strings.ToUpper(fields[3]),\n\t\t\t\t// Ports:   fields[4],\n\t\t\t\tLastStarted: fields[4],\n\t\t\t})\n\t\t}\n\t}\n\treturn containers, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"VersionNegotiation())","CodeQwen2.5-7B":"Version(\"1.41\"))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer client.Close()","copilot":"VersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tContainers, err := client.ContainerList(context.Background(), types.ContainerListOptions{})\n","deepseek-chat-lite":"VersionNegotiation())","CodeQwen1.5-7B":"VersionNegotiation())"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"os/exec\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/client\"","infill":"\"github.com/gin-gonic/gin\"","suffix":"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tContainers, err := client.ContainerList(context.Background(), filter\n\n\t// Execute 'docker ps -a' command\n\tout, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Split the output into lines\n\tlines := strings.Split(string(out), \"\\n\")\n\n\tlog.Println(lines)\n\n\t// Parse the lines into a list of Container objects\n\tcontainers := []Container{}\n\tfor _, line := range lines { // Skip the header line\n\t\tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t\t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t\t})\n\t\tlog.Println(fields)\n\t\tif len(fields) > 0 {\n\t\t\tcontainers = append(containers, Container{\n\t\t\t\tID:     fields[0],\n\t\t\t\tImage:  fields[1],\n\t\t\t\tName:   fields[2],\n\t\t\t\tStatus: strings.ToUpper(fields[3]),\n\t\t\t\t// Ports:   fields[4],\n\t\t\t\tLastStarted: fields[4],\n\t\t\t})\n\t\t}\n\t}\n\treturn containers, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n\t\"github.com/gin-gonic/gin\"","copilot":"\"github.com/gin-gonic/gin\"","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(","infill":"containers","suffix":")\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"containers","CodeQwen2.5-7B":"\"Containers: \", containers","copilot":"containers","deepseek-chat-lite":"containers)","CodeQwen1.5-7B":"containers)"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll: true,\n\t\tFilters: filters.Args{\n\t\t\tfilters: map[string]map[string]bool{","infill":"\"name\": {\n\t\t\t\t\t\"(_|-)dev\": true,\n\t\t\t\t},","suffix":"\n\t\t\t},\n\t\t}})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n<file_path>parse.go\nmultiple values.\n*/\npackage filters // import \"github.com/docker/docker/api/types/filters\"\n\nimport (\n\t\"encoding/json\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// Args stores a mapping of keys to a set of multiple values.\ntype Args struct {\n\tfields map[string]map[string]bool\n}\n\n// KeyValuePair are used to initialize a new Args\ntype KeyValuePair struct {\n\tKey   string\n\tValue string\n}\n\n// Arg creates a new KeyValuePair for initializing Args\nfunc Arg(key, value string) KeyValuePair {\n\treturn KeyValuePair{Key: key, Value: value}\n}\n\n// NewArgs returns a new Args populated with the initial args\nfunc NewArgs(initialArgs ...KeyValuePair) Args {\n\targs := Args{fields: map[string]map[string]bool{}}\n\tfor _, arg := range initialArgs {\n\t\targs.Add(arg.Key, arg.Value)\n\t}\n\treturn args\n}\n\n// Keys returns all the keys in list of Args\nfunc (args Args) Keys() []string {\n\tkeys := make([]string, 0, len(args.fields))\n\tfor k := range args.fields {\n\t\tkeys = append(keys, k)\n\t}\n\treturn keys\n}\n\n// MarshalJSON returns a JSON byte representation of the Args\nfunc (args Args) MarshalJSON() ([]byte, error) {\n\tif len(args.fields) == 0 {\n\t\treturn []byte(\"{}\"), nil\n\t}\n\treturn json.Marshal(args.fields)\n}\n\n// ToJSON returns the Args as a JSON encoded string\nfunc ToJSON(a Args) (string, error) {\n\tif a.Len() == 0 {\n\t\treturn \"\", nil\n\t}\n\tbuf, err := json.Marshal(a)","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"\"label\": {\"com.github.copilot\": true}},","CodeQwen2.5-7B":"\n\t\t\t\t\"name\": map[string]bool{\n\t\t\t\t\t\"(_|-)dev$\": true,\n\t\t\t\t},<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"\"name\": {\n\t\t\t\t\t\"(_|-)dev\": true,\n\t\t\t\t},","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll: true,\n\t\tFilters: filters.Args{\n\t\t\tfields: map[string]map[string]bool{\n\t\t\t\t\"name\": {\n\t\t\t\t\t\"(_|-)dev\": true,\n\t\t\t\t}","infill":",\n","suffix":"\t\t\t},\n\t\t}})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n<file_path>parse.go\nmultiple values.\n*/\npackage filters // import \"github.com/docker/docker/api/types/filters\"\n\nimport (\n\t\"encoding/json\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// Args stores a mapping of keys to a set of multiple values.\ntype Args struct {\n\tfields map[string]map[string]bool\n}\n\n// KeyValuePair are used to initialize a new Args\ntype KeyValuePair struct {\n\tKey   string\n\tValue string\n}\n\n// Arg creates a new KeyValuePair for initializing Args\nfunc Arg(key, value string) KeyValuePair {\n\treturn KeyValuePair{Key: key, Value: value}\n}\n\n// NewArgs returns a new Args populated with the initial args\nfunc NewArgs(initialArgs ...KeyValuePair) Args {\n\targs := Args{fields: map[string]map[string]bool{}}\n\tfor _, arg := range initialArgs {\n\t\targs.Add(arg.Key, arg.Value)\n\t}\n\treturn args\n}\n\n// Keys returns all the keys in list of Args\nfunc (args Args) Keys() []string {\n\tkeys := make([]string, 0, len(args.fields))\n\tfor k := range args.fields {\n\t\tkeys = append(keys, k)\n\t}\n\treturn keys\n}\n\n// MarshalJSON returns a JSON byte representation of the Args\nfunc (args Args) MarshalJSON() ([]byte, error) {\n\tif len(args.fields) == 0 {\n\t\treturn []byte(\"{}\"), nil\n\t}\n\treturn json.Marshal(args.fields)\n}\n\n// ToJSON returns the Args as a JSON encoded string\nfunc ToJSON(a Args) (string, error) {\n\tif a.Len() == 0 {\n\t\treturn \"\", nil\n\t}\n\tbuf, err := json.Marshal(a)","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":",","CodeQwen2.5-7B":",\n","copilot":",\n","deepseek-chat-lite":",","CodeQwen1.5-7B":","}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter ","infill":":= filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")","suffix":"\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n<file_path>parse.go\nmultiple values.\n*/\npackage filters // import \"github.com/docker/docker/api/types/filters\"\n\nimport (\n\t\"encoding/json\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// Args stores a mapping of keys to a set of multiple values.\ntype Args struct {\n\tfields map[string]map[string]bool\n}\n\n// KeyValuePair are used to initialize a new Args\ntype KeyValuePair struct {\n\tKey   string\n\tValue string\n}\n\n// Arg creates a new KeyValuePair for initializing Args\nfunc Arg(key, value string) KeyValuePair {\n\treturn KeyValuePair{Key: key, Value: value}\n}\n\n// NewArgs returns a new Args populated with the initial args\nfunc NewArgs(initialArgs ...KeyValuePair) Args {\n\targs := Args{fields: map[string]map[string]bool{}}\n\tfor _, arg := range initialArgs {\n\t\targs.Add(arg.Key, arg.Value)\n\t}\n\treturn args\n}\n\n// Keys returns all the keys in list of Args\nfunc (args Args) Keys() []string {\n\tkeys := make([]string, 0, len(args.fields))\n\tfor k := range args.fields {\n\t\tkeys = append(keys, k)\n\t}\n\treturn keys\n}\n\n// MarshalJSON returns a JSON byte representation of the Args\nfunc (args Args) MarshalJSON() ([]byte, error) {\n\tif len(args.fields) == 0 {\n\t\treturn []byte(\"{}\"), nil\n\t}\n\treturn json.Marshal(args.fields)\n}\n\n// ToJSON returns the Args as a JSON encoded string\nfunc ToJSON(a Args) (string, error) {\n\tif a.Len() == 0 {\n\t\treturn \"\", nil\n\t}\n\tbuf, err := json.Marshal(a)","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":":= filters.NewArgs()","CodeQwen2.5-7B":":= filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")","copilot":":= filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")","deepseek-chat-lite":":= filters.NewArgs()","CodeQwen1.5-7B":" := filters.NewArgs()"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()","infill":"filter.Add(\"name\", \"(_|-)dev\")","suffix":"\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n<file_path>parse.go\nmultiple values.\n*/\npackage filters // import \"github.com/docker/docker/api/types/filters\"\n\nimport (\n\t\"encoding/json\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// Args stores a mapping of keys to a set of multiple values.\ntype Args struct {\n\tfields map[string]map[string]bool\n}\n\n// KeyValuePair are used to initialize a new Args\ntype KeyValuePair struct {\n\tKey   string\n\tValue string\n}\n\n// Arg creates a new KeyValuePair for initializing Args\nfunc Arg(key, value string) KeyValuePair {\n\treturn KeyValuePair{Key: key, Value: value}\n}\n\n// NewArgs returns a new Args populated with the initial args\nfunc NewArgs(initialArgs ...KeyValuePair) Args {\n\targs := Args{fields: map[string]map[string]bool{}}\n\tfor _, arg := range initialArgs {\n\t\targs.Add(arg.Key, arg.Value)\n\t}\n\treturn args\n}\n\n// Keys returns all the keys in list of Args\nfunc (args Args) Keys() []string {\n\tkeys := make([]string, 0, len(args.fields))\n\tfor k := range args.fields {\n\t\tkeys = append(keys, k)\n\t}\n\treturn keys\n}\n\n// MarshalJSON returns a JSON byte representation of the Args\nfunc (args Args) MarshalJSON() ([]byte, error) {\n\tif len(args.fields) == 0 {\n\t\treturn []byte(\"{}\"), nil\n\t}\n\treturn json.Marshal(args.fields)\n}\n\n// ToJSON returns the Args as a JSON encoded string\nfunc ToJSON(a Args) (string, error) {\n\tif a.Len() == 0 {\n\t\treturn \"\", nil\n\t}\n\tbuf, err := json.Marshal(a)","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n\tfilter.Add(\"name\", \"(_|-)dev$\")","copilot":"filter.Add(\"name\", \"(_|-)dev\")","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerL","infill":"ist := []Container{}\n\tfor _, container := range containers {\n\t\tcontainerList = append(containerList, Container{\n\t\t\tID:          container.ID,\n\t\t\tName:        container.Names[0],\n\t\t\tImage:       container.Image,\n\t\t\tStatus:      container.State,\n\t\t\tPorts:       container.Ports[0].IP + \":\" + container.Ports[0].PublicPort + \"->\" + container.Ports[0].PrivatePort + \"/\" + container.Ports[0].Type,\n\t\t\tLastStarted: container.Created,\n\t\t})\n\t}","suffix":"\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"ist := []Container{}\n    for _, container := range containers {\n        containerList = append(containerList, Container{\n            ID:         container.ID[:10],\n            Name:       container.Names[0][1:],\n            Image:      container.Image,","CodeQwen2.5-7B":"ist := []Container{}\n\tfor _, container := range containers {\n\t\tcontainerList = append(containerList, Container{\n\t\t\tID:          container.ID,\n\t\t\tImage:       container.Image,\n\t\t\tName:        container.Names[0],\n\t\t\tStatus:      container.Status,\n\t\t\tPorts:       container.Ports[0].PublicPort,\n\t\t\tLastStarted: container.CreatedAt,\n\t\t})\n\t}\n\treturn containerList, nil\n}","copilot":"ist := []Container{}\n\tfor _, container := range containers {\n\t\tcontainerList = append(containerList, Container{\n\t\t\tID:          container.ID,\n\t\t\tName:        container.Names[0],\n\t\t\tImage:       container.Image,\n\t\t\tStatus:      container.State,\n\t\t\tPorts:       container.Ports[0].IP + \":\" + container.Ports[0].PublicPort + \"->\" + container.Ports[0].PrivatePort + \"/\" + container.Ports[0].Type,\n\t\t\tLastStarted: container.Created,\n\t\t})\n\t}","deepseek-chat-lite":"ist := []Container{}","CodeQwen1.5-7B":" ist := []Container{}"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tcontainerList = append(containerList, Container{","infill":"ID:          container.ID,\n\t\t\tName:        container.Names[0],\n\t\t\tImage:       container.Image,\n\t\t\tStatus:      container.State,\n\t\t\tPorts:       container.Ports[0].IP + \":\" + container.Ports[0].PublicPort + \"->\" + container.Ports[0].PrivatePort + \"/\" + container.Ports[0].Type,\n\t\t\tLastStarted: container.Created,\n\t\t})\n\t}","suffix":"\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>port.go\npackage types\n\n// This file was generated by the swagger tool.\n// Editing this file might prove futile when you re-run the swagger generate command\n\n// Port An open port on a container\n// swagger:model Port\ntype Port struct {\n\n\t// Host IP address that the container's port is mapped to\n\tIP string `json:\"IP,omitempty\"`\n\n\t// Port on the container\n\t// Required: true\n\tPrivatePort uint16 `json:\"PrivatePort\"`\n\n\t// Port exposed on the host\n\tPublicPort uint16 `json:\"PublicPort,omitempty\"`\n\n\t// type\n\t// Required: true\n\tType string `json:\"Type\"`\n}\n<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"\n            ID:         container.ID,\n            Image:      container.Image,\n            Names:       container.Names[0],\n            Status:     container.Status,\n            Ports:       container.Ports,\n            LastStarted: container.Created,\n        })\n   ","CodeQwen2.5-7B":"\n\t\t\tID:          container.ID,\n\t\t\tName:        container.Names[0],\n\t\t\tImage:       container.Image,\n\t\t\tStatus:      container.Status,\n\t\t\tPorts:       container.Ports[0].PublicPort,\n\t\t\tLastStarted: container.CreatedAt,\n\t\t})\n\t}\n\treturn containerList, nil\n\t// // Create a new Docker client\n\t// client, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }","copilot":"ID:          container.ID,\n\t\t\tName:        container.Names[0],\n\t\t\tImage:       container.Image,\n\t\t\tStatus:      container.State,\n\t\t\tPorts:       container.Ports[0].IP + \":\" + container.Ports[0].PublicPort + \"->\" + container.Ports[0].PrivatePort + \"/\" + container.Ports[0].Type,\n\t\t\tLastStarted: container.Created,\n\t\t})\n\t}","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tcontainerList = append(containerList, Container{\n\t\t\tID:          container.ID,\n\t\t\tName:        container.Names[0],\n\t\t\tImage:       container.Image,\n\t\t\tStatus:      container.State,","infill":"Ports:       container.Ports[0].IP + \":\" + container.Ports[0].PublicPort + \"->\" + container.Ports[0].PrivatePort + \"/\" + container.Ports[0].Type,\n\t\t\tLastStarted: container.Created,\n\t\t})\n\t}","suffix":"\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>port.go\npackage types\n\n// This file was generated by the swagger tool.\n// Editing this file might prove futile when you re-run the swagger generate command\n\n// Port An open port on a container\n// swagger:model Port\ntype Port struct {\n\n\t// Host IP address that the container's port is mapped to\n\tIP string `json:\"IP,omitempty\"`\n\n\t// Port on the container\n\t// Required: true\n\tPrivatePort uint16 `json:\"PrivatePort\"`\n\n\t// Port exposed on the host\n\tPublicPort uint16 `json:\"PublicPort,omitempty\"`\n\n\t// type\n\t// Required: true\n\tType string `json:\"Type\"`\n}\n<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":" // ??","CodeQwen2.5-7B":"\n\t\t\tPorts:       container.Ports[0].PublicPort,\n\t\t\tLastStarted: container.Created.String(),\n\t\t})\n\t}\n\treturn containerList, nil\n\t// // Create a new Docker client\n\t// client, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }","copilot":"Ports:       container.Ports[0].IP + \":\" + container.Ports[0].PublicPort + \"->\" + container.Ports[0].PrivatePort + \"/\" + container.Ports[0].Type,\n\t\t\tLastStarted: container.Created,\n\t\t})\n\t}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tcontainerList = append(containerList, Container{\n\t\t\tID:          container.ID,\n\t\t\tName:        container.Names[0],\n\t\t\tImage:       container.Image,\n\t\t\tStatus:      container.State,\n\t\t\tPorts:       container.Ports[0].IP + \":\" + container.Ports[0].PublicPort + \"->\" + container.Ports[0].PrivatePort + \"/\" + container.Ports[0].Type,","infill":"LastStarted: container.Created,\n\t\t})\n\t}","suffix":"\n\n\t// // Execute 'docker ps -a' command\n\t// out, err := exec.Command(\"docker\", \"ps\", \"-a\", \"--filter\", `name=(_|-)dev$`, \"--format\", `{{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.State}}\\t{{.CreatedAt}}`).Output()\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }\n\n\t// // Split the output into lines\n\t// lines := strings.Split(string(out), \"\\n\")\n\n\t// log.Println(lines)\n\n\t// // Parse the lines into a list of Container objects\n\t// containers := []Container{}\n\t// for _, line := range lines { // Skip the header line\n\t// \tfields := strings.FieldsFunc(line, func(r rune) bool {\n\t// \t\treturn r == '\\t' || r == '\\n' || r == '\\r' || r == '\\v' || r == '\\f'\n\t// \t})\n\t// \tlog.Println(fields)\n\t// \tif len(fields) > 0 {\n\t// \t\tcontainers = append(containers, Container{\n\t// \t\t\tID:     fields[0],\n\t// \t\t\tImage:  fields[1],\n\t// \t\t\tName:   fields[2],\n\t// \t\t\tStatus: strings.ToUpper(fields[3]),\n\t// \t\t\t// Ports:   fields[4],\n\t// \t\t\tLastStarted: fields[4],\n\t// \t\t})\n\t// \t}\n\t// }\n\t// return containers, nil\n\treturn nil, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>port.go\npackage types\n\n// This file was generated by the swagger tool.\n// Editing this file might prove futile when you re-run the swagger generate command\n\n// Port An open port on a container\n// swagger:model Port\ntype Port struct {\n\n\t// Host IP address that the container's port is mapped to\n\tIP string `json:\"IP,omitempty\"`\n\n\t// Port on the container\n\t// Required: true\n\tPrivatePort uint16 `json:\"PrivatePort\"`\n\n\t// Port exposed on the host\n\tPublicPort uint16 `json:\"PublicPort,omitempty\"`\n\n\t// type\n\t// Required: true\n\tType string `json:\"Type\"`\n}\n<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":" // Only grab the first exposed port. Docker usually only exposes a single exposed port.","CodeQwen2.5-7B":"\n\t\t\tLastStarted: container.CreatedAt,\n\t\t})\n\t}\n\treturn containerList, nil\n\t// // Create a new Docker client\n\t// client, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\t// if err != nil {\n\t// \treturn nil, err\n\t// }","copilot":"LastStarted: container.Created,\n\t\t})\n\t}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: container.State,\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tc.Ports += port.IP + \":\" + strconv.Itoa(int(port.PublicPort)) + \"->\" + strconv.Itoa(int(port.PrivatePort)) + \"/\" + port.Type + \"\\","infill":"n\"","suffix":"\n\t\t}\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>itoa.go\n\npackage strconv\n\nimport \"math/bits\"\n\nconst fastSmalls = true // enable fast path for small integers\n\n// FormatUint returns the string representation of i in the given base,\n// for 2 <= base <= 36. The result uses the lower-case letters 'a' to 'z'\n// for digit values >= 10.\nfunc FormatUint(i uint64, base int) string {\n\tif fastSmalls && i < nSmalls && base == 10 {\n\t\treturn small(int(i))\n\t}\n\t_, s := formatBits(nil, i, base, false, false)\n\treturn s\n}\n\n// FormatInt returns the string representation of i in the given base,\n// for 2 <= base <= 36. The result uses the lower-case letters 'a' to 'z'\n// for digit values >= 10.\nfunc FormatInt(i int64, base int) string {\n\tif fastSmalls && 0 <= i && i < nSmalls && base == 10 {\n\t\treturn small(int(i))\n\t}\n\t_, s := formatBits(nil, uint64(i), base, i < 0, false)\n\treturn s\n}\n\n// Itoa is equivalent to FormatInt(int64(i), 10).\nfunc Itoa(i int) string {\n\treturn FormatInt(int64(i), 10)\n}\n\n// AppendInt appends the string form of the integer i,\n// as generated by FormatInt, to dst and returns the extended buffer.\nfunc AppendInt(dst []byte, i int64, base int) []byte {\n\tif fastSmalls && 0 <= i && i < nSmalls && base == 10 {\n\t\treturn append(dst, small(int(i))...)\n\t}\n\tdst, _ = formatBits(dst, uint64(i), base, i < 0, true)\n\treturn dst\n}\n\n// AppendUint appends the string form of the unsigned integer i,\n// as generated by FormatUint, to dst and returns the extended buffer.\nfunc AppendUint(dst []byte, i uint64, base int) []byte {\n\tif fastSmalls && i < nSmalls && base == 10 {\n\t\treturn append(dst, small(int(i))...)\n\t}\n\tdst, _ = formatBits(dst, i, base, false, true)\n\treturn dst\n}\n\n// small returns the string for an i with 0 <= i < nSmalls.\nfunc small(i int) string {\n\tif i < 10 {\n\t\treturn digits[i : i+1]\n\t}\n\treturn smallsString[i*2 : i*2+2]\n<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"n\"","CodeQwen2.5-7B":"n\"\n\t\t}","copilot":"n\"","deepseek-chat-lite":"n\"","CodeQwen1.5-7B":"n\""}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   container.Names[0],\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tc.Ports += port.IP + \":\" + strconv.Itoa(int(port.PublicPort)) + \"->\" + strconv.Itoa(int(port.PrivatePort)) + \"/\" + port.Type + \"\\n\"\n\t\t}\n\n\t\tc.LastStarted = time.Tim\n\n\t\tcontainerList","infill":" = append(containerList, c)\n\t}\n\n\treturn con","suffix":"tainerList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>itoa.go\n// Copyright 2009 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\npackage strconv\n\nimport \"math/bits\"\n\nconst fastSmalls = true // enable fast path for small integers\n\n// FormatUint returns the string representation of i in the given base,\n// for 2 <= base <= 36. The result uses the lower-case letters 'a' to 'z'\n// for digit values >= 10.\nfunc FormatUint(i uint64, base int) string {\n\tif fastSmalls && i < nSmalls && base == 10 {\n\t\treturn small(int(i))\n\t}\n\t_, s := formatBits(nil, i, base, false, false)\n\treturn s\n}\n\n// FormatInt returns the string representation of i in the given base,\n// for 2 <= base <= 36. The result uses the lower-case letters 'a' to 'z'\n// for digit values >= 10.\nfunc FormatInt(i int64, base int) string {\n\tif fastSmalls && 0 <= i && i < nSmalls && base == 10 {\n\t\treturn small(int(i))\n\t}\n\t_, s := formatBits(nil, uint64(i), base, i < 0, false)\n\treturn s\n}\n\n// Itoa is equivalent to FormatInt(int64(i), 10).\nfunc Itoa(i int) string {\n\treturn FormatInt(int64(i), 10)\n}\n\n// AppendInt appends the string form of the integer i,\n// as generated by FormatInt, to dst and returns the extended buffer.\nfunc AppendInt(dst []byte, i int64, base int) []byte {\n\tif fastSmalls && 0 <= i && i < nSmalls && base == 10 {\n\t\treturn append(dst, small(int(i))...)\n\t}\n\tdst, _ = formatBits(dst, uint64(i), base, i < 0, true)\n\treturn dst\n}\n\n// AppendUint appends the string form of the unsigned integer i,\n// as generated by FormatUint, to dst and returns the extended buffer.\nfunc AppendUint(dst []byte, i uint64, base int) []byte {\n\tif fastSmalls && i < nSmalls && base == 10 {\n\t\treturn append(dst, small(int(i))...)\n\t}\n\tdst, _ = formatBits(dst, i, base, false, true)\n\treturn dst\n}\n\n// small returns the string for an i with 0 <= i < nSmalls.\nfunc small(i int) string {\n\tif i < 10 {\n<file_path>options.go\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":" = append(containerList, c)\n    }\n\n    return con","CodeQwen2.5-7B":" = append(con","copilot":" = append(containerList, c)\n\t}\n\n\treturn con","deepseek-chat-lite":" = append(containerList, c)","CodeQwen1.5-7B":" = append(containerList, c)"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort ","infill":"== 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}","suffix":"\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\npackage container\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"> 0 {\n                c.Ports += strconv.Itoa(int(port.PrivatePort)) + \" \"\n            }","CodeQwen2.5-7B":"!= 0 {\n\t\t\t\tc.Ports += strconv.Itoa(int(port.PrivatePort)) + \" -> \" + strconv.Itoa(int(port.PublicPort)) + \" \"\n\t\t\t}<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"== 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}","deepseek-chat-lite":"!= 0 {","CodeQwen1.5-7B":" == 80 {"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\nfunc GetImages","infill":"FromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}","suffix":"\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"FromDocker() ([]image.ImageSummary, error) {\n    return nil, nil\n}","CodeQwen2.5-7B":"FromDocker() ([]image.ImageSummary, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}","copilot":"FromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}","deepseek-chat-lite":"FromDocker() ([]image.ImageSummary, error) {","CodeQwen1.5-7B":" FromDocker() ([]image.Summary, error) {"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\nfunc GetImagesFromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters","infill":" := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")","suffix":"\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t\t\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":":= filters.NewArgs()","CodeQwen2.5-7B":" := filters.NewArgs()\n\tfilters.Add(\"reference\", \"dev-*\")","copilot":" := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")","deepseek-chat-lite":" := filters.NewArgs()","CodeQwen1.5-7B":" := filters.NewArgs()"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\nfunc GetImagesFromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters","infill":".Add(\"dangling\", \"false\")","suffix":"\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t\t\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":".Add(\"dangling\", \"false\")","CodeQwen2.5-7B":".Add(\"reference\", \"dev*\")","copilot":".Add(\"dangling\", \"false\")","deepseek-chat-lite":".Add(\"reference\", \"(_|-)dev$\")","CodeQwen1.5-7B":".Add(\"reference\", \"dev\")"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\nt\n\nfunc GetImagesFromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{","infill":"All:     true,\n\t\tFilters: filters,","suffix":"\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"\n        Filters: filters,","CodeQwen2.5-7B":"\n\t\tFilters: filters,","copilot":"All:     true,\n\t\tFilters: filters,","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype\n\nfunc GetImagesFromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,","infill":"Filters: filters,","suffix":"\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"\n        Filters: filters,","CodeQwen2.5-7B":"\n\t\tFilters: filters,","copilot":"Filters: filters,","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\t\n\nfunc GetImagesFromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})","infill":"r.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})","suffix":"\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"r.GET(\"/api/images\", func(c *gin.Context) {\n        imageList, err := GetImagesFromDocker()\n        if err != nil {\n            c.JSON(http.StatusInternalServerError, gin.H{\n                \"error\": err.","CodeQwen2.5-7B":"","copilot":"r.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n\t\n\nfunc GetImagesFromDocker() ([]string, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []string{}\n\tfor _, image := range images {\n\t\timageList = append(imageList, image.RepoTags[0])\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {","infill":"c.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})","suffix":"\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\nimport \"github.com/docker/docker/api/types/filters\"\n\n// ResizeOptions holds parameters to resize a TTY.\n// It can be used to resize container TTYs and\n// exec process TTYs too.\ntype ResizeOptions struct {\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"\n            c.JSON(http.StatusInternalServerError, gin.H{\n                \"error\": err.Error(),\n            })\n            return\n        }\n        c.JSON(http.StatusOK, imageList)\n    })","CodeQwen2.5-7B":"\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})","copilot":"c.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}\n\nfunc GetImagesFromDocker() ([], error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []Image{}\n\tfor _, image := range images {\n\t\timg ","infill":":= Image{\n\t\t\tID:          image.ID,\n\t\t\tRepository:  image.RepoTags[0],\n\t\t\tTag:         image.RepoTags[1],\n\t\t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),\n\t\t\tSize:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t\t\tVirtualSize: strconv.Itoa(int(image.VirtualSize / 1024 / 1024)),\n\t\t}","suffix":"\n\n\t\timageList = append(imageList, img)\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":":= Image{\n            ID:         image.ID,\n            Repository: image.RepoTags[0],\n            Tag:        image.Tags[0],\n            Created:    time.Unix(image.Created, 0).Format(time.DateTime),\n            Size","CodeQwen2.5-7B":":= Image{\n\t\t\tID:          image.ID,\n\t\t\tRepository:  image.RepoTags[0],\n\t\t\tTag:         image.RepoTags[1],\n\t\t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),\n\t\t\tSize:        strconv.FormatInt(image.Size, 10),\n\t\t\tVirtualSize: strconv.FormatInt(image.VirtualSize, 10),\n\t\t}","copilot":":= Image{\n\t\t\tID:          image.ID,\n\t\t\tRepository:  image.RepoTags[0],\n\t\t\tTag:         image.RepoTags[1],\n\t\t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),\n\t\t\tSize:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t\t\tVirtualSize: strconv.Itoa(int(image.VirtualSize / 1024 / 1024)),\n\t\t}","deepseek-chat-lite":":= Image{","CodeQwen1.5-7B":" := Image{"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}\n\nfunc GetImagesFromDocker() ([]Image, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\timageList := []Image{}\n\tfor _, image := range images {\n\t\timg := Image{\n\t\t\tID:          image.ID,\n\t\t\tRepository:  image.RepoTags[0],\n\t\t\tTag:         image.RepoTags[1],\n\t\t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),","infill":"Size:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t\t\tVirtualSize: strconv.Itoa(int(image.VirtualSize / 1024 / 1024)),\n\t\t}","suffix":"\n\n\t\timageList = append(imageList, img)\n\t}\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n\t\t\tSize:        strconv.FormatInt(image.Size, 10),\n\t\t\tVirtualSize: strconv.FormatInt(image.VirtualSize, 10),\n\t\t}","copilot":"Size:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t\t\tVirtualSize: strconv.Itoa(int(image.VirtualSize / 1024 / 1024)),\n\t\t}","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}\n\nfunc GetImagesFromDocker() ([]Image, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"dangling\", \"false\")\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog","infill":".Println(images)","suffix":"\n\n\timageList := []Image{}\n\t// for _, image := range images {\n\t// \timg := Image{\n\t// \t\tID:          image.ID,\n\t// \t\tRepository:  image.RepoTags[0],\n\t// \t\tTag:         image.RepoTags[1],\n\t// \t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),\n\t// \t\tSize:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t// \t\tVirtualSize: strconv.Itoa(int(image.VirtualSize / 1024 / 1024)),\n\t// \t}\n\n\t// \timageList = append(imageList, img)\n\t// }\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":".Println(images)","CodeQwen2.5-7B":".Println(images)<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":".Println(images)","deepseek-chat-lite":".Println(images)","CodeQwen1.5-7B":" .Println(images)"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}\n\nfunc GetImagesFromDocker() ([]Image, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"la","infill":"bel\", \"gms-env-dev\")","suffix":"\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(images)\n\n\timageList := []Image{}\n\t// for _, image := range images {\n\t// \timg := Image{\n\t// \t\tID:          image.ID,\n\t// \t\tRepository:  image.RepoTags[0],\n\t// \t\tTag:         image.RepoTags[1],\n\t// \t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),\n\t// \t\tSize:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t// \t\tVirtualSize: strconv.Itoa(int(image.VirtualSize / 1024 / 1024)),\n\t// \t}\n\n\t// \timageList = append(imageList, img)\n\t// }\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"belongs-to=kalmproject/devimage\")","CodeQwen2.5-7B":"st\", \"10\")","copilot":"bel\", \"gms-env-dev\")","deepseek-chat-lite":"bel\", \"com.docker.compose.project=dev\")","CodeQwen1.5-7B":" bel\", \"dev\")"}},{"prefix":"package main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"log\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/client\"\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype Container struct {\n\tID          string `json:\"id\"`\n\tName        string `json:\"name\"`\n\tImage       string `json:\"image\"`\n\tStatus      string `json:\"status\"`\n\tPorts       string `json:\"ports\"`\n\tLastStarted string `json:\"lastStarted\"`\n}\n\nfunc GetContainersFromDocker() ([]Container, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilter := filters.NewArgs()\n\tfilter.Add(\"name\", \"(_|-)dev$\")\n\n\tcontainers, err := client.ContainerList(context.Background(), container.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filter})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(containers)\n\n\tcontainerList := []Container{}\n\tfor _, container := range containers {\n\t\tc := Container{\n\t\t\tID:     container.ID,\n\t\t\tName:   strings.TrimLeft(container.Names[0], \"/\"),\n\t\t\tImage:  container.Image,\n\t\t\tStatus: strings.ToUpper(container.State),\n\t\t}\n\n\t\tc.Ports = \"\"\n\t\tfor _, port := range container.Ports {\n\t\t\tif port.PrivatePort == 22 {\n\t\t\t\tc.Ports = strconv.Itoa(int(port.PublicPort))\n\t\t\t}\n\t\t}\n\n\t\t// the value of timestamp is in nanoseconds, convert it to time.Time\n\t\tc.LastStarted = time.Unix(0, container.Created*int64(time.Second)).Format(time.DateTime)\n\n\t\tcontainerList = append(containerList, c)\n\t}\n\n\treturn containerList, nil\n}\n\ntype Image struct {\n\tID          string `json:\"id\"`\n\tRepository  string `json:\"repository\"`\n\tTag         string `json:\"tag\"`\n\tCreated     string `json:\"created\"`\n\tSize        string `json:\"size\"`\n\tVirtualSize string `json:\"virtualSize\"`\n}\n\nfunc GetImagesFromDocker() ([]Image, error) {\n\tclient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfilters := filters.NewArgs()\n\tfilters.Add(\"label\", \"gms-env-dev","infill":"\")","suffix":"\n\n\timages, err := client.ImageList(context.Background(), image.ListOptions{\n\t\tAll:     true,\n\t\tFilters: filters,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Println(images)\n\n\timageList := []Image{}\n\t// for _, image := range images {\n\t// \timg := Image{\n\t// \t\tID:          image.ID,\n\t// \t\tRepository:  image.RepoTags[0],\n\t// \t\tTag:         image.RepoTags[1],\n\t// \t\tCreated:     time.Unix(image.Created, 0).Format(time.DateTime),\n\t// \t\tSize:        strconv.Itoa(int(image.Size / 1024 / 1024)),\n\t// \t\tVirtualSize: strconv.Itoa(int(image.VirtualSize / 1024 / 1024)),\n\t// \t}\n\n\t// \timageList = append(imageList, img)\n\t// }\n\n\treturn imageList, nil\n}\n\nfunc main() {\n\tport := flag.String(\"port\", \"8080\", \"port to listen on\")\n\tflag.Parse()\n\n\tr := gin.Default()\n\n\t// Serve the index.html file from the 'dist' directory\n\tr.StaticFile(\"/\", \"../dist/index.html\")\n\tr.StaticFile(\"/index.html\", \"../dist/index.html\")\n\tr.StaticFile(\"/tailwindcss.js\", \"../dist/tailwindcss.js\")\n\tr.StaticFile(\"/20x20.png\", \"../dist/20x20.png\")\n\n\t// Serve all static files under the 'dist' directory\n\tr.Static(\"/js\", \"../dist/js\")\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tc.JSON(http.StatusOK, gin.H{\n\t\t\t\"message\": \"pong\",\n\t\t})\n\t})\n\n\tr.GET(\"/api/containers\", func(c *gin.Context) {\n\t\tcontainerList, err := GetContainersFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, containerList)\n\t})\n\n\tr.GET(\"/api/images\", func(c *gin.Context) {\n\t\timageList, err := GetImagesFromDocker()\n\t\tif err != nil {\n\t\t\tc.JSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\"error\": err.Error(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\tc.JSON(http.StatusOK, imageList)\n\t})\n\n\tr.Run(\":\" + *port)\n}\n","relevantFile":"<file_path>options.go\n\tHeight uint\n\tWidth  uint\n}\n\n// AttachOptions holds parameters to attach to a container.\ntype AttachOptions struct {\n\tStream     bool\n\tStdin      bool\n\tStdout     bool\n\tStderr     bool\n\tDetachKeys string\n\tLogs       bool\n}\n\n// CommitOptions holds parameters to commit changes into a container.\ntype CommitOptions struct {\n\tReference string\n\tComment   string\n\tAuthor    string\n\tChanges   []string\n\tPause     bool\n\tConfig    *Config\n}\n\n// RemoveOptions holds parameters to remove containers.\ntype RemoveOptions struct {\n\tRemoveVolumes bool\n\tRemoveLinks   bool\n\tForce         bool\n}\n\n// StartOptions holds parameters to start containers.\ntype StartOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\n// ListOptions holds parameters to list containers with.\ntype ListOptions struct {\n\tSize    bool\n\tAll     bool\n\tLatest  bool\n\tSince   string\n\tBefore  string\n\tLimit   int\n\tFilters filters.Args\n}\n\n// LogsOptions holds parameters to filter logs with.\ntype LogsOptions struct {\n\tShowStdout bool\n\tShowStderr bool\n\tSince      string\n\tUntil      string\n\tTimestamps bool\n\tFollow     bool\n\tTail       string\n\tDetails    bool\n}\n<file_path>image_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/filters\"\n\t\"github.com/docker/docker/api/types/image\"\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// ImageList returns a list of images in the docker host.\nfunc (cli *Client) ImageList(ctx context.Context, options image.ListOptions) ([]image.Summary, error) {\n\tvar images []image.Summary\n\n\t// Make sure we negotiated (if the client is configured to do so),\n\t// as code below contains API-version specific handling of options.\n\t//\n\t// Normally, version-negotiation (if enabled) would not happen until\n\t// the API request is made.\n\tif err := cli.checkVersion(ctx); err != nil {\n\t\treturn images, err\n\t}\n\n\tquery := url.Values{}\n\n\toptionFilters := options.Filters\n\treferenceFilters := optionFilters.Get(\"reference\")\n\tif versions.LessThan(cli.version, \"1.25\") && len(referenceFilters) > 0 {\n\t\tquery.Set(\"filter\", referenceFilters[0])\n\t\tfor _, filterValue := range referenceFilters {\n\t\t\toptionFilters.Del(\"reference\", filterValue)\n\t\t}\n\t}\n\tif optionFilters.Len() > 0 {\n\t\t//nolint:staticcheck // ignore SA1019 for old code\n\t\tfilterJSON, err := filters.ToParamWithVersion(cli.version, optionFilters)\n\t\tif err != nil {\n\t\t\treturn images, err\n\t\t}\n\t\tquery.Set(\"filters\", filterJSON)\n\t}\n\tif options.All {\n\t\tquery.Set(\"all\", \"1\")\n\t}\n\tif options.SharedSize && versions.GreaterThanOrEqualTo(cli.version, \"1.42\") {\n\t\tquery.Set(\"shared-size\", \"1\")\n\t}\n\n\tserverResp, err := cli.get(ctx, \"/images/json\", query, nil)\n\tdefer ensureReaderClosed(serverResp)\n\tif err != nil {\n\t\treturn images, err\n\t}\n\n\terr = json.NewDecoder(serverResp.body).Decode(&images)\n\treturn images, err\n}","relevantFileList":[],"filePath":"backend/main.go","template":"go","multiRes":{"hipilot":"=true\")","CodeQwen2.5-7B":"\")","copilot":"\")","deepseek-chat-lite":"\")","CodeQwen1.5-7B":" =true\")"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQF10(","infill":"f10TaskId string, subjects string) (ThsMarketCode, error) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAs","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"thsCode string) (my_orm.UnderlyingAssets, error) {\n    var underAssets my_orm.UnderlyingAssets\n    underAssetsRsp, err := d.RequestUnderlyingAssets(thsCode)\n    if err != nil {\n        return","CodeQwen2.5-7B":"f10TaskId string, ids string, subjects string) (string, string) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn \"\", \"\"\n\t}\n\t// 获取推送指标的市场\n\tthsCode := \"\"\n\tmarket := \"\"\n\tfor _, v := range indicatorInfoFromF10.Data {\n\t\tif v[\"market\"] != \"\" {\n\t\t\tmarket = v[\"market\"]\n\t\t\tthsCode = v[\"code\"]\n\t\t\tbreak\n\t\t}\n\t}\n\treturn thsCode, market\n}","copilot":"f10TaskId string, subjects string) (ThsMarketCode, error) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n","deepseek-chat-lite":"string) (string, string, error) {","CodeQwen1.5-7B":" f10TaskId string, ids string, subjects string) (string, error) {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string) (ThsMarketCode, error) {\n\tids, ok := d.MapTaskIdIndi","infill":"cators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAs","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"cators[f10TaskId]\n    if !ok {\n        levellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n        return ThsMarketCode{}, errors.New","CodeQwen2.5-7B":"cators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn ThsMarketCode{}, errors.New(\"taskid not found in local database\")\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn ThsMarketCode{}, errors.New(\"date is null\")\n\t}\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif","copilot":"cators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n","deepseek-chat-lite":"cators[f10TaskId]","CodeQwen1.5-7B":" cators[f10TaskId]"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string) (ThsMarketCode, error) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in ","infill":"local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\n\t// 先根据 subjects 查询一下推送指标的市场\n\t\n","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAs","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"local database\", zap.String(\"taskid\", f10TaskId))\n        return ThsMarketCode{}, errors.New(\"taskid kafka message not configured in local database\")\n    }\n    // 先根据 subjects 查询一下推送指标的市场\n   ","CodeQwen2.5-7B":"local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn ThsMarketCode{}, errors.New(\"taskid kafka message not configured in local database\")\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn ThsMarketCode{}, errors.New(\"taskid date is null\")\n\t}\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode,","copilot":"local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\n\t// 先根据 subjects 查询一下推送指标的市场\n\t\n","deepseek-chat-lite":"local database\", zap.String(\"taskid\", f10TaskId))","CodeQwen1.5-7B":" local database\", zap.String(\"taskid\", f10TaskId))"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\n\treturn ThsMarketCode","infill":"{market, code}, nil","suffix":"\n\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderly","relevantFile":"<file_path>app/orm/f10.go\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {\n\t\tSubject SubjectData         `json:\"subject\"`","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"{}, nil","CodeQwen2.5-7B":"{}, nil\n}","copilot":"{market, code}, nil","deepseek-chat-lite":"{}, nil","CodeQwen1.5-7B":" {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\n\treturn ThsMarketCode{market, code}","infill":", nil","suffix":"\n\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderly","relevantFile":"<file_path>app/orm/f10.go\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {\n\t\tSubject SubjectData         `json:\"subject\"`","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":", nil","CodeQwen2.5-7B":", nil\n}","copilot":", nil","deepseek-chat-lite":", nil","CodeQwen1.5-7B":" // TODO: 获取市场"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from ","infill":"response\n","suffix":"\n\treturn ThsMarketCode{market, code}, nil\n\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\t","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"f10 rsp","CodeQwen2.5-7B":"f10返回的数据\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tcode := jsonF10Rsp.Data.Data[0].Subject.Code\n\tif market == \"\" || code == \"\" {\n\t\treturn ThsMarketCode{}, errors.New(\"get market and code from f10 rsp failed\")\n\t}\n\treturn ThsMarketCode{market, code}, nil\n}","copilot":"response\n","deepseek-chat-lite":"response","CodeQwen1.5-7B":" f10"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (\n\tStageAll       = \"all\"       //完整阶段\n\tStageExtract   = \"extract\"   //抽取\n\tStageTransform = \"transform\" //转换\n\tStageLoad      = \"load\"      //持久化\n)\n\n// 数据源source\nconst (\n\tSourcePg              = \"pg\"              // pg数据库\n\tSourceF10             = \"f10\"             // F10\n\tSourceFUsEtfAdjFactor = \"FUsEtfAdjFactor\" //美股ETF复权因子\n)\n\nvar (\n\t// 接收数据条数\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_rx_traffic\",\n\t\t\tHelp: \"count received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// qps\n\t// trigger: cron kafka\n\t// source: pg, f10\n\tqpsCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_qps\",\n\t\t\tHelp: \"request received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// 处理耗时分布\n\t// trigger: cron manual kafka\n\t// stage: extract proccess load\n\t// export: all bbrq rtime real code\n\t// source: pg, f10\n\tcostReqBucketVec = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"gms_etl_cost_histogram\",\n\t\t\tHelp:    \"cost of time for each request.\",\n\t\t\tBuckets: []float64{100, 1000, 5000, 10000, 50000, 120000, 600000},\n\t\t},\n\t\t[]string{\"db\", \"table\", \"stage\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// label_name 面板显示时公有显示\n\tnameCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_label_name\",\n\t\t\tHelp: \"show label name\",\n\t\t},\n\t\t[]string{\"db\", \"table\"},\n\t)\n\n\t// 错误数 记录接口错误,内部错误等\n\terrorCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_error\",\n\t\t\tHelp: \"errors\",\n\t\t},\n\t\t[]string{\"status\"},\n\t)\n\n\t// some metrics of kafka consumer, such as package, data flow, etc.\n\tkafkaDataRecvPackage = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_package\",\n\t\t\tHelp: \"data recv package\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaRecvSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_recv_task_id\",\n\t\t\tHelp: \"skip recv task id\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\", \"error\"},\n\t)\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n\n\tprometheus.MustRegister(kafkaDataRecvPackage)\n\tprometheus.MustRegister(kafkaDataRecvFlow)\n\tprometheus.MustRegister(kafkaRecvSkipTaskId)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n// KafkaDataRecvPackage kafka数据包接收统计\nfunc KafkaDataRecvPackage(topic string, partition int32, taskId string, packageNum int) {\n\tkafkaDataRecvPackage.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(packageNum))\n}\n\n// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition int32, taskId string, flow int) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(flow))\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string, errType ","infill":"string","suffix":") {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId, errType).Inc()\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"string","CodeQwen2.5-7B":"string","copilot":"string","deepseek-chat-lite":"string","CodeQwen1.5-7B":" string"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tif validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetInvalidZeroFromRecv","infill":"Msg(message.Topic, message.Partition, jsonF10.TaskId)","suffix":"\n\t\treturn\n\n\t}\n\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, recvDate)\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRe","relevantFile":"<file_path>app/metrics/metrics.go\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"TaskId(message.Topic, message.Partition, jsonF10.TaskId, \"ValidZero\")\n\t\treturn\n\t}","copilot":"Msg(message.Topic, message.Partition, jsonF10.TaskId)","deepseek-chat-lite":"Msg(thsMarketCode.Code, recvDate, taskInfo)","CodeQwen1.5-7B":" (message.Topic, message.Partition, jsonF10.TaskId, thsMarketCode.Code, recvDate)"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tif validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetInvalidZeroFromRecvMsg(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\n\t}\n\n\tjsonF10Rsp, err := d.RequestF10Data(","infill":"indicatorIDs, subjects, currency, recvDate)","suffix":"\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn\n\t}\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, r","relevantFile":"<file_path>app/metrics/metrics.go\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"jsonF10.TaskId, subjects, currency, recvDate)<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"indicatorIDs, subjects, currency, recvDate)","deepseek-chat-lite":"jsonF10.TaskId, indicatorIDs, subjects, recvDate)","CodeQwen1.5-7B":" jsonF10.TaskId, subjects, indicatorIDs, \"\")"}},{"prefix":"package server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.Post(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tvar msg my_orm.F10Update","infill":"err := c.BindJSON(&msg)\n\tif err != nil {\n\t\tc.String(400, \"bind json failed\")\n\t\treturn\n\t}\n\n\tlevellog.Log.Info(\"push msg\", z","suffix":"\n\n\tc.String(200, \"ok\")\n}\n","relevantFile":"<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract_manager/extract_manager.go\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\nfunc GetManualExtract(dbName string, tableName string, http extract.ReqParam) extract.Extract {\n\tdb, ok := finance_info.GlobalFinanceInfoManager[dbName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tbasic, ok := db.TableBasic[tableName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif basic.Datasource == \"PG\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestPG{\n\t\t\t\tDatasource:   my_orm.RequestTcpPg,","relevantFileList":[],"filePath":"app/server/http.go","template":"go","multiRes":{"hipilot":"Msg","CodeQwen2.5-7B":"Msg\n\terr := c.BindJSON(&msg)\n\tif err != nil {\n\t\tc.String(400, err.Error())\n\t\treturn\n\t}\n\terr = my_orm.PushMsg(msg)\n\tif err != nil {\n\t\tc.String(400, err.Error())\n\t\treturn\n\t}","copilot":"err := c.BindJSON(&msg)\n\tif err != nil {\n\t\tc.String(400, \"bind json failed\")\n\t\treturn\n\t}\n\n\tlevellog.Log.Info(\"push msg\", z","deepseek-chat-lite":"Msg","CodeQwen1.5-7B":" Msg"}},{"prefix":"package server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"go.uber.org/zap\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.Post(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tvar msg my_orm.F10Update\n\terr := c.BindJSON(&msg)\n\tif err != nil {\n\t\tc.String(400, \"bind json failed\")\n\t\treturn\n\t}\n\n\tlevellog","infill":".Log.Info(\"push msg\", zap.Any(\"msg\", msg))\n\n\t","suffix":"\n\n\tc.String(200, \"ok\")\n}\n","relevantFile":"<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract_manager/extract_manager.go\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\nfunc GetManualExtract(dbName string, tableName string, http extract.ReqParam) extract.Extract {\n\tdb, ok := finance_info.GlobalFinanceInfoManager[dbName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tbasic, ok := db.TableBasic[tableName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif basic.Datasource == \"PG\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestPG{\n\t\t\t\tDatasource:   my_orm.RequestTcpPg,","relevantFileList":[],"filePath":"app/server/http.go","template":"go","multiRes":{"hipilot":".Log.Debug(\"receive update\", zap.Any(\"update\", msg))","CodeQwen2.5-7B":".Log.Info(\"push msg\", zap.Any(\"msg\", msg))","copilot":".Log.Info(\"push msg\", zap.Any(\"msg\", msg))\n\n\t","deepseek-chat-lite":".Log.Info(\"push msg\", zap.Any(\"msg\", msg))","CodeQwen1.5-7B":".Log.Info(\"push msg\", zap.Any(\"msg\", msg))"}},{"prefix":"package server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"go.uber.org/zap\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.Post(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tvar msg my_orm.F10Update\n\terr := c.BindJSON(&msg)\n\tif err != nil {\n\t\tc.String(400, \"bind json failed\")\n\t\treturn\n\t}\n\n\tlevellog.Log.Info(\"push msg\", z","infill":"ap.Any(\"msg\", msg))\n\n\t","suffix":"\n\n\tc.String(200, \"ok\")\n}\n","relevantFile":"<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract_manager/extract_manager.go\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\nfunc GetManualExtract(dbName string, tableName string, http extract.ReqParam) extract.Extract {\n\tdb, ok := finance_info.GlobalFinanceInfoManager[dbName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tbasic, ok := db.TableBasic[tableName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif basic.Datasource == \"PG\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestPG{\n\t\t\t\tDatasource:   my_orm.RequestTcpPg,","relevantFileList":[],"filePath":"app/server/http.go","template":"go","multiRes":{"hipilot":"zap.Any(\"msg\", msg))","CodeQwen2.5-7B":"ap.Any(\"msg\", msg))","copilot":"ap.Any(\"msg\", msg))\n\n\t","deepseek-chat-lite":"ap.Any(\"msg\", msg))","CodeQwen1.5-7B":" ap.Any(\"msg\", msg))"}},{"prefix":"package extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n\n\tDebugPushMessage(my_orm.F10Update)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n\nfun ","infill":"(e AutoExtract) DebugPushMessage(f10Update my_orm.F10Update) (int, error) {\n\te.Update.DebugPushMessage(f10Update)","suffix":"\n","relevantFile":"<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"go.uber.org/zap\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.POST(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract_manager/extract_manager.go\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"go.uber.org/zap\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n<file_path>app/extract/extract_api.go\npackage extract\n\nimport my_orm \"finance_etl/app/orm\"\n\n// Extract 数据抽取接口定义\ntype Extract interface {\n\tGetMarket() string       // 获取抽取器所属市场名\n\tGetTable() string        // 获取抽取器抽取的表名\n\tGetTrigMod() int         // 获取抽取触发类型（主动 or 被动）\n\tDoExtract() (int, error) // 触发数据更新,包括手动触发与自动更新\n\tStop() (int, error)\n\n\tDebugPushMessage(my_orm.F10Update) // 调试接口\n}","relevantFileList":[],"filePath":"app/extract/extract_auto.go","template":"go","multiRes":{"hipilot":"(e AutoExtract) DebugPushMessage(f10u my_orm.F10Update) {\n    e.Update.DebugPushMessage(f10u)\n}","CodeQwen2.5-7B":"(e AutoExtract) DebugPushMessage(msg my_orm.F10Update) {\n\te.Update.DebugPushMessage(msg)\n}","copilot":"(e AutoExtract) DebugPushMessage(f10Update my_orm.F10Update) (int, error) {\n\te.Update.DebugPushMessage(f10Update)","deepseek-chat-lite":"c (e AutoExtract) DebugPushMessage(msg my_orm.F10Update) {","CodeQwen1.5-7B":"c(e AutoExtract) {"}},{"prefix":"package extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n\n\tDebugPushMessage(my_orm.F10Update)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n\nfun (e AutoExtract) DebugPushMessage(f10Update my_orm.F10Update) (int, error) {","infill":"e.Update.DebugPushMessage(f10Update)","suffix":"\n","relevantFile":"<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"go.uber.org/zap\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.POST(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract_manager/extract_manager.go\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"go.uber.org/zap\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n<file_path>app/extract/extract_api.go\npackage extract\n\nimport my_orm \"finance_etl/app/orm\"\n\n// Extract 数据抽取接口定义\ntype Extract interface {\n\tGetMarket() string       // 获取抽取器所属市场名\n\tGetTable() string        // 获取抽取器抽取的表名\n\tGetTrigMod() int         // 获取抽取触发类型（主动 or 被动）\n\tDoExtract() (int, error) // 触发数据更新,包括手动触发与自动更新\n\tStop() (int, error)\n\n\tDebugPushMessage(my_orm.F10Update) // 调试接口\n}","relevantFileList":[],"filePath":"app/extract/extract_auto.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n\treturn e.Update.DebugPushMessage(f10Update)\n}","copilot":"e.Update.DebugPushMessage(f10Update)","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)","infill":"DebugPushMessage(my_orm.F10Update)","suffix":"\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n\nfunc (e AutoExtract) DebugPushMessage(f10Update my_orm.F10Update) {\n\te.Update.DebugPushMessage(f10Update)\n}\n","relevantFile":"<file_path>app/server/http.go\npackage server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"go.uber.org/zap\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.POST(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract_manager/extract_manager.go\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"go.uber.org/zap\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n<file_path>app/extract/extract_api.go\npackage extract\n\nimport my_orm \"finance_etl/app/orm\"\n\n// Extract 数据抽取接口定义\ntype Extract interface {\n\tGetMarket() string       // 获取抽取器所属市场名\n\tGetTable() string        // 获取抽取器抽取的表名\n\tGetTrigMod() int         // 获取抽取触发类型（主动 or 被动）\n\tDoExtract() (int, error) // 触发数据更新,包括手动触发与自动更新\n\tStop() (int, error)\n\n\tDebugPushMessage(my_orm.F10Update) // 调试接口\n}","relevantFileList":[],"filePath":"app/extract/extract_auto.go","template":"go","multiRes":{"hipilot":"error","CodeQwen2.5-7B":"\n\tDebugPushMessage(f10Update my_orm.F10Update)","copilot":"DebugPushMessage(my_orm.F10Update)","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"go.uber.org/zap\"\n)\n\n// UpdateDataCron 定时更新指定表数据\ntype UpdateDataCronPg struct {\n\tFinanceDb    string // 财务数据库\n\tFinanceTable string // 财务数据表\n}\n\nconst retryCnt int = 3 // 最多重试三次\n\nfunc (d UpdateDataCronPg) AddTasks() error {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManag","infill":"func (d UpdateDataCronPg) DebugPushMessage(f10Update my_orm.F10Update) {\n}\n","suffix":"","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n\n\tDebugPushMessage(my_orm.F10Update)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n\nfunc (e AutoExtract) DebugPushMessage(f10Update my_orm.F10Update) {\n\te.Update.DebugPushMessage(f10Update)\n}\n<file_path>app/server/http.go\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tvar msg my_orm.F10Update\n\terr := c.BindJSON(&msg)\n\tif err != nil {\n\t\tc.String(400, \"bind json failed\")\n\t\treturn\n\t}\n\n\tlevellog.Log.Info(\"push msg\", zap.Any(\"msg\", msg))\n\n\textract_manager.GlobalExtractManager.F10Kafka.DebugPushMessage(msg)\n\tc.String(200, \"ok\")\n<file_path>app/extract_manager/extract_manager.go\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\nfunc GetManualExtract(dbName string, tableName string, http extract.ReqParam) extract.Extract {\n\tdb, ok := finance_info.GlobalFinanceInfoManager[dbName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tbasic, ok := db.TableBasic[tableName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif basic.Datasource == \"PG\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestPG{\n\t\t\t\tDatasource:   my_orm.RequestTcpPg,\n\t\t\t\tFinanceTable: tableName,\n\t\t\t\tFinanceDb:    dbName,\n\t\t\t\tTriggerType:  my_orm.UpdateByManual,\n<file_path>app/extract/automatic/update_cron_fUsEtfAdjFactor.go\nfunc (d UpdateDataCronFUsEtfAdjFactor) AddTasks() error {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support database %s\", d.FinanceDb)\n\t}\n\ttaskTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support table %s\", d.FinanceTable)\n\t}\n\n\treq := manual.DataRequestFUsEtfAdjFactor{\n\t\tDatasource:   my_orm.RequestHTTPFUsEtfAdjFactor,\n\t\tUrl:          config.GetFUsEtfAdjFactor().Url,\n\t\tFinanceTable: d.FinanceTable,\n\t\tFinanceDb:    d.FinanceDb,\n\t\tTriggerType:  my_orm.UpdateByCron,\n\t}\n\n\ttimes := strings.Split(taskTable.Cron, \";\")\n\tfor _, t := range times {\n\t\tif t == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\ttaskName := d.FinanceDb + d.FinanceTable\n\t\t// 定时任务的回调函数\n\t\tcallback := func() {\n\t\t\tretryCnt := 3\n\t\t\tfor i := 0; i < retryCnt; i++ {\n\t\t\t\trows, err := req.RequestData()\n\t\t\t\tif err == nil {\n\t\t\t\t\tlevellog.Log.Info(fmt.Sprintf(\"export data by cron successfully\"), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"affects row\", rows), zap.Int(\"retry\", i))\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif i == retryCnt-1 {\n\t\t\t\t\tlevellog.Log.Error(fmt.Sprintf(\"export data by cron failed: %s\", err.Error()), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t} else {\n\t\t\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"export data failed: %s, start to retry\", err.Error()), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t\ttime.Sleep(time.Duration(5) * time.Second)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr := cron.GCronManager.AddTask(taskName, t, callback)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"add task failed\"), zap.String(\"table\", d.FinanceTable), zap.String(\"db\", d.FinanceDb), zap.String(\"cron\", t))\n\t\t\tcontinue\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (d UpdateDataCronFUsEtfAdjFactor) Start() (int, error) {\n\treturn 0, d.AddTasks()\n}\n\nfunc (d UpdateDataCronFUsEtfAdjFactor) Stop() (int, error) {\n\ttaskName := d.FinanceDb + d.FinanceTable\n\tcron.GCronManager.RemoveTask(taskName)","relevantFileList":[],"filePath":"app/extract/automatic/update_cron_pg.go","template":"go","multiRes":{"hipilot":"er.GetDbInfo(d.FinanceDb)\n    if !ok {\n        err := fmt.Errorf(\"finance database [%s] not found\", d.FinanceDb)\n        levellog.GlobalLevelLog.Error(\"UpdateDataCronPg.AddTasks","CodeQwen2.5-7B":"er.GetFinanceDbInfo(d.FinanceDb)\n\tif !ok {\n\t\treturn fmt.Errorf(\"finance db %s not found\", d.FinanceDb)\n\t}\n\t// 获取表信息\n\ttableInfo, ok := dbInfo.GetTableInfo(d.FinanceTable)\n\tif !ok {\n\t\treturn fmt.Errorf(\"finance table %s not found\", d.FinanceTable)\n\t}\n\t// 获取表字段信息\n\ttableFieldInfo, ok := tableInfo.GetTableFieldInfo()\n\tif !ok {\n\t\treturn fmt.Errorf(\"finance table %s field not found\", d.FinanceTable)\n\t}\n\t// 获取表字段信息\n\ttableFieldInfo, ok = tableInfo.GetTableFieldInfo()\n\tif !ok {\n\t\treturn fmt.Errorf(\"finance table %s field not found\", d.FinanceTable)\n\t}\n\t// 获取表字段信息\n\ttableFieldInfo, ok = tableInfo.GetTableFieldInfo()\n\tif !ok {\n\t\treturn fmt.Errorf(\"finance table %s field not found\", d.FinanceTable)\n\t}\n\t// 获取表字段信息\n\ttableFieldInfo, ok = tableInfo.GetTableFieldInfo()\n\tif !ok {\n\t\treturn fmt.Errorf(\"finance table %s field not found\", d.FinanceTable)\n\t}\n\t// 获取表字段信息\n\ttableFieldInfo, ok = tableInfo.GetTableFieldInfo()\n\tif !ok {\n\t\treturn fmt.Errorf(\"finance table %s field not found\", d.FinanceTable)\n\t}\n\t// 获取","copilot":"func (d UpdateDataCronPg) DebugPushMessage(f10Update my_orm.F10Update) {\n}\n","deepseek-chat-lite":"er.GetDbInfo(d.FinanceDb)","CodeQwen1.5-7B":"er.GetDbInfo(d.FinanceDb)"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\tmy_orm \"finance_etl/app/orm\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMess","infill":"c (d KafkaConsumer) DebugPushMessage(f10Update my_orm.F10Update) {\n}","suffix":"\n\nfunc (d KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\td.chErr <- err\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n\tclose(d.chErr)\n}\n","relevantFile":"<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n\n\tDebugPushMessage(my_orm.F10Update)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n\nfunc (e AutoExtract) DebugPushMessage(f10Update my_orm.F10Update) {\n\te.Update.DebugPushMessage(f10Update)\n}\n<file_path>app/extract/automatic/update_cron_fUsEtfAdjFactor.go\n\ntype UpdateDataCronFUsEtfAdjFactor struct {\n\tFinanceDb    string // 财务数据库\n\tFinanceTable string // 财务数据表\n}\n\nfunc (d UpdateDataCronFUsEtfAdjFactor) AddTasks() error {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support database %s\", d.FinanceDb)\n\t}\n\ttaskTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support table %s\", d.FinanceTable)\n\t}\n\n\treq := manual.DataRequestFUsEtfAdjFactor{\n\t\tDatasource:   my_orm.RequestHTTPFUsEtfAdjFactor,\n\t\tUrl:          config.GetFUsEtfAdjFactor().Url,\n\t\tFinanceTable: d.FinanceTable,\n\t\tFinanceDb:    d.FinanceDb,\n\t\tTriggerType:  my_orm.UpdateByCron,\n\t}\n\n\ttimes := strings.Split(taskTable.Cron, \";\")\n\tfor _, t := range times {\n\t\tif t == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\ttaskName := d.FinanceDb + d.FinanceTable\n\t\t// 定时任务的回调函数\n\t\tcallback := func() {\n\t\t\tretryCnt := 3\n\t\t\tfor i := 0; i < retryCnt; i++ {\n\t\t\t\trows, err := req.RequestData()\n\t\t\t\tif err == nil {\n\t\t\t\t\tlevellog.Log.Info(fmt.Sprintf(\"export data by cron successfully\"), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"affects row\", rows), zap.Int(\"retry\", i))\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif i == retryCnt-1 {\n\t\t\t\t\tlevellog.Log.Error(fmt.Sprintf(\"export data by cron failed: %s\", err.Error()), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t} else {\n\t\t\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"export data failed: %s, start to retry\", err.Error()), zap.String(\"table\", req.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", req.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t\ttime.Sleep(time.Duration(5) * time.Second)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr := cron.GCronManager.AddTask(taskName, t, callback)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"add task failed\"), zap.String(\"table\", d.FinanceTable), zap.String(\"db\", d.FinanceDb), zap.String(\"cron\", t))\n\t\t\tcontinue\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (d UpdateDataCronFUsEtfAdjFactor) Start() (int, error) {\n<file_path>app/extract/automatic/update_cron_pg.go\ntype UpdateDataCronPg struct {\n\tFinanceDb    string // 财务数据库\n\tFinanceTable string // 财务数据表\n}\n\nconst retryCnt int = 3 // 最多重试三次\n\nfunc (d UpdateDataCronPg) AddTasks() error {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support database %s\", d.FinanceDb)\n\t}\n\ttaskTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support table %s\", d.FinanceTable)\n\t}\n\treqPG := manual.DataRequestPG{\n\t\tDatasource:   my_orm.RequestTcpPg,\n\t\tFinanceTable: d.FinanceTable,\n\t\tFinanceDb:    d.FinanceDb,\n\t\tTriggerType:  my_orm.UpdateByCron,\n\t}\n\ttimes := strings.Split(taskTable.Cron, \";\")\n\tfor _, t := range times {\n\t\tif t == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\ttaskName := d.FinanceDb + d.FinanceTable\n\t\t// 定时任务的回调函数\n\t\tcallback := func() {\n\t\t\tfor i := 0; i < retryCnt; i++ {\n\t\t\t\trows, err := reqPG.RequestData()\n\t\t\t\tif err == nil {\n\t\t\t\t\tlevellog.Log.Info(fmt.Sprintf(\"export data by cron successfully\"), zap.String(\"table\", reqPG.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", reqPG.FinanceDb), zap.Int(\"affects row\", rows), zap.Int(\"retry\", i))\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif i == retryCnt-1 {\n\t\t\t\t\tlevellog.Log.Error(fmt.Sprintf(\"export data by cron failed: %s\", err.Error()), zap.String(\"table\", reqPG.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", reqPG.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t} else {\n\t\t\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"export data failed: %s, start to retry\", err.Error()), zap.String(\"table\", reqPG.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", reqPG.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t\ttime.Sleep(time.Duration(5) * time.Second)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr := cron.GCronManager.AddTask(taskName, t, callback)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"add task failed\"), zap.String(\"table\", d.FinanceTable), zap.String(\"db\", d.FinanceDb), zap.String(\"cron\", t))\n\t\t\tcontinue\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (d UpdateDataCronPg) Start() (int, error) {\n\treturn 0, d.AddTasks()\n}\n<file_path>app/extract_manager/extract_manager.go\n\t\"fmt\"\n\t\"strings\"\n\n\t\"go.uber.org/zap\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka.go","template":"go","multiRes":{"hipilot":"age) (err error)","CodeQwen2.5-7B":"age) error\n}","copilot":"c (d KafkaConsumer) DebugPushMessage(f10Update my_orm.F10Update) {\n}","deepseek-chat-lite":"age) error","CodeQwen1.5-7B":"age) error"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype KafkaConsumer struct {\n\tConfig        kafka_dao.Config\n\tTask          KafkaTaskInfo\n\tconsumer      kafka_dao.Consumer\n\tconsumerGroup sarama.ConsumerGroup\n\tctx           context.Context\n\tcancel        context.CancelFunc\n\tchErr         chan error\n}\n\nfunc (d KafkaConsumer) Start() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\tlevellog.Log.Info(\"No need to start kafka consume\")\n\t\treturn 0, nil\n\t}\n\tlevellog.Log.Info(\"start init kafka consumer\", zap.Strings(\"brokers\", d.Config.Brokers),\n\t\tzap.String(\"username\", d.Config.Username), zap.String(\"password\", d.Config.Password),\n\t\tzap.String(\"consumer_group\", d.Config.ConsumerGroup), zap.String(\"SecurityProtocol\", d.Config.SecurityProtocol),\n\t\tzap.String(\"SaslMechanism\", d.Config.SaslMechanism), zap.String(\"topic\", d.Task.GetTopic()))\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) Stop() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\treturn 0, nil\n\t}\n\td.cancel()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) DebugPushMessage(msg []byte) {\n\tmessage ","infill":":= &sarama.ConsumerMessage{\n\t\tValue: msg,\n\t\tTopic: \"\"\n\t}","suffix":"\n\td.consumer.Handle(d.ctx, message)\n}\n\nfunc (d KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\td.chErr <- err\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n\tclose(d.chErr)\n}\n","relevantFile":"<file_path>app/extract/extract_api.go\npackage extract\n\n// Extract 数据抽取接口定义\ntype Extract interface {\n\tGetMarket() string       // 获取抽取器所属市场名\n\tGetTable() string        // 获取抽取器抽取的表名\n\tGetTrigMod() int         // 获取抽取触发类型（主动 or 被动）\n\tDoExtract() (int, error) // 触发数据更新,包括手动触发与自动更新\n\tStop() (int, error)\n\n\tDebugPushMessage([]byte) // 调试接口\n}\n<file_path>app/server/http.go\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tmsg, _ := c.GetRawData()\n\tlevellog.Log.Info(\"push msg\", zap.ByteString(\"request\", msg))\n\n\textract_manager.GlobalExtractManager.F10Kafka.DebugPushMessage(msg)","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka.go","template":"go","multiRes":{"hipilot":":= &sarama.ConsumerMessage{\n        Topic: d.Task.GetTopic(),\n        Value: sarama.ByteEncoder(msg),\n    }","CodeQwen2.5-7B":":= &sarama.ConsumerMessage{\n\t\tTopic: d.Task.GetTopic(),\n\t\tValue: sarama.ByteEncoder(msg),\n\t}","copilot":":= &sarama.ConsumerMessage{\n\t\tValue: msg,\n\t\tTopic: \"\"\n\t}","deepseek-chat-lite":":= &sarama.ConsumerMessage{","CodeQwen1.5-7B":" := &sarama.ConsumerMessage{"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype KafkaConsumer struct {\n\tConfig        kafka_dao.Config\n\tTask          KafkaTaskInfo\n\tconsumer      kafka_dao.Consumer\n\tconsumerGroup sarama.ConsumerGroup\n\tctx           context.Context\n\tcancel        context.CancelFunc\n\tchErr         chan error\n}\n\nfunc (d KafkaConsumer) Start() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\tlevellog.Log.Info(\"No need to start kafka consume\")\n\t\treturn 0, nil\n\t}\n\tlevellog.Log.Info(\"start init kafka consumer\", zap.Strings(\"brokers\", d.Config.Brokers),\n\t\tzap.String(\"username\", d.Config.Username), zap.String(\"password\", d.Config.Password),\n\t\tzap.String(\"consumer_group\", d.Config.ConsumerGroup), zap.String(\"SecurityProtocol\", d.Config.SecurityProtocol),\n\t\tzap.String(\"SaslMechanism\", d.Config.SaslMechanism), zap.String(\"topic\", d.Task.GetTopic()))\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) Stop() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\treturn 0, nil\n\t}\n\td.cancel()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) DebugPushMessage(msg []byte) {\n\tmessage := &sarama.ConsumerMessage{","infill":"Value: msg,\n\t\tTopic: \"om_d\"\n\t}","suffix":"\n\td.consumer.Handle(d.ctx, message)\n}\n\nfunc (d KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\td.chErr <- err\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n\tclose(d.chErr)\n}\n","relevantFile":"<file_path>app/extract/extract_api.go\npackage extract\n\n// Extract 数据抽取接口定义\ntype Extract interface {\n\tGetMarket() string       // 获取抽取器所属市场名\n\tGetTable() string        // 获取抽取器抽取的表名\n\tGetTrigMod() int         // 获取抽取触发类型（主动 or 被动）\n\tDoExtract() (int, error) // 触发数据更新,包括手动触发与自动更新\n\tStop() (int, error)\n\n\tDebugPushMessage([]byte) // 调试接口\n}\n<file_path>app/server/http.go\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tmsg, _ := c.GetRawData()\n\tlevellog.Log.Info(\"push msg\", zap.ByteString(\"request\", msg))\n\n\textract_manager.GlobalExtractManager.F10Kafka.DebugPushMessage(msg)","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka.go","template":"go","multiRes":{"hipilot":"Value: msg}","CodeQwen2.5-7B":"\n\t\tTopic: d.Task.GetTopic(),\n\t\tKey:   sarama.StringEncoder(\"debug\"),\n\t\tValue: sarama.ByteEncoder(msg),\n\t}<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"Value: msg,\n\t\tTopic: \"om_d\"\n\t}","deepseek-chat-lite":"","CodeQwen1.5-7B":""}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype KafkaConsumer struct {\n\tConfig        kafka_dao.Config\n\tTask          KafkaTaskInfo\n\tconsumer      kafka_dao.Consumer\n\tconsumerGroup sarama.ConsumerGroup\n\tctx           context.Context\n\tcancel        context.CancelFunc\n\tchErr         chan error\n}\n\nfunc (d KafkaConsumer) Start() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\tlevellog.Log.Info(\"No need to start kafka consume\")\n\t\treturn 0, nil\n\t}\n\tlevellog.Log.Info(\"start init kafka consumer\", zap.Strings(\"brokers\", d.Config.Brokers),\n\t\tzap.String(\"username\", d.Config.Username), zap.String(\"password\", d.Config.Password),\n\t\tzap.String(\"consumer_group\", d.Config.ConsumerGroup), zap.String(\"SecurityProtocol\", d.Config.SecurityProtocol),\n\t\tzap.String(\"SaslMechanism\", d.Config.SaslMechanism), zap.String(\"topic\", d.Task.GetTopic()))\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) Stop() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\treturn 0, nil\n\t}\n\td.cancel()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) DebugPushMessage(msg []byte) {\n\tmessage := &sarama.ConsumerMessage{\n\t\tValue: msg,\n\t\tTopic: \"om_debug_push_messs","infill":"age","suffix":"\",\n\t}\n\td.consumer.Handle(d.ctx, message)\n}\n\nfunc (d KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\td.chErr <- err\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n\tclose(d.chErr)\n}\n","relevantFile":"<file_path>app/extract/extract_api.go\npackage extract\n\n// Extract 数据抽取接口定义\ntype Extract interface {\n\tGetMarket() string       // 获取抽取器所属市场名\n\tGetTable() string        // 获取抽取器抽取的表名\n\tGetTrigMod() int         // 获取抽取触发类型（主动 or 被动）\n\tDoExtract() (int, error) // 触发数据更新,包括手动触发与自动更新\n\tStop() (int, error)\n\n\tDebugPushMessage([]byte) // 调试接口\n}\n<file_path>app/server/http.go\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tmsg, _ := c.GetRawData()\n\tlevellog.Log.Info(\"push msg\", zap.ByteString(\"request\", msg))\n\n\textract_manager.GlobalExtractManager.F10Kafka.DebugPushMessage(msg)","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka.go","template":"go","multiRes":{"hipilot":"age","CodeQwen2.5-7B":"age<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"age","deepseek-chat-lite":"age","CodeQwen1.5-7B":" age<file_sep> /app/dao/kafka/kafka_consumer.go"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype KafkaConsumer struct {\n\tConfig        kafka_dao.Config\n\tTask          KafkaTaskInfo\n\tconsumer      kafka_dao.Consumer\n\tconsumerGroup sarama.ConsumerGroup\n\tctx           context.Context\n\tcancel        context.CancelFunc\n\tchErr         chan error\n}\n\nfunc (d KafkaConsumer) Start() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\tlevellog.Log.Info(\"No need to start kafka consume\")\n\t\treturn 0, nil\n\t}\n\tlevellog.Log.Info(\"start init kafka consumer\", zap.Strings(\"brokers\", d.Config.Brokers),\n\t\tzap.String(\"username\", d.Config.Username), zap.String(\"password\", d.Config.Password),\n\t\tzap.String(\"consumer_group\", d.Config.ConsumerGroup), zap.String(\"SecurityProtocol\", d.Config.SecurityProtocol),\n\t\tzap.String(\"SaslMechanism\", d.Config.SaslMechanism), zap.String(\"topic\", d.Task.GetTopic()))\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) Stop() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\treturn 0, nil\n\t}\n\td.cancel()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) DebugPushMessage(msg []byte) {\n\tmessage := &sarama.ConsumerMessage{\n\t\tValue: msg,\n\t\tTopic: \"om_debug_push_messsage\",\n\t}\n\nd.Task.\n\n\tif d.consumer.Handle == nil { // 未初始化\n\t\tlevellog.Log.Info(\"kafka consumer handle is nil\", \n\t\treturn\n\t}\n\n\td.consumer.Handle(d.ctx, ","infill":"\n\t\treturn\n\t}\n\n\td.consumer.Handle(d.ctx, ","suffix":"\n\t\treturn\n\t}\n\n\td.consumer.Handle(d.ctx, message)\n}\n\nfunc (d KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\td.chErr <- err\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n\tclose(d.chErr)\n}\n","relevantFile":"<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka.go","template":"go","multiRes":{"hipilot":"message)","CodeQwen2.5-7B":"message)\n}","copilot":"\n\t\treturn\n\t}\n\n\td.consumer.Handle(d.ctx, ","deepseek-chat-lite":"message)","CodeQwen1.5-7B":" message)"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype KafkaConsumer struct {\n\tConfig        kafka_dao.Config\n\tTask          KafkaTaskInfo\n\tconsumer      kafka_dao.Consumer\n\tconsumerGroup sarama.ConsumerGroup\n\tctx           context.Context\n\tcancel        context.CancelFunc\n\tchErr         chan error\n}\n\nfunc (d KafkaConsumer) Start() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\tlevellog.Log.Info(\"No need to start kafka consume\")\n\t\treturn 0, nil\n\t}\n\tlevellog.Log.Info(\"start init kafka consumer\", zap.Strings(\"brokers\", d.Config.Brokers),\n\t\tzap.String(\"username\", d.Config.Username), zap.String(\"password\", d.Config.Password),\n\t\tzap.String(\"consumer_group\", d.Config.ConsumerGroup), zap.String(\"SecurityProtocol\", d.Config.SecurityProtocol),\n\t\tzap.String(\"SaslMechanism\", d.Config.SaslMechanism), zap.String(\"topic\", d.Task.GetTopic()))\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) Stop() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\treturn 0, nil\n\t}\n\td.cancel()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) DebugPushMessage(msg []byte) {\n\tmessage := &sarama.ConsumerMessage{\n\t\tValue: msg,\n\t\tTopic: \"om_debug_push_messsage\",\n\t}\n\n\tif d.consumer.Handle == nil { // 未初始化\n\t\tlevellog.Log.Info(\"kafka consumer handle is nil\",","infill":" zap.String(\"topic\", d.Task.GetTopic()))","suffix":"\n\t\treturn\n\t}\n\n\td.consumer.Handle(d.ctx, message)\n}\n\nfunc (d KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\td.chErr <- err\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n\tclose(d.chErr)\n}\n","relevantFile":"<file_path>app/server/http.go\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tmsg, _ := c.GetRawData()\n\tlevellog.Log.Info(\"push msg\", zap.ByteString(\"request\", msg))\n\n\tif extract_manager.GlobalExtractManager.F10Kafka == nil {\n\t\tc.String(400, \"f10 kafka not init\")\n\t\treturn\n\t}\n\n\textract_manager.GlobalExtractManager.F10Kafka.DebugPushMessage(msg)\n\tc.String(200, \"ok\")\n}","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka.go","template":"go","multiRes":{"hipilot":"zap.String(\"topic\", d.Task.GetTopic()))","CodeQwen2.5-7B":" zap.String(\"topic\", d.Task.GetTopic()))\n\t\treturn\n\t}","copilot":" zap.String(\"topic\", d.Task.GetTopic()))","deepseek-chat-lite":" zap.String(\"topic\", \"om_debug_push_messsage\"))","CodeQwen1.5-7B":"  zap.String(\"topic\", d.Task.GetTopic()))<file_sep> /app/dao/kafka/kafka_consumer.go"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 ","infill":"struct {\n\tMarket string `json:\"market\"`\n\tCode   string `json:\"code\"`\t\n\tDate   string \n}","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicat","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"struct {\n    CalculateMethod string `json:\"calculate_method\"`\n    Cycle            string `json:\"cycle\"`\n    Currency        string `json:\"currency\"`\n    Date            string `json:\"date\"`\n    Format          string `json:\"format\"`\n","CodeQwen2.5-7B":"struct {\n\tIndicatorId string `json:\"indicator_id\"`\n\tIndicator   string `json:\"indicator\"`\n}","copilot":"struct {\n\tMarket string `json:\"market\"`\n\tCode   string `json:\"code\"`\t\n\tDate   string \n}","deepseek-chat-lite":"struct {","CodeQwen1.5-7B":" struct {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket","infill":" string `json:\"market\"`","suffix":"\n\tCode   string `json:\"code\"`\n\tDate   string `json:\"date\"`\n\tA\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"string `json:\"market\"`","CodeQwen2.5-7B":"Type string `json:\"market_type\"`\n\t// 证券代码\n\tCode string `json:\"code\"`\n\t// 证券名称\n\tName string `json:\"name\"`\n\t// 证券类型\n\tAssetType string `json:\"asset_type\"`\n\t// 证券类型\n\tAssetType2 string `json:\"asset_type2\"`\n\t// 证券类型\n\tAssetType3 string `json:\"asset_type3\"`\n\t// 证券类型\n\tAssetType4 string `json:\"asset_type4\"`\n\t// 证券类型\n\tAssetType5 string `json:\"asset_type5\"`\n\t// 证券类型\n\tAssetType6 string `json:\"asset_type6\"`\n\t// 证券类型\n\tAssetType7 string `json:\"asset_type7\"`\n\t// 证券类型\n\tAssetType8 string `json:\"asset_type8\"`\n\t// 证券类型\n\tAssetType9 string `json:\"asset_type9\"`\n\t// 证券类型\n\tAssetType10 string `json:\"asset_type10\"`\n\t// 证券类型\n\tAssetType11 string `json:\"asset_type11\"`\n\t// 证券类型\n\tAssetType12 string `json:\"asset_type12\"`\n\t// 证券类型\n\tAssetType13 string `json:\"asset_type13\"`\n\t// 证券类型\n\tAssetType14 string `json:\"asset_type14\"`\n\t// 证券类型\n","copilot":" string `json:\"market\"`","deepseek-chat-lite":"Type string `url:\"marketType\"`","CodeQwen1.5-7B":"  string `json:\"market\"`"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket string `","infill":"json:\"market\"`","suffix":"\n\tCode   string `json:\"code\"`\n\tDate   string `json:\"date\"`\n\tAsse\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"json:\"market\"`","CodeQwen2.5-7B":"json:\"market\"`<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"json:\"market\"`","deepseek-chat-lite":"json:\"market\"`","CodeQwen1.5-7B":"json:\"market\"`"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket string `json:\"market\"`\n\tCode   string ","infill":"`json:\"code\"`","suffix":"\n\tDate   string `json:\"date\"`\n\tA\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"`json:\"code\"`","CodeQwen2.5-7B":"`json:\"code\"`\n\tName   string `json:\"name\"`\n}","copilot":"`json:\"code\"`","deepseek-chat-lite":"`json:\"code\"`","CodeQwen1.5-7B":" `json:\"code\"`"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc ","infill":"(d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicat","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"(d F10KafkaTaskInfo) Setup() error {\n    return nil\n}","CodeQwen2.5-7B":"(d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (IndicatorInfoFromF10, error) {\n\tvar jsonF10 IndicatorInfoFromF10\n\tvar err error\n\t// 1. 请求CNY币种的最新数据\n\tjsonF10, err = d.RequestF10Data(indicatorIDs, subjects, currency, date)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn jsonF10, err\n\t}","copilot":"(d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))","deepseek-chat-lite":"(d F10KafkaTaskInfo) RequestF10Data(indicatorIds string, subjects string, currency string, date string) (*my_orm.F10Responses, error) {","CodeQwen1.5-7B":" (d F10KafkaTaskInfo) RequestF10Data(indicatorIds string, subjects string, currency string, date string) (my_orm.F10Responses, error) {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)","infill":"taskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t\t","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicat","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"(thsCode)\n    return market, thsCode, assetData, err","CodeQwen2.5-7B":"\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}","copilot":"taskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t\t","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency","infill":"if currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\n\t","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicat","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"\"","CodeQwen2.5-7B":"\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n}","copilot":"if currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\n\t","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(F10KafkaTaskInfo, f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\n\t}","infill":"assetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n}","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicat","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"assetData = jsonF10Rsp.Data.Data[0]\n    return market, thsCode, assetData, err\n    // // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n    // //    如果一致，则","CodeQwen2.5-7B":"\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n}","copilot":"assetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(F10KafkaTaskInfo , f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]","infill":"return market, thsCode, assetData, err\n}","suffix":"\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicat","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"0\n    return market, thsCode, assetData, err","CodeQwen2.5-7B":"\n\treturn market, thsCode, assetData, err\n}","copilot":"return market, thsCode, assetData, err\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (IndicatorInfoFromF10, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn IndicatorInfoFromF10{}, err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn IndicatorInfoFromF10{","infill":"Market:    market,\n\t\t\tCode:      thsCode,\n\t\t\tCurrency:  currency,\n\t\t\tDate:      assetData.Data[0][taskInfo.Date],\n\t\t\tAssetData: assetData,","suffix":"\n\t\t}, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixti","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"}, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)","CodeQwen2.5-7B":"Market: market, Code: thsCode, Currency: currency, Date: assetData.Data[0][taskInfo.Date], AssetData: assetData}, nil\n\t}","copilot":"Market:    market,\n\t\t\tCode:      thsCode,\n\t\t\tCurrency:  currency,\n\t\t\tDate:      assetData.Data[0][taskInfo.Date],\n\t\t\tAssetData: assetData,","deepseek-chat-lite":"}, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)","CodeQwen1.5-7B":" Market: market, Code: thsCode, Currency: currency, Date: assetData.Data[0][taskInfo.Date], AssetData: assetData}"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (IndicatorInfoFromF10, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn IndicatorInfoFromF10{}, err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn IndicatorInfoFromF10{\n\t\t\tMarket:    market,\n\t\t\tCode:      thsCode,\n\t\t\tCurrency:  currency,","infill":"Date:      assetData.Data[0][taskInfo.Date],\n\t\t\tAssetData: assetData,","suffix":"\n\t\t}, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixti","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"Date:      assetData.Data[0][d.Date], AssetData: assetData,","CodeQwen2.5-7B":"\n\t\t\tDate:      assetData.Data[0].Date,\n\t\t\tAssetData: assetData,\n\t\t}, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}","copilot":"Date:      assetData.Data[0][taskInfo.Date],\n\t\t\tAssetData: assetData,","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype IndicatorInfoFromF10 struct {\n\tMarket    string           `json:\"market\"`\n\tCode      string           `json:\"code\"`\n\tCurrency  string           `json:\"currency\"`\n\tDate      string           `json:\"date\"`\n\tAssetData my_orm.AssetData `json:\"assetData\"`\n}\n\nfunc (d *F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (IndicatorInfoFromF10, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn IndicatorInfoFromF10{}, err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\n\tindicatorInfo","infill":"FromF10 :=","suffix":" IndicatorInfoFromF10{\n\t\tMarket:    market,\n\t\tCode:      thsCode,\n\t\tCurrency:  currency,\n\t\tDate:      assetData.Data[0][taskInfo.Date],\n\t\tAssetData: assetData,\n\t}\n\n\tif !ok {\n\t\treturn indicatorInfoFromF10, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\n// GetAssetDataByRequestF10 根据提供的参数从F10 API获取资产数据。\n// 该函数首先请求\"CNY\"货币的最新数据，然后根据返回的市场的货币类型可能会再次请求数据。\n// 如果返回的数据日期与请求的日期一致，函数将直接返回数据，否则会再次请求数据。\n//\n// 注意：如果在请求F10数据时出现错误，函数将记录警告并返回错误。\n//\n// 参数：\n// - f10TaskId：F10任务的ID，字符串类型。\n// - indicatorIds：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - market：市场的字符串表示。\n// - thsCode：THS代码的字符串表示。\n// - assetData：AssetData类型的对象，包含从F10 API获取的资产数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// f10TaskId := \"task123\"\n// indicatorIds := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// date := \"2022-01-01\"\n// market, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, indicatorIds, subjects, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"获取资产数据失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"市场: %s, THS代码: %s, 资产数据: %v\", market, thsCode, assetData)\nfunc (d F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, indicatorIds string, subjects string) (string, string, my_orm.AssetData, error) {\n\t// 1. 请求CNY币种的最新数据\n\tcurrency := \"CNY\"\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"request f10 data from api failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects), zap.Error(err))\n\t\treturn \"\", \"\", jsonF10Rsp.Data.Data[0], err\n\t}\n\n\tassetData := jsonF10Rsp.Data.Data[0]\n\tmarket := common.SubMarket2Market(assetData.Subject.Market)\n\tthsCode := common.TransCodeToThsCode(assetData.Subject.Code, market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn market, thsCode, assetData, fmt.Errorf(\"taskid and market map not found, taskid: %s, market: %s\", f10TaskId, market)\n\t}\n\n\t// 2. 如果返回的市场的币种是 CNY,则直接使用数据，否则调整币种类型再请求一次\n\tcurrency = taskInfo.Currency\n\tif currency != \"CNY\" {\n\t\tjsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, \"\")\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\t\treturn market, thsCode, assetData, err\n\t\t}\n\t}\n\tassetData = jsonF10Rsp.Data.Data[0]\n\treturn market, thsCode, assetData, err\n\t// // 3. 判断数据的日期是否有效，即是否和请求的日期一致\n\t// //    如果一致，则直接返回数据，否则再请求一次\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\t// if assetData.Data[0][taskInfo.Date] == date {\n\t// \treturn market, thsCode, assetData, nil\n\t// }\n\t// jsonF10Rsp, err = d.RequestF10Data(indicatorIds, subjects, currency, date)\n\t// assetData = jsonF10Rsp.Data.Data[0]\n\n\t// return market, thsCode, assetData, err\n}\n\n// 处理Kafka接收的指标数据，处理逻辑如下\n// 1. 指标数据变更会推送kafka消息，根据推送的消息提取代码和日期\n// 2. 由于变更存在新增、删除、更新三个状态，并且如果时删除操作的情况扶摇接口查询的结果为空。所以需要先按最新状态查询一次数据，再按日期查询一次数据\n// 3. 如果第一次查询的日期和kafka推送的日期一致，则直接使用第一次查询的数据更新数据库\n// 4. 如果第一次查询的日期和kafka推送的日期不一致，则再按照kafka推送的日期查询一次数据，再更新数据库\n// 5. 如果第二次查询的数据为空，则更新为无效数据\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, f func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tmarket, thsCode, assetData, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetri","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"FromF10 :=","CodeQwen2.5-7B":"FromF10 := IndicatorInfoFromF10{\n\t\tMarket:    market,\n\t\tCode:      thsCode,\n\t\tCurrency:  currency,\n\t\tDate:      assetData.Data[0][taskInfo.Date],\n\t\tAssetData: assetData,\n\t}","copilot":"FromF10 :=","deepseek-chat-lite":"FromF10 :=","CodeQwen1.5-7B":" FromF10 := IndicatorInfoFromF10{"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d","infill":" F10KafkaTaskInfo) GetMarketCodeFromF10(f10TaskId string, subjects string) (ThsMarketCode, error) {\n\t\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr","suffix":" = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskI","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"F10KafkaTaskInfo) ProcessMessage(message *sarama.ConsumerMessage) {\n    var jsonF10 common.JsonF10\n    err := json.Unmarshal(message.Value, &jsonF10)\n    if err != nil {","CodeQwen2.5-7B":" F10KafkaTaskInfo) GetAssetDataByRequestF10(f10TaskId string, ids string, subjects string) (my_orm.F10Responses, error) {\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取该指标的市场\n\t// 从数据库中获取","copilot":" F10KafkaTaskInfo) GetMarketCodeFromF10(f10TaskId string, subjects string) (ThsMarketCode, error) {\n\t\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr","deepseek-chat-lite":" F10KafkaTaskInfo) GetTaskProcess() map[string]F10KafkaTask {","CodeQwen1.5-7B":"  F10KafkaTaskInfo) GetTaskIdIndicators() map[string]string {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\tmarket := jsonF10Rsp.Data.Data[0].Market\n\tif market == \"\" {\n\n}\n\nfunc (d F10KafkaTask","infill":"\n\n}\n\nfunc (d F10KafkaTask","suffix":"\n\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderly","relevantFile":"<file_path>app/orm/f10.go\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {\n\t\tSubject SubjectData         `json:\"subject\"`","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"Info) UpdateUnderlyingAssetData(thsCode ThsMarketCode, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n    // 更新本地mysql数据","CodeQwen2.5-7B":"Info) GetAssetDataByRequestF10(f10TaskId string, ids string, subjects string) (my_orm.F10Responses, error) {\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, ids, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}","copilot":"\n\n}\n\nfunc (d F10KafkaTask","deepseek-chat-lite":"Info) GetAssetDataByRequestF10(f10TaskId string, indicators string, subjects string) (my_orm.F10Responses, error) {","CodeQwen1.5-7B":" Info) GetAssetDataByRequestF10(f10TaskId string, ids string, subjects string) (my_orm.F10Responses, error) {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get \n\n\treturn ThsMarketCode{market, code}, nil\n\n}\n\nfunc (d F10KafkaTaskInfo) Consume(c","infill":"\n\n}\n\nfunc (d F10KafkaTaskInfo) Consume(c","suffix":"\n\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\tids, ok := d.MapTaskIdIndicators[f10TaskId]\n\tif !ok {\n\t\tlevellog.Log.Debug(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", f10TaskId))\n\t\treturn\n\t}\n\t// 先根据 subjects 查询一下推送指标的市场\n\tindicatorInfoFromF10, err := d.GetAssetDataByRequestF10(f10TaskId, ids, subjects)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderly","relevantFile":"<file_path>app/orm/f10.go\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {\n\t\tSubject SubjectData         `json:\"subject\"`","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"tx context.Context, message *sarama.ConsumerMessage)","CodeQwen2.5-7B":"tx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()","copilot":"\n\n}\n\nfunc (d F10KafkaTaskInfo) Consume(c","deepseek-chat-lite":"tx context.Context, message *sarama.ConsumerMessage) {","CodeQwen1.5-7B":" tx context.Context, message *sarama.ConsumerMessage) {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\treturn\n\t}\n\n\tthsMarketCode","infill":", err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t}","suffix":"\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGe","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":", err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs) // 至少要获取一个\n    if err != nil {\n        levellog.Log.Info(\"get market and code from kafka push message","CodeQwen2.5-7B":", err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":", err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t}","deepseek-chat-lite":", err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)","CodeQwen1.5-7B":" , err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by t\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)","infill":"return\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"];","suffix":" ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, func(dateField string) string { return jsonF10.Data[dateField] })\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tc","relevantFile":"<file_path>app/orm/f10.go\npackage my_orm\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"0\n        return\n    }\n\n    valid, validInt := \"\", 0\n    if valid, ok = jsonF10.Data[\"valid\"];","CodeQwen2.5-7B":"\n\t\treturn\n\t}","copilot":"return\n\t}\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"];","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\trecvDate ","infill":":= jsonF10.Data[taskInfo.Date]","suffix":"\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, recvDate)\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Respon","relevantFile":"<file_path>app/orm/f10.go\n\n// F10Request F10请求结构体，需要转为form格式\n//type F10Request struct {\n//\tIds      string `url:\"ids\"`\n//\tCode     string `url:\"code\"`\n//\tMarket   string `url:\"market\"`\n//\tType     string `url:\"type\"`\n//\tCurrency string `url:\"currency\"`\n//\tStart    string `url:\"start\"`\n//\tEnd      string `url:\"end\"`\n//\tLimit    int    `url:\"limit\"`\n//}\n\n// F10Responses F10响应，从json转为该结构体\n//type (\n//\tF10Data struct {\n//\t\tReport   string                 `json:\"report\"`\n//\t\tDate     string                 `json:\"date\"`\n//\t\tCurrency string                 `json:\"currency\"`\n//\t\tData     map[string]interface{} `json:\"data\"`\n//\t}\n//\n//\tF10Responses struct {\n//\t\tStatusCode int       `json:\"status_code\"`\n//\t\tStatusMsg  string    `json:\"status_msg\"`\n//\t\tData       []F10Data `json:\"data\"`\n//\t}\n//\n//\tF10Update struct {\n//\t\tData   map[string]interface{} `json:\"data\"`\n//\t\tTaskId string                 `json:\"taskId\"`\n//\t\tTime   string                 `json:\"time\"`\n//\t}\n//)\n\n// F10Request F10请求结构体，需要转为form格式\ntype F10Request struct {\n\tIds      string `url:\"ids\"`\n\tSubjects string `url:\"subjects\"`\n\tCategory string `url:\"category\"`\n\tStrategy string `url:\"strategy\"`\n\tFrom     string `url:\"from\"`\n\tTo       string `url:\"to\"`\n\tLanguage string `url:\"language\"`\n\tCurrency string `url:\"currency\"`\n}\n\n// F10Responses F10响应，从json转为该结构体\ntype (\n\tSubjectData struct {\n\t\tMarket   string `json:\"market\"`\n\t\tSecId    string `url:\"sec_id\"`\n\t\tCode     string `json:\"code\"`\n\t\tOrgId    string `url:\"org_id\"`\n\t\tThsCode  string `url:\"ths_code\"`\n\t\tCategory string `url:\"category\"`\n\t}\n\n\tAssetData struct {\n\t\tSubject SubjectData         `json:\"subject\"`","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":":= funcRecvDate(taskInfo.Date) // 获取日期\n\tif recvDate == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"date\", recvDate))\n\t\treturn\n\t}<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":":= jsonF10.Data[taskInfo.Date]","deepseek-chat-lite":":= func(dateField string) string {","CodeQwen1.5-7B":" := func(dateField string) string {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency","infill":" := taskInfo.Currency","suffix":"\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, recvDate)\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Respon","relevantFile":"<file_path>app/metrics/metrics.go\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":" := jsonF10.Data[\"currency\"]\n\tif currency == \"\" {\n\t\tcurrency = \"CNY\"\n\t}","copilot":" := taskInfo.Currency","deepseek-chat-lite":" := jsonF10.Data[taskInfo.Currency]","CodeQwen1.5-7B":" := \"\""}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"","infill":"NotFoundTaskIdFromConfig","suffix":"\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorsFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, recvDate)\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.Strin","relevantFile":"<file_path>app/metrics/metrics.go\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"NotFoundTaskIdFromConfig","CodeQwen2.5-7B":"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}","copilot":"NotFoundTaskIdFromConfig","deepseek-chat-lite":"NotFoundTaskInfoFromConfig\")","CodeQwen1.5-7B":"NotFoundTaskIdFromConfig\")"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tif validInt == 0 {\n\t\td.ProcessInvalidZeroData(","infill":"thsMarketCode.Code, recvDate, taskInfo)","suffix":"\n\t\treturn\n\n\t}\n\n\td.ProcessIndicatorData(jsonF10.TaskId, validInt, subjects, recvDate)\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRe","relevantFile":"<file_path>app/metrics/metrics.go\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"thsCode, recvDate, taskInfo)\n\t\treturn\n\t}","copilot":"thsMarketCode.Code, recvDate, taskInfo)","deepseek-chat-lite":"thsMarketCode.Code, recvDate, taskInfo)","CodeQwen1.5-7B":" thsCode.Code, recvDate, taskInfo)"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tif validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetInvalidZeroFromRecvMsg(message.Topic, message.Partition, jsonF10.TaskId)\n\t\treturn\n\n\t}\n\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIDs, subjects, currency, recvDate)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request indicator data from f10 b","infill":"message.Topic, message.Partition, jsonF10.TaskId, \"RequestF10ApiFailed\")","suffix":"\n\t\treturn\n\t}\n}\n\nfunc (d F10KafkaTaskInfo) ProcessIndicatorData(f10TaskId string, validInt int, subjects string, funcRecvDate func(dateField string) string) {\n\n\t// 先根据 subjects 查询一下推送指标的市场，\n\tjsonF10Rsp, err := d.RequestF10Data(ids, subjects, \"CNY\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn\n\t}\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\ttaskInfo, ok := d.MapTaskProcess[f10TaskId+\"_\"+market]\n\tif !ok {\n\t\treturn\n\t}\n\n\t// 获取同花顺代码\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\n\t// 获取日期, 这里不同的指标，取日期字段可能是不同的；\n\t// 同一个taskid 下的指标返回在同一个数据段里面, 这里每个tasid下的指标使用相同的日期字段\n\trecvDete := f(taskInfo.Date)\n\tdateStr, ok := assetData.Data[0][taskInfo.Date]\n\tif !ok || dateStr == \"\" {\n\t\tlevellog.Log.Info(\"taskid date is null\", zap.String(\"task\", f10TaskId), zap.String(\"date\", dateStr))\n\t\treturn\n\t}\n\n\t// 如果 validInt = 0,则将对应的数据的 isvalid 置为 0\n\tif validInt == 0 || dateStr == \"\" {\n\t\td.ProcessInvalidZeroData(thsCode, dateStr, taskInfo)\n\t\treturn\n\t}\n\n\t//开始解析返回的F10数据\n\td.UpdateUnderlyingAssetData(thsCode, validInt, dateStr, jsonF10Rsp, taskInfo)\n\treturn\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldN","relevantFile":"<file_path>app/metrics/metrics.go\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"y taskid failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn\n\t}","copilot":"message.Topic, message.Partition, jsonF10.TaskId, \"RequestF10ApiFailed\")","deepseek-chat-lite":"ase on taskid failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))","CodeQwen1.5-7B":" y taskid failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))"}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (\n\tStageAll       = \"all\"       //完整阶段\n\tStageExtract   = \"extract\"   //抽取\n\tStageTransform = \"transform\" //转换\n\tStageLoad      = \"load\"      //持久化\n)\n\n// 数据源source\nconst (\n\tSourcePg              = \"pg\"              // pg数据库\n\tSourceF10             = \"f10\"             // F10\n\tSourceFUsEtfAdjFactor = \"FUsEtfAdjFactor\" //美股ETF复权因子\n)\n\nvar (\n\t// 接收数据条数\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_rx_traffic\",\n\t\t\tHelp: \"count received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// qps\n\t// trigger: cron kafka\n\t// source: pg, f10\n\tqpsCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_qps\",\n\t\t\tHelp: \"request received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// 处理耗时分布\n\t// trigger: cron manual kafka\n\t// stage: extract proccess load\n\t// export: all bbrq rtime real code\n\t// source: pg, f10\n\tcostReqBucketVec = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"gms_etl_cost_histogram\",\n\t\t\tHelp:    \"cost of time for each request.\",\n\t\t\tBuckets: []float64{100, 1000, 5000, 10000, 50000, 120000, 600000},\n\t\t},\n\t\t[]string{\"db\", \"table\", \"stage\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// label_name 面板显示时公有显示\n\tnameCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_label_name\",\n\t\t\tHelp: \"show label name\",\n\t\t},\n\t\t[]string{\"db\", \"table\"},\n\t)\n\n\t// 错误数 记录接口错误,内部错误等\n\terrorCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_error\",\n\t\t\tHelp: \"errors\",\n\t\t},\n\t\t[]string{\"status\"},\n\t)\n\n\t// some metrics of kafka consumer, such as package, data flow, etc.\n\tkafkaDataRecvPackage = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_package\",\n\t\t\tHelp: \"data recv package\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaRecvSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_recv_task_id\",\n\t\t\tHelp: \"skip recv task id\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\", \"error\"},\n\t)\n\n\tcountSetInvalidZero = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"set_invalid_zero\",\n\t\t\tHelp: \"set invalid zero\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"task_id\", \"reason\"},\n\t)\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n\n\tprometheus.MustRegister(kafkaDataRecvPackage)\n\tprometheus.MustRegister(kafkaDataRecvFlow)\n\tprometheus.MustRegister(kafkaRecvSkipTaskId)\n\n\tprometheus.MustRegister(countSetInvalidZero)\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n// KafkaDataRecvPackage kafka数据包接收统计\nfunc KafkaDataRecvPackage(topic string, partition int32, taskId string, packageNum int) {\n\tkafkaDataRecvPackage.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(packageNum))\n}\n\n// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition int32, taskId string, flow int) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(flow))\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string, errType string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId, errType).Inc()\n}\n\nfunc SetInvalidZero(","infill":"db string, table string, taskId string, reason string) {","suffix":"\n\tcountSetInvalidZero.WithLabelValues(db, table, taskId, reason).Inc()\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"db string, table string, taskId string, reason string) {","CodeQwen2.5-7B":"db string, table string, taskId string, reason string) {","copilot":"db string, table string, taskId string, reason string) {","deepseek-chat-lite":"db string, table string, taskId string, reason string) {","CodeQwen1.5-7B":" db string, table string, taskId string, reason string) {<file_sep> /app/dao/pg/pg_test.go"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tif validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetInvalidZero(taskInfo.TableName, j","infill":"sonF10.TaskId","suffix":", \"RecvZero\")\n\t\treturn\n\n\t}\n\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIDs, subjects, currency, recvDate)\n\tif err != nil {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetInvalidZero(taskInfo.TableName, jsonF10.TaskId, \"RequestEmpty\")\n\t\treturn\n\t}\n\n\td.UpdateUnderlyingAssetData(thsMarketCode.Code, 1, recvDate, jsonF10Rsp, taskInfo)\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"<file_path>app/metrics/metrics.go\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"sonF10.TaskId<|file_sep|><|fim_prefix|>/app/transform/transform.go\npackage transform","copilot":"sonF10.TaskId","deepseek-chat-lite":"sonF10.TaskId","CodeQwen1.5-7B":" sonF10.TaskId<file_sep> /app/transform/transform.go"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(f10TaskId, indicators, subjects, \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tif validInt == 0 {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetInvalidZero(taskInfo.TableName, jsonF10.TaskId, \"RecvZero\")\n\t\treturn\n\n\t}\n\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIDs, subjects, currency, recvDate)\n\tif err != nil {\n\t\td.ProcessInvalidZeroData(thsMarketCode.Code, recvDate, taskInfo)","infill":"taskInfo.TableName, jsonF10.TaskId, \"RequestEmpty\")","suffix":"\n\t\treturn\n\t}\n\n\td.UpdateUnderlyingAssetData(thsMarketCode.Code, 1, recvDate, jsonF10Rsp, taskInfo)\n}\n\nfunc (d F10KafkaTaskInfo) ProcessInvalidZeroData(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，将使用当前日期。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"<file_path>app/metrics/metrics.go\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"metrics.SetInvalidZero(taskInfo.TableName, jsonF10.TaskId, \"ReqF10Failed\")","CodeQwen2.5-7B":"\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn\n\t}","copilot":"taskInfo.TableName, jsonF10.TaskId, \"RequestEmpty\")","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (\n\tStageAll       = \"all\"       //完整阶段\n\tStageExtract   = \"extract\"   //抽取\n\tStageTransform = \"transform\" //转换\n\tStageLoad      = \"load\"      //持久化\n)\n\n// 数据源source\nconst (\n\tSourcePg              = \"pg\"              // pg数据库\n\tSourceF10             = \"f10\"             // F10\n\tSourceFUsEtfAdjFactor = \"FUsEtfAdjFactor\" //美股ETF复权因子\n)\n\nvar (\n\t// 接收数据条数\n\trxTrafficCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_rx_traffic\",\n\t\t\tHelp: \"count received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// qps\n\t// trigger: cron kafka\n\t// source: pg, f10\n\tqpsCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_qps\",\n\t\t\tHelp: \"request received.\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// 处理耗时分布\n\t// trigger: cron manual kafka\n\t// stage: extract proccess load\n\t// export: all bbrq rtime real code\n\t// source: pg, f10\n\tcostReqBucketVec = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"gms_etl_cost_histogram\",\n\t\t\tHelp:    \"cost of time for each request.\",\n\t\t\tBuckets: []float64{100, 1000, 5000, 10000, 50000, 120000, 600000},\n\t\t},\n\t\t[]string{\"db\", \"table\", \"stage\", \"trigger\", \"export\", \"source\"},\n\t)\n\n\t// label_name 面板显示时公有显示\n\tnameCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_label_name\",\n\t\t\tHelp: \"show label name\",\n\t\t},\n\t\t[]string{\"db\", \"table\"},\n\t)\n\n\t// 错误数 记录接口错误,内部错误等\n\terrorCounterVec = prometheus.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"gms_etl_error\",\n\t\t\tHelp: \"errors\",\n\t\t},\n\t\t[]string{\"status\"},\n\t)\n\n\t// some metrics of kafka consumer, such as package, data flow, etc.\n\tkafkaDataRecvPackage = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_package\",\n\t\t\tHelp: \"data recv package\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaDataRecvFlow = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"data_recv_flow\",\n\t\t\tHelp: \"data recv flow\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\"},\n\t)\n\n\tkafkaRecvSkipTaskId = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"skip_recv_task_id\",\n\t\t\tHelp: \"skip recv task id\",\n\t\t},\n\t\t[]string{\"topic\", \"partition\", \"task_id\", \"error\"},\n\t)\n\n\tcountSetInvalidZero = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"set_invalid_zero\",\n\t\t\tHelp: \"set invalid zero\",\n\t\t},\n\t\t[]string{\"db\", \"table\", \"task_id\", \"reason\"},\n\t)\n\n\t// 处理函数\n\tpromHttpHandler = gin.WrapH(promhttp.Handler())\n)\n\n// GetMetrics 指标结果获取接口\nfunc GetMetrics(c *gin.Context) {\n\tpromHttpHandler(c)\n}\n\n// InitMetrics 指标初始化\nfunc InitMetrics() {\n\tprometheus.MustRegister(rxTrafficCounterVec)\n\tprometheus.MustRegister(qpsCounterVec)\n\tprometheus.MustRegister(costReqBucketVec)\n\tprometheus.MustRegister(errorCounterVec)\n\tprometheus.MustRegister(nameCounterVec)\n\n\tprometheus.MustRegister(kafkaDataRecvPackage)\n\tprometheus.MustRegister(kafkaDataRecvFlow)\n\tprometheus.MustRegister(kafkaRecvSkipTaskId)","infill":"prometheus.MustRegister(countSetInvalidZero)","suffix":"\n}\n\n// TrafficMetricsAdd 流量指标统计接口\nfunc TrafficMetricsAdd(db string, table string, trigger string, export string, source string, kb int) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Add(float64(kb))\n}\n\nfunc TrafficMetricsInc(db string, table string, trigger string, export string, source string) {\n\trxTrafficCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// QpsMetricsInc qps统计接口\nfunc QpsMetricsInc(db string, table string, trigger string, export string, source string) {\n\tqpsCounterVec.WithLabelValues(db, table, trigger, export, source).Inc()\n}\n\n// CostBucketMetricsObserve 处理耗时统计接口\nfunc CostBucketMetricsObserve(db string, table string, stage string, trigger string, export string, costMs float64, source string) {\n\tcostReqBucketVec.WithLabelValues(db, table, stage, trigger, export, source).Observe(costMs)\n}\n\nfunc ErrorMetricsInc(status string) {\n\terrorCounterVec.WithLabelValues(status).Inc()\n}\n\n// NameMetricsInc 表头枚举指标\nfunc NameMetricsInc(db string, table string) {\n\tnameCounterVec.WithLabelValues(db, table).Inc()\n}\n\nfunc GetExportType(export int) string {\n\tval, ok := exportTypeDict[export]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\nfunc GetTriggerType(trigger int) string {\n\tval, ok := trigTypeDict[trigger]\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn val\n}\n\n// KafkaDataRecvPackage kafka数据包接收统计\nfunc KafkaDataRecvPackage(topic string, partition int32, taskId string, packageNum int) {\n\tkafkaDataRecvPackage.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(packageNum))\n}\n\n// KafkaDataRecvFlow kafka数据流量接收统计\nfunc KafkaDataRecvFlow(topic string, partition int32, taskId string, flow int) {\n\tkafkaDataRecvFlow.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId).Add(float64(flow))\n}\n\n// KafkaRecvSkipTaskId kafka消费跳过任务统计\nfunc KafkaRecvSkipTaskId(topic string, partition int32, taskId string, errType string) {\n\tkafkaRecvSkipTaskId.WithLabelValues(topic, strconv.Itoa(int(partition)), taskId, errType).Inc()\n}\n\nfunc SetInvalidZero(db string, table string, taskId string, reason string) {\n\tcountSetInvalidZero.WithLabelValues(db, table, taskId, reason).Inc()\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/metrics/metrics.go","template":"go","multiRes":{"hipilot":"prometheus.MustRegister(countSetInvalidZero)","CodeQwen2.5-7B":"\n\tprometheus.MustRegister(countSetInvalidZero)","copilot":"prometheus.MustRegister(countSetInvalidZero)","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package server\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/dao/pg\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract_manager\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"finance_etl/app/security_code_change\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"go.uber.org/zap\"\n)\n\ntype HttpServer struct {\n\tEngine *gin.Engine\n}\n\nvar globalServer HttpServer\n\nfunc Init() {\n\tmetrics.InitMetrics()\n\tglobalServer.Engine = gin.New()\n\tlevellog.GinLog(config.GetLog().GinLogPath, globalServer.Engine)\n\t// 设置路由句柄\n\tinitRoute(globalServer.Engine)\n\tglobalServer.Engine.Run(fmt.Sprintf(\":%d\", config.GetService().HttpPort))\n}\n\n// initRoute http请求路由设置\nfunc initRoute(r *gin.Engine) {\n\tr.GET(\"/readiness\", healthCheckHandler)\n\tr.POST(\"/export\", exportHandler)\n\tr.GET(\"/ping\", pingHandler)\n\tr.GET(\"/metrics\", metrics.GetMetrics)    // prometheus指标采集接口\n\tr.GET(\"/code_change\", codeChangeHandler) // 代码变更接口\n\tr.POST(\"/debug/pushmsg\", pushMsgHandler)\n}\n\n// ping命令\nfunc pingHandler(c *gin.Context) {\n\t// http相关的返回\n\tc.JSON(200, \"pong\")\n}\n\n// 健康检查\nfunc healthCheckHandler(c *gin.Context) {\n\tlevellog.Log.Debug(\"service health check\")\n\t// todo 程序自检\n\tc.String(0, \"ok\")\n}\n\n// 手动导出数据\nfunc exportHandler(c *gin.Context) {\n\tdb := c.PostForm(pg.DATABASE)\n\ttable := c.PostForm(pg.TABLE)\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tvar msg my_orm.F10Update\n\terr := c.BindJSON(&msg)\n\tif err != nil {\n\t\tc.String(400, \"bind json failed\")\n\t\treturn\n\t}\n\n\tlevellog.Log.Info(\"push msg\", zap.Any(\"msg\", msg))\n\n\textract_manager.GlobalExtractManager.F10Kafka.DebugPushMessage(","infill":"msg)","suffix":"\n\tc.String(200, \"ok\")\n}\n","relevantFile":"<file_path>app/extract/extract_api.go\npackage extract\n\nimport my_orm \"finance_etl/app/orm\"\n\n// Extract 数据抽取接口定义\ntype Extract interface {\n\tGetMarket() string       // 获取抽取器所属市场名\n\tGetTable() string        // 获取抽取器抽取的表名\n\tGetTrigMod() int         // 获取抽取触发类型（主动 or 被动）\n\tDoExtract() (int, error) // 触发数据更新,包括手动触发与自动更新\n\tStop() (int, error)\n\n\tDebugPushMessage(my_orm.F10Update) // 调试接口\n}\n<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract_manager/extract_manager.go\npackage extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"go.uber.org/zap\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}","relevantFileList":[],"filePath":"app/server/http.go","template":"go","multiRes":{"hipilot":"msg)","CodeQwen2.5-7B":"msg)","copilot":"msg)","deepseek-chat-lite":"msg)","CodeQwen1.5-7B":"msg)"}},{"prefix":"package extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n\n\tDebugPushMessage([]byte)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n\nfunc (e AutoExtract) DebugPushMessage(","infill":"msg []byte) {","suffix":"\n\te.Update.DebugPushMessage(msg)\n}\n","relevantFile":"<file_path>app/metrics/metrics.go\npackage metrics\n\nimport (\n\t\"finance_etl/app/dao/pg\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"strconv\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n// 导出方式export,适合于pg\nvar exportTypeDict = map[int]string{\n\tpg.OpAll:   \"all\",   //全量导出\n\tpg.OpBbrq:  \"bbrq\",  //按日期导出\n\tpg.OpRtime: \"rtime\", //按rtime导出\n\tpg.OpReal:  \"real\",  //按实时更新导出\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n<file_path>app/extract/automatic/update_cron_pg.go\npackage automatic\n\nimport (\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"go.uber.org/zap\"\n)\n\n// UpdateDataCron 定时更新指定表数据\ntype UpdateDataCronPg struct {\n\tFinanceDb    string // 财务数据库\n\tFinanceTable string // 财务数据表\n}\n\nconst retryCnt int = 3 // 最多重试三次\n\nfunc (d UpdateDataCronPg) AddTasks() error {\n\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[d.FinanceDb]\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support database %s\", d.FinanceDb)\n\t}\n\ttaskTable, ok := dbInfo.TableBasic[d.FinanceTable]\n\tif !ok {\n\t\treturn fmt.Errorf(\"not support table %s\", d.FinanceTable)\n\t}\n\treqPG := manual.DataRequestPG{\n\t\tDatasource:   my_orm.RequestTcpPg,\n\t\tFinanceTable: d.FinanceTable,\n\t\tFinanceDb:    d.FinanceDb,\n\t\tTriggerType:  my_orm.UpdateByCron,\n\t}\n\ttimes := strings.Split(taskTable.Cron, \";\")\n\tfor _, t := range times {\n\t\tif t == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\ttaskName := d.FinanceDb + d.FinanceTable\n\t\t// 定时任务的回调函数\n\t\tcallback := func() {\n\t\t\tfor i := 0; i < retryCnt; i++ {\n\t\t\t\trows, err := reqPG.RequestData()\n\t\t\t\tif err == nil {\n\t\t\t\t\tlevellog.Log.Info(fmt.Sprintf(\"export data by cron successfully\"), zap.String(\"table\", reqPG.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", reqPG.FinanceDb), zap.Int(\"affects row\", rows), zap.Int(\"retry\", i))\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tif i == retryCnt-1 {\n\t\t\t\t\tlevellog.Log.Error(fmt.Sprintf(\"export data by cron failed: %s\", err.Error()), zap.String(\"table\", reqPG.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", reqPG.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t} else {\n\t\t\t\t\tlevellog.Log.Warn(fmt.Sprintf(\"export data failed: %s, start to retry\", err.Error()), zap.String(\"table\", reqPG.FinanceTable),\n\t\t\t\t\t\tzap.String(\"db\", reqPG.FinanceDb), zap.Int(\"retry\", i))\n\t\t\t\t\ttime.Sleep(time.Duration(5) * time.Second)\n<file_path>app/extract_manager/extract_manager.go\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"go.uber.org/zap\"\n\t\"strings\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n<file_path>app/extract/extract_api.go\npackage extract\n\n// Extract 数据抽取接口定义\ntype Extract interface {\n\tGetMarket() string       // 获取抽取器所属市场名\n\tGetTable() string        // 获取抽取器抽取的表名\n\tGetTrigMod() int         // 获取抽取触发类型（主动 or 被动）\n\tDoExtract() (int, error) // 触发数据更新,包括手动触发与自动更新\n\tStop() (int, error)\n\n\tDebugPushMessage([]byte) // 调试接口\n}","relevantFileList":[],"filePath":"app/extract/extract_auto.go","template":"go","multiRes":{"hipilot":"msg []byte) {","CodeQwen2.5-7B":"msg []byte) {<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"msg []byte) {","deepseek-chat-lite":"msg []byte) {","CodeQwen1.5-7B":" msg []byte) {<file_sep> /app/orm/orm.go"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype KafkaConsumer struct {\n\tConfig        kafka_dao.Config\n\tTask          KafkaTaskInfo\n\tconsumer      kafka_dao.Consumer\n\tconsumerGroup sarama.ConsumerGroup\n\tctx           context.Context\n\tcancel        context.CancelFunc\n\tchErr         chan error\n}\n\nfunc (d KafkaConsumer) Start() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\tlevellog.Log.Info(\"No need to start kafka consume\")\n\t\treturn 0, nil\n\t}\n\tlevellog.Log.Info(\"start init kafka consumer\", zap.Strings(\"brokers\", d.Config.Brokers),\n\t\tzap.String(\"username\", d.Config.Username), zap.String(\"password\", d.Config.Password),\n\t\tzap.String(\"consumer_group\", d.Config.ConsumerGroup), zap.String(\"SecurityProtocol\", d.Config.SecurityProtocol),\n\t\tzap.String(\"SaslMechanism\", d.Config.SaslMechanism), zap.String(\"topic\", d.Task.GetTopic()))\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) Stop() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\treturn 0, nil\n\t}\n\td.cancel()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) DebugPushMessage(msg []byte) {\n\tmessage := &sarama.ConsumerMessage{\n\t\tValue: msg,\n\t\tTopic: \"om_debug_push_messsage\",\n\t}\n\n\tif d.consumer.Handle","infill":" == nil { // 未初始化\n\t\treturn\n\t}","suffix":"\n\n\td.consumer.Handle(d.ctx, message)\n}\n\nfunc (d KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\td.chErr <- err\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n\tclose(d.chErr)\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka.go","template":"go","multiRes":{"hipilot":"== nil {\n        return\n    }","CodeQwen2.5-7B":" != nil {\n\t\td.consumer.Handle(d.ctx, message)\n\t}\n}","copilot":" == nil { // 未初始化\n\t\treturn\n\t}","deepseek-chat-lite":" != nil {","CodeQwen1.5-7B":"  != nil {"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype KafkaConsumer struct {\n\tConfig        kafka_dao.Config\n\tTask          KafkaTaskInfo\n\tconsumer      kafka_dao.Consumer\n\tconsumerGroup sarama.ConsumerGroup\n\tctx           context.Context\n\tcancel        context.CancelFunc\n\tchErr         chan error\n}\n\nfunc (d KafkaConsumer) Start() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\tlevellog.Log.Info(\"No need to start kafka consume\")\n\t\treturn 0, nil\n\t}\n\tlevellog.Log.Info(\"start init kafka consumer\", zap.Strings(\"brokers\", d.Config.Brokers),\n\t\tzap.String(\"username\", d.Config.Username), zap.String(\"password\", d.Config.Password),\n\t\tzap.String(\"consumer_group\", d.Config.ConsumerGroup), zap.String(\"SecurityProtocol\", d.Config.SecurityProtocol),\n\t\tzap.String(\"SaslMechanism\", d.Config.SaslMechanism), zap.String(\"topic\", d.Task.GetTopic()))\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) Stop() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\treturn 0, nil\n\t}\n\td.cancel()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) DebugPushMessage(msg []byte) {\n\tmessage := &sarama.ConsumerMessage{\n\t\tValue: msg,\n\t\tTopic: \"om_debug_push_messsage\",\n\t}\n\n\tif d.consumer.Handle == nil { ","infill":"// 未初始化\n\t\treturn\n\t}","suffix":"\n\n\td.consumer.Handle(d.ctx, message)\n}\n\nfunc (d KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\td.chErr <- err\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n\tclose(d.chErr)\n}\n","relevantFile":"<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka.go","template":"go","multiRes":{"hipilot":"return }","CodeQwen2.5-7B":"// 未配置，不需要开启\n\t\treturn\n\t}","copilot":"// 未初始化\n\t\treturn\n\t}","deepseek-chat-lite":"// 防止未初始化","CodeQwen1.5-7B":"// 测试时，直接调用"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype KafkaConsumer struct {\n\tConfig        kafka_dao.Config\n\tTask          KafkaTaskInfo\n\tconsumer      kafka_dao.Consumer\n\tconsumerGroup sarama.ConsumerGroup\n\tctx           context.Context\n\tcancel        context.CancelFunc\n\tchErr         chan error\n}\n\nfunc (d KafkaConsumer) Start() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\tlevellog.Log.Info(\"No need to start kafka consume\")\n\t\treturn 0, nil\n\t}\n\tlevellog.Log.Info(\"start init kafka consumer\", zap.Strings(\"brokers\", d.Config.Brokers),\n\t\tzap.String(\"username\", d.Config.Username), zap.String(\"password\", d.Config.Password),\n\t\tzap.String(\"consumer_group\", d.Config.ConsumerGroup), zap.String(\"SecurityProtocol\", d.Config.SecurityProtocol),\n\t\tzap.String(\"SaslMechanism\", d.Config.SaslMechanism), zap.String(\"topic\", d.Task.GetTopic()))\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) Stop() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\treturn 0, nil\n\t}\n\td.cancel()\n\treturn 0, nil\n}\n\nfunc (d KafkaConsumer) DebugPushMessage(msg []byte) {\n\tmessage := &sarama.ConsumerMessage{\n\t\tValue: msg,\n\t\tTopic: \"om_debug_push_messsage\",\n\t}\n\n\tif d.consumer.Handle == nil { // 未初始化\n\t\tlevellog","infill":".Log.Info(\"kafka consumer handle is nil\", ","suffix":"\n\t\treturn\n\t}\n\n\td.consumer.Handle(d.ctx, message)\n}\n\nfunc (d KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\td.chErr <- err\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n\tclose(d.chErr)\n}\n","relevantFile":"<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka.go","template":"go","multiRes":{"hipilot":".Log.Info(\"kafka consume handler is nil\")","CodeQwen2.5-7B":".Log.Info(\"kafka consume not init\")\n\t\treturn\n\t}","copilot":".Log.Info(\"kafka consumer handle is nil\", ","deepseek-chat-lite":".Log.Info(\"kafka consumer not init\")","CodeQwen1.5-7B":" .Log.Info(\"kafka consumer not init\")"}},{"prefix":"package extract_manager\n\nimport (\n\t\"finance_etl/app/config\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\t\"finance_etl/app/extract\"\n\t\"finance_etl/app/extract/automatic\"\n\t\"finance_etl/app/extract/manual\"\n\t\"finance_etl/app/finance_info\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"go.uber.org/zap\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\tGlobalExtractManager.F10Kafka.DebugPushMessage(","infill":"[]byte(\"{}\"))","suffix":"\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费\n}\n\n// GetManualExtract 获取手动导出接口对象\nfunc GetManualExtract(dbName string, tableName string, http extract.ReqParam) extract.Extract {\n\tdb, ok := finance_info.GlobalFinanceInfoManager[dbName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tbasic, ok := db.TableBasic[tableName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif basic.Datasource == \"PG\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestPG{\n\t\t\t\tDatasource:   my_orm.RequestTcpPg,\n\t\t\t\tFinanceTable: tableName,\n\t\t\t\tFinanceDb:    dbName,\n\t\t\t\tTriggerType:  my_orm.UpdateByManual,\n\t\t\t\tReqParam:     http,\n\t\t\t},\n\t\t}\n\t} else if basic.Datasource == \"F10\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestF10{\n\t\t\t\tDatasource:   my_orm.RequestHttpF10,\n\t\t\t\tUrl:          config.GetF10().Url,\n\t\t\t\tFinanceTable: tableName,\n\t\t\t\tFinanceDb:    dbName,\n\t\t\t\tTriggerType:  my_orm.UpdateByManual,\n\t\t\t\tReqParam:     http,\n\t\t\t},\n\t\t}\n\t} else if basic.Datasource == \"FUsEtfAdjFactor\" {\n\t\treturn extract.ManualExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tRequest: manual.DataRequestFUsEtfAdjFactor{\n\t\t\t\tDatasource:   my_orm.RequestHTTPFUsEtfAdjFactor,\n\t\t\t\tUrl:          config.GetFUsEtfAdjFactor().Url,\n\t\t\t\tFinanceTable: tableName,\n\t\t\t\tFinanceDb:    dbName,\n\t\t\t\tTriggerType:  my_orm.UpdateByManual,\n\t\t\t\tReqParam:     http,\n\t\t\t},\n\t\t}\n\t} else {\n\t\tlevellog.Log.Error(\"GetManualExtract failed\", zap.String(\"datasource\", basic.Datasource))\n\t}\n\treturn nil\n}\n\n// GetCronExtract 获取自动导出对象接口\nfunc GetCronExtract(dbName string, tableName string) extract.Extract {\n\tdb, ok := finance_info.GlobalFinanceInfoManager[dbName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tbasic, ok := db.TableBasic[tableName]\n\tif !ok {\n\t\treturn nil\n\t}\n\tif basic.Datasource == \"PG\" {\n\t\treturn extract.AutoExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tUpdate: automatic.UpdateDataCronPg{\n\t\t\t\tFinanceDb:    dbName,\n\t\t\t\tFinanceTable: tableName,\n\t\t\t}}\n\t} else if basic.Datasource == \"FUsEtfAdjFactor\" {\n\t\treturn extract.AutoExtract{\n\t\t\tMarketName: dbName,\n\t\t\tTableName:  tableName,\n\t\t\tUpdate: automatic.UpdateDataCronFUsEtfAdjFactor{\n\t\t\t\tFinanceDb:    dbName,\n\t\t\t\tFinanceTable: tableName,\n\t\t\t}}\n\t}\n\treturn nil\n}\n\n// GetF10KafkaExtract 获取自动导出对象接口\nfunc GetF10KafkaExtract() extract.Extract {\n\t// kafka连接配置\n\tcfg := kafka_dao.Config{\n\t\tBrokers:          strings.Split(config.GetKafka().Brokers, \",\"),\n\t\tUsername:         config.GetKafka().Username,\n\t\tPassword:         config.GetKafka().Password,\n\t\tConsumerGroup:    config.GetKafka().ConsumerGroup,\n\t\tSecurityProtocol: config.GetKafka().SecurityProtocol,\n\t\tSaslMechanism:    config.GetKafka().SaslMechanism,\n\t}\n\t// 任务配置\n\tF10Task := automatic.F10KafkaTaskInfo{\n\t\tTopic:              config.GetF10().Topic,\n\t\tUrl:                config.GetF10().Url,\n\t\tUnderlyingAssetUrl: config.GetF10().UnderlyingAssetUrl,\n\t}\n\tF10Task.MapTaskProcess = make(map[string]automatic.F10KafkaTask)\n\tF10Task.MapTaskIdIndicators = make(map[string]string)\n\tfor database, detail := range finance_info.GlobalFinanceInfoManager {\n\t\tfor k, v := range detail.TableBasic {\n\t\t\tlevellog.Log.Info(\"Init f10 kafka task\", zap.String(\"database\", database),\n\t\t\t\tzap.String(\"task\", v.TaskIds), zap.String(\"table\", k))\n\n\t\t\tmarket := detail.TableBasic[k].Market\n\t\t\ttaskIds := strings.Split(v.TaskIds, \",\") // taskids 有多个 taskid，每个 taskid 对应多个指标\n\t\t\tfor _, taskId := range taskIds {\n\t\t\t\tif taskId == \"\" {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\ttaskIndicators, ok := detail.MapTaskIndicator[taskId]\n\t\t\t\tif ok == false {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tmapOtherName := make(map[string]string)\n\t\t\t\tidList := strings.Split(taskIndicators.Ids, \",\")\n\t\t\t\tmapTmp := detail.TableFieldName\n\t\t\t\tfor _, val := range idList {\n\t\t\t\t\tdestName, ok := mapTmp[val]\n\t\t\t\t\tif ok == false {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tmapOtherName[val] = destName.FieldName // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\t\t\t\t\tlevellog.Log.Info(\"Init F10 kafka field transfer\", zap.String(\"taskid\", v.TaskIds),\n\t\t\t\t\t\tzap.String(\"src_field\", val), zap.String(\"dest_field\", destName.FieldName))\n\t\t\t\t}\n\n\t\t\t\tF10Task.MapTaskProcess[taskId+\"_\"+market] = automatic.F10KafkaTask{\n\t\t\t\t\tTaskName:    taskId,\n\t\t\t\t\tDbName:      database,\n\t\t\t\t\tTableName:   k,\n\t\t\t\t\tCurrency:    detail.TableBasic[k].Currency,\n\t\t\t\t\tMarketType:  detail.TableBasic[k].MarketType,\n\t\t\t\t\tMapItemName: mapOtherName,\n\t\t\t\t\tDate:        taskIndicators.DateType,\n\t\t\t\t\tHadReport:   taskIndicators.HadReport,\n\t\t\t\t}\n\n\t\t\t\t// 对于多个市场使用同一个 taskid的情况下，这里只需要取第一个市场的 taskid，因为同一个 taskid 对应的指标是相同的；\n\t\t\t\tif _, ok := F10Task.MapTaskIdIndicators[taskId]; !ok {\n\t\t\t\t\tF10Task.MapTaskIdIndicators[taskId] = taskIndicators.Ids\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif len(F10Task.MapTaskProcess) == 0 {\n\t\tF10Task.Topic = \"\"\n\t}\n\t// 生成消费者\n\treturn extract.AutoExtract{\n\t\tUpdate: automatic.KafkaConsumer{\n\t\t\tConfig: cfg,\n\t\t\tTask:   F10Task,\n\t\t}}\n}\n","relevantFile":"<file_path>app/extract/extract_api.go\npackage extract\n\n// Extract 数据抽取接口定义\ntype Extract interface {\n\tGetMarket() string       // 获取抽取器所属市场名\n\tGetTable() string        // 获取抽取器抽取的表名\n\tGetTrigMod() int         // 获取抽取触发类型（主动 or 被动）\n\tDoExtract() (int, error) // 触发数据更新,包括手动触发与自动更新\n\tStop() (int, error)\n\n\tDebugPushMessage([]byte) // 调试接口\n}\n<file_path>app/extract/extract_auto.go\npackage extract\n\nimport (\n\tmy_orm \"finance_etl/app/orm\"\n)\n\ntype DataUpdate interface {\n\tStart() (int, error)\n\tStop() (int, error)\n\n\tDebugPushMessage([]byte)\n}\n\ntype AutoExtract struct {\n\tExtract\n\tMarketName string\n\tTableName  string\n\tUpdate     DataUpdate\n}\n\nfunc (e AutoExtract) GetTrigMod() int {\n\treturn my_orm.UpdateByCron // 后续确定是kafka还是cron\n}\n\nfunc (e AutoExtract) GetMarket() string {\n\treturn e.MarketName\n}\n\nfunc (e AutoExtract) GetTable() string {\n\treturn e.TableName\n}\n\nfunc (e AutoExtract) DoExtract() (int, error) {\n\treturn e.Update.Start()\n}\n\nfunc (e AutoExtract) Stop() (int, error) {\n\treturn e.Update.Stop()\n}\n\nfunc (e AutoExtract) DebugPushMessage(msg []byte) {\n\te.Update.DebugPushMessage(msg)\n}\n<file_path>app/extract/automatic/update_kafka.go\npackage automatic\n\nimport (\n\t\"context\"\n\tkafka_dao \"finance_etl/app/dao/kafka\"\n\tlevellog \"finance_etl/app/log\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n)\n\ntype KafkaTaskInfo interface {\n\tGetTopic() string\n\tConsume(ctx context.Context, message *sarama.ConsumerMessage)\n}\n\ntype KafkaConsumer struct {\n\tConfig        kafka_dao.Config\n\tTask          KafkaTaskInfo\n\tconsumer      kafka_dao.Consumer\n\tconsumerGroup sarama.ConsumerGroup\n\tctx           context.Context\n\tcancel        context.CancelFunc\n\tchErr         chan error\n}\n\nfunc (d KafkaConsumer) Start() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\tlevellog.Log.Info(\"No need to start kafka consume\")\n\t\treturn 0, nil\n\t}\n\tlevellog.Log.Info(\"start init kafka consumer\", zap.Strings(\"brokers\", d.Config.Brokers),\n\t\tzap.String(\"username\", d.Config.Username), zap.String(\"password\", d.Config.Password),\n\t\tzap.String(\"consumer_group\", d.Config.ConsumerGroup), zap.String(\"SecurityProtocol\", d.Config.SecurityProtocol),\n\t\tzap.String(\"SaslMechanism\", d.Config.SaslMechanism), zap.String(\"topic\", d.Task.GetTopic()))\n\td.ctx, d.cancel = context.WithCancel(context.Background())\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n<file_path>app/server/http.go\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tmsg, _ := c.GetRawData()\n\tlevellog.Log.Info(\"push msg\", zap.ByteString(\"request\", msg))\n\n\tif extract_manager.GlobalExtractManager.F10Kafka == nil {\n\t\tc.String(400, \"f10 kafka not init\")\n\t\treturn\n\t}\n\n\textract_manager.GlobalExtractManager.F10Kafka.DebugPushMessage(msg)\n\tc.String(200, \"ok\")","relevantFileList":[],"filePath":"app/extract_manager/extract_manager.go","template":"go","multiRes":{"hipilot":"\"test\")","CodeQwen2.5-7B":")\n\tlevellog.Log.Info(\"f10 kafka consumer start\")<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"[]byte(\"{}\"))","deepseek-chat-lite":"\"f10 kafka init success\")","CodeQwen1.5-7B":" )"}},{"prefix":"package automatic\n\nimport (\n\t\"conte","infill":"return nil, fmt.Errorf(\"failed to get response after %d retries\", retryCount)","suffix":"\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"<file_path>app/dao/kafka/consumer_cfg.go\n\tSaslMechanism    string   `yaml:\"sasl_mechanism\" json:\"sasl_mechanism\"`\n}\n\nfunc (k *Config) SaramaConfig() (*sarama.Config, error) {\n\tsaramaCfg := sarama.NewConfig()\n\tif k.SecurityProtocol == \"sasl_plaintext\" {\n\t\tsaramaCfg.Net.SASL.Enable = true\n\t\tsaramaCfg.Net.SASL.User = k.Username\n\t\tsaramaCfg.Net.SASL.Password = k.Password\n\t\tsaramaCfg.Net.SASL.Mechanism = sarama.SASLMechanism(k.SaslMechanism)\n\t\tif sarama.SASLMechanism(k.SaslMechanism) == sarama.SASLTypeSCRAMSHA512 {\n\t\t\tsaramaCfg.Net.SASL.SCRAMClientGeneratorFunc = func() sarama.SCRAMClient { return &XDGSCRAMClient{HashGeneratorFcn: SHA512} }\n\t\t\tsaramaCfg.Net.SASL.Mechanism = sarama.SASLTypeSCRAMSHA512\n\t\t} else if sarama.SASLMechanism(k.SaslMechanism) == sarama.SASLTypeSCRAMSHA256 {\n\t\t\tsaramaCfg.Net.SASL.SCRAMClientGeneratorFunc = func() sarama.SCRAMClient { return &XDGSCRAMClient{HashGeneratorFcn: SHA256} }\n\t\t\tsaramaCfg.Net.SASL.Mechanism = sarama.SASLTypeSCRAMSHA256\n\t\t} else {\n\t\t\treturn nil, fmt.Errorf(\"invalid SHA algorithm %s: can be either %s or %s\",\n\t\t\t\tk.SaslMechanism,\n\t\t\t\tsarama.SASLTypeSCRAMSHA256,\n\t\t\t\tsarama.SASLTypeSCRAMSHA512)\n\t\t}\n\t}\n\tsaramaCfg.Consumer.Offsets.Initial = sarama.OffsetNewest\n\tsaramaCfg.Consumer.Offsets.AutoCommit.Enable = false\n\treturn saramaCfg, nil\n}\n\nfunc (k *Config) SaramaClient() (sarama.Client, error) {\n\tcfg, err := k.SaramaConfig()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn sarama.NewClient(k.Brokers, cfg)\n}\n\nvar (\n\tSHA256 scram.HashGeneratorFcn = sha256.New\n\tSHA512 scram.HashGeneratorFcn = sha512.New\n)\n\ntype XDGSCRAMClient struct {\n\t*scram.Client\n\t*scram.ClientConversation\n\tscram.HashGeneratorFcn\n}\n\nfunc (x *XDGSCRAMClient) Begin(userName, password, authzID string) (err error) {\n\tx.Client, err = x.HashGeneratorFcn.NewClient(userName, password, authzID)\n\tif err != nil {\n\t\treturn err\n\t}\n\tx.ClientConversation = x.Client.NewConversation()\n\treturn nil\n}\n\nfunc (x *XDGSCRAMClient) Step(challenge string) (response string, err error) {\n\tresponse, err = x.ClientConversation.Step(challenge)\n\treturn\n}\n<file_path>app/extract/automatic/update_kafka.go\n\tclient, err := d.Config.SaramaClient()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.consumerGroup, err = sarama.NewConsumerGroupFromClient(d.Config.ConsumerGroup, client)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\td.consumer = kafka_dao.Consumer{\n\t\tReady:  make(chan bool),\n\t\tHandle: d.Task.Consume,\n\t}\n\n\td.chErr = make(chan error)\n\n\tgo d.Run()\n\n\t<-d.consumer.Ready\n\tlevellog.Log.Info(\"kafka subscribe succeed\")\n\tgo d.Wait()\n\treturn 0, nil\n}\n\nfunc (d *KafkaConsumer) Stop() (int, error) {\n\tif d.Config.Brokers == nil || d.Task.GetTopic() == \"\" {\n\t\t// 未配置，不需要开启\n\t\treturn 0, nil\n\t}\n\td.cancel()\n\treturn 0, nil\n}\n\nfunc (d *KafkaConsumer) Run() {\n\tfor {\n\t\ttopics := make([]string, 0)\n\t\tif err := d.consumerGroup.Consume(d.ctx, append(topics, d.Task.GetTopic()), &d.consumer); err != nil {\n\t\t\tlevellog.Log.Info(\"kafka consume terminate\", zap.Error(err), zap.String(\"topic\", d.Task.GetTopic()))\n\t\t\tclose(d.chErr)\n\t\t\treturn\n\t\t}\n\t\td.consumer.Ready = make(chan bool)\n\t}\n}\n\nfunc (d *KafkaConsumer) Wait() {\n\tfor {\n\t\tselect {\n\t\tcase <-d.ctx.Done():\n\t\t\tlevellog.Log.Info(\"kafka consume done\")\n\t\t\td.consumerGroup.Close()\n\t\t\tclose(d.chErr)\n\t\tcase err := <-d.chErr:\n\t\t\tlevellog.Log.Error(\"kafka consume err\", zap.Error(err))\n\t\t}\n\t}\n}\n\nfunc (d *KafkaConsumer) DebugPushMessage(msg []byte) {\n\tmessage := &sarama.ConsumerMessage{\n<file_path>app/server/http.go\n\thttp := getHttpParams(c)\n\textract := extract_manager.GetManualExtract(db, table, http)\n\tif extract == nil {\n\t\tc.String(400, \"Not support finance table, check the table in mysql config_table\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\t\treturn\n\t}\n\trows, err := extract.DoExtract()\n\tif err == nil {\n\t\tc.String(0, fmt.Sprintf(\"affected rows: %d\", rows))\n\t\treturn\n\t}\n\terr = fmt.Errorf(\"%s, export manual %s %s\", err.Error(), db, table)\n\tmetrics.ErrorMetricsInc(metrics.ErrClientRequestInvalid)\n\tc.String(400, err.Error())\n\treturn\n}\n\nfunc getHttpParams(c *gin.Context) (hp extract.ReqParam) {\n\tif hp.StartTime, _ = strconv.Atoi(c.PostForm(pg.STARTDATE)); hp.StartTime == 0 {\n\t\ty, m, d := time.Now().AddDate(0, 0, -1).Date()\n\t\thp.StartTime = y*10000 + 100*int(m) + d\n\t}\n\tif hp.EndTime, _ = strconv.Atoi(c.PostForm(pg.ENDDATE)); hp.EndTime == 0 {\n\t\ty, m, d := time.Now().Date()\n\t\thp.EndTime = y*10000 + 100*int(m) + d\n\t}\n\tif c.PostForm(pg.TYPE) == \"\" {\n\t\thp.ExportType = pg.OpRtime\n\t} else {\n\t\thp.ExportType, _ = strconv.Atoi(c.PostForm(pg.TYPE))\n\t}\n\thp.CodeList = c.PostForm(pg.CODELIST)\n\n\tif c.PostForm(\"strategy\") == \"\" {\n\t\thp.Strategy = \"period\"\n\t} else {\n\t\thp.Strategy = c.PostForm(\"strategy\")\n\t}\n\n\treturn\n}\n\n// 代码变更接口\nfunc codeChangeHandler(c *gin.Context) {\n\thttpcode, msg := security_code_change.CodeChangeHTTPAPI(c)\n\tc.String(httpcode, msg)\n}\n\n// 推送消息\nfunc pushMsgHandler(c *gin.Context) {\n\t// todo 推送消息\n\tmsg, _ := c.GetRawData()\n\tlevellog.Log.Info(\"push msg\", zap.ByteString(\"request\", msg))\n\n\tif extract_manager.GlobalExtractManager.F10Kafka == nil {\n\t\tc.String(400, \"f10 kafka not init\")\n\t\treturn\n\t}\n<file_path>app/extract_manager/extract_manager.go\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"go.uber.org/zap\"\n)\n\ntype ExtractManager struct {\n\tMapCron  map[string]extract.Extract\n\tF10Kafka extract.Extract\n}\n\nvar GlobalExtractManager ExtractManager\n\nfunc StartAutoExtract(sinkDb []string) error {\n\tGlobalExtractManager.MapCron = make(map[string]extract.Extract)\n\tcronTaskNum := 0\n\tfor _, db := range sinkDb {\n\t\tdbInfo, ok := finance_info.GlobalFinanceInfoManager[db]\n\t\tif !ok {\n\t\t\tlevellog.Log.Warn(\"not support database sink\", zap.String(\"db\", db))\n\t\t\tcontinue\n\t\t}\n\t\tfor table, _ := range dbInfo.TableBasic {\n\t\t\tcronExtract := GetCronExtract(db, table)\n\t\t\tif cronExtract != nil {\n\t\t\t\t_, err := cronExtract.DoExtract()\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = fmt.Errorf(\"%s,  crontask\", err.Error())\n\t\t\t\t\tmetrics.ErrorMetricsInc(metrics.ErrCronTaskExecFailed)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tkey := db + \"_\" + table\n\t\t\t\tGlobalExtractManager.MapCron[key] = cronExtract\n\t\t\t\tcronTaskNum++\n\t\t\t}\n\t\t}\n\t}\n\tlevellog.Log.Info(\"cron task inited\", zap.Int(\"task_cnt\", cronTaskNum))\n\t// 从f10 kafka订阅数据\n\tGlobalExtractManager.F10Kafka = GetF10KafkaExtract()\n\t_, err := GlobalExtractManager.F10Kafka.DoExtract()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%s, f10 kafka extract\", err.Error())\n\t\tlevellog.Log.Error(\"start f10 kafka consumer failed\", zap.Error(err))\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc StopAutoExtract() {\n\t// 关闭定时任务\n\tfor key, val := range GlobalExtractManager.MapCron {\n\t\tval.Stop()\n\t\tlevellog.Log.Info(\" cron stop\", zap.String(\"task\", key))\n\t}\n\t// 关闭f10 kafka消费\n\tGlobalExtractManager.F10Kafka.Stop()\n\t// 关闭基金 kafka消费","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"time\"\n    \"f10_service/config\"\n    \"f10_service/pkg/err\"\n    \"f10_service/pkg/log\"\n    \"f10_service/pkg/metrics\"\n    \"github.com/Shopify/s","CodeQwen2.5-7B":"xt\"\n\t\"fmt\"\n\t\"github.com/zeromicro/go-zero/core/logx\"\n\t\"github.com/zeromicro/go-zero/core/metric\"\n\t\"github.com/zeromicro/go-zero/core/metric/metrics\"\n\t\"github.com/zeromicro/go-zero/core/stores/sqlx\"\n\t\"github.com/zeromicro/go-zero/core/timex\"\n\t\"github.com/zeromicro/go-zero/core/trace\"\n\t\"github.com/zeromicro/go-zero/core/trace/traceutil\"\n\t\"github.com/zeromicro/go-zero/core/zapx\"\n\t\"github.com/zeromicro/go-zero/zrpc\"\n\t\"go.opentelemetry.io/otel/attribute\"\n\t\"go.opentelemetry.io/otel/trace\"\n\t\"go.uber.org/zap\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n\t\"google.golang.org/grpc/metadata\"\n\t\"google.golang.org/protobuf/types/known/timestamppb\"\n\t\"reflect\"\n\t\"regexp\"\n\t\"strings\"\n\t\"time\"\n)","copilot":"return nil, fmt.Errorf(\"failed to get response after %d retries\", retryCount)","deepseek-chat-lite":"xt\"","CodeQwen1.5-7B":" xt\""}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(indicators, subjects, \"\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tlevellog","infill":".Log.Info(\"kafka push message\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects),\n\t\tzap.String(\"indicators\", indicatorIDs), zap.String(\"currency\", currency),\n\t\tzap.String(\"date\", recvDate), zap.Int(\"valid\", validInt))","suffix":"\n\n\tif validInt == 0 {\n\t\td.SetIsValidZeroToMysql(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetIsValidZero(taskInfo.TableName, jsonF10.TaskId, \"RecvZero\")\n\t\treturn\n\n\t}\n\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIDs, subjects, currency, recvDate)\n\tif err != nil {\n\t\td.SetIsValidZeroToMysql(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetIsValidZero(taskInfo.TableName, jsonF10.TaskId, \"RequestEmpty\")\n\t\treturn\n\t}\n\n\td.UpdateUnderlyingAssetData(thsMarketCode.Code, 1, recvDate, jsonF10Rsp, taskInfo)\n}\n\nfunc (d F10KafkaTaskInfo) SetIsValidZeroToMysql(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，使用newest策略。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\n\tif resp != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries\", retryCount)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":".Log.Info(\"kafka push message valid field\", zap.String(\"valid\", strconv.Itoa(validInt)), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))","copilot":".Log.Info(\"kafka push message\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects),\n\t\tzap.String(\"indicators\", indicatorIDs), zap.String(\"currency\", currency),\n\t\tzap.String(\"date\", recvDate), zap.Int(\"valid\", validInt))","deepseek-chat-lite":".Log.Info(\"kafka push message valid\", zap.Int(\"valid\", validInt), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))","CodeQwen1.5-7B":" .Log.Info(\"kafka message\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"subjects\", subjects), zap.String(\"indicatorIDs\", indicatorIDs),"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(indicators, subjects, \"\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tlevellog.Log.Info(\"kafka push message\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects),\n\t\tzap.String(\"indicators\", indicatorIDs), ","infill":"zap.String(\"currency\", currency),\n\t\tzap.String(\"date\", recvDate), zap.Int(\"valid\", validInt))","suffix":"\n\n\tif validInt == 0 {\n\t\td.SetIsValidZeroToMysql(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetIsValidZero(taskInfo.TableName, jsonF10.TaskId, \"RecvZero\")\n\t\treturn\n\t}\n\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIDs, subjects, currency, recvDate)\n\tif err != nil {\n\t\td.SetIsValidZeroToMysql(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetIsValidZero(taskInfo.TableName, jsonF10.TaskId, \"RequestEmpty\")\n\t\treturn\n\t}\n\n\td.UpdateUnderlyingAssetData(thsMarketCode.Code, 1, recvDate, jsonF10Rsp, taskInfo)\n}\n\nfunc (d F10KafkaTaskInfo) SetIsValidZeroToMysql(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，使用newest策略。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\n\tif resp != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries\", retryCount)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"zap.String(\"currency\", currency), zap.String(\"date\", recvDate), zap.Int(\"valid\", validInt))<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"zap.String(\"currency\", currency),\n\t\tzap.String(\"date\", recvDate), zap.Int(\"valid\", validInt))","deepseek-chat-lite":"zap.String(\"currency\", currency), zap.String(\"date\", recvDate), zap.Int(\"valid\", validInt))","CodeQwen1.5-7B":" zap.String(\"date\", recvDate), zap.String(\"currency\", currency), zap.Int(\"valid\", validInt))"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(indicators, subjects, \"\", \"\")\n\tif err != nil {\n\t\tlevellog.Log.Info(\"request kafka push message failed\", zap.String(\"task\", f10TaskId), zap.String(\"subjects\", subjects))\n\t\treturn ThsMarketCode{}, err\n\t}\n\n\t// get market and code from response\n\tmarket := common.SubMarket2Market(jsonF10Rsp.Data.Data[0].Subject.Market)\n\tthsCode := common.TransCodeToThsCode(jsonF10Rsp.Data.Data[0].Subject.Code, market)\n\treturn ThsMarketCode{market, thsCode}, nil\n}\n\nfunc (d F10KafkaTaskInfo) Consume(ctx context.Context, message *sarama.ConsumerMessage) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr := fmt.Sprintf(\"%s\", r)\n\t\t\tlevellog.Log.Error(\"Consume failed\", zap.String(\"msg\", err))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaConsumeError)\n\t\t}\n\t}()\n\n\tvar jsonF10 my_orm.F10Update\n\terr := json.Unmarshal(message.Value, &jsonF10)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"kafka message unmarshal failed, err: %s, data: %s\", err.Error(), string(message.Value))\n\t\tlevellog.Log.Info(\"Kafka message unmarshal failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgParseFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Info(\"receive kafka message\", zap.String(\"data\", string(message.Value)))\n\n\tmetrics.KafkaDataRecvPackage(message.Topic, message.Partition, jsonF10.TaskId, 1)\n\tmetrics.KafkaDataRecvFlow(message.Topic, message.Partition, jsonF10.TaskId, len(message.Value))\n\n\t// 接收到的kafka消息的taskid是否是extractF10Task表中配置的\n\tisFindTaskId := false\n\tfor _, v := range d.MapTaskProcess {\n\t\tif v.TaskName == jsonF10.TaskId {\n\t\t\tisFindTaskId = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif isFindTaskId == false { // 不需要处理\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskIdFromConfig\")\n\t\treturn\n\t}\n\n\tindicatorIDs, ok := d.MapTaskIdIndicators[jsonF10.TaskId]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskid kafka message not configured in local database\", zap.String(\"taskid\", jsonF10.TaskId))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundIndicatorIDFromConfig\")\n\t\treturn\n\t}\n\n\t// 获取机构id 和 证券id, 至少有一个\n\tsubjects := \"\"\n\tif orgId, ok := jsonF10.Data[\"org_id\"]; ok && orgId != \"\" {\n\t\tsubjects = orgId\n\t} else if secId, ok := jsonF10.Data[\"sec_id\"]; ok && secId != \"\" {\n\t\tsubjects = secId\n\t} else {\n\t\tlevellog.Log.Info(\"orgId and secId not found in kafka msg\")\n\t\tmetrics.ErrorMetricsInc(metrics.ErrKafkaMsgMissingField)\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundSubjectFromRecvMsg\")\n\t\treturn\n\t}\n\n\tthsMarketCode, err := d.GetMarketCodeFromQueryF10(jsonF10.TaskId, subjects, indicatorIDs)\n\tif err != nil {\n\t\tlevellog.Log.Info(\"get market code from query f10 failed\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundThsCodeFromF10\")\n\t\treturn\n\t}\n\n\ttaskInfo, ok := d.MapTaskProcess[jsonF10.TaskId+\"_\"+thsMarketCode.Market]\n\tif !ok {\n\t\tlevellog.Log.Info(\"taskinfo not found in by taskid and market\", zap.String(\"taskid\", jsonF10.TaskId), zap.String(\"market\", thsMarketCode.Market))\n\t\tmetrics.KafkaRecvSkipTaskId(message.Topic, message.Partition, jsonF10.TaskId, \"NotFoundTaskInfoFromConfig\")\n\t\treturn\n\t}\n\n\trecvDate := jsonF10.Data[taskInfo.Date]\n\tcurrency := taskInfo.Currency\n\n\t// 是否是置否消息\n\tvalidInt := 1\n\tif valid, ok := jsonF10.Data[\"valid\"]; ok && valid != \"\" {\n\t\tvalidInt, err = strconv.Atoi(valid)\n\t\tif err != nil {\n\t\t\tlevellog.Log.Info(\"kafka push message valid field not support\", zap.String(\"valid\", valid), zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t\t}\n\t} else {\n\t\tlevellog.Log.Info(\"valid not found in kafka push msg\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects))\n\t}\n\n\tlevellog.Log.Info(\"kafka push message\", zap.String(\"task\", jsonF10.TaskId), zap.String(\"subjects\", subjects),\n\t\tzap.String(\"indicators\", indicatorIDs), zap.String(\"currency\", currency), zap.String(\"date_field\", ","infill":"taskInfo.Date),","suffix":"\n\t\tzap.String(\"date\", recvDate), zap.Int(\"valid\", validInt))\n\n\tif validInt == 0 {\n\t\td.SetIsValidZeroToMysql(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetIsValidZero(taskInfo.TableName, jsonF10.TaskId, \"RecvZero\")\n\t\treturn\n\t}\n\n\tjsonF10Rsp, err := d.RequestF10Data(indicatorIDs, subjects, currency, recvDate)\n\tif err != nil {\n\t\td.SetIsValidZeroToMysql(thsMarketCode.Code, recvDate, taskInfo)\n\t\tmetrics.SetIsValidZero(taskInfo.TableName, jsonF10.TaskId, \"RequestEmpty\")\n\t\treturn\n\t}\n\n\td.UpdateUnderlyingAssetData(thsMarketCode.Code, 1, recvDate, jsonF10Rsp, taskInfo)\n}\n\nfunc (d F10KafkaTaskInfo) SetIsValidZeroToMysql(codeStr string, dateStr string, taskInfo F10KafkaTask) {\n\t// 此条数据valid为0,把数据库中这条消息的对应的本地mysql的这条数据(zqdm+unixtime)的isvalid字段置为0\n\ttimeZone := finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone\n\tunixTime := common.ConvertDateToUnixTime(dateStr, timeZone)\n\tupdateIsValidSql := fmt.Sprintf(\"UPDATE `%s` SET isvalid = 0 WHERE zqdm = %s AND unix_time = %d\", taskInfo.TableName, codeStr, unixTime)\n\tif _, err := load.SinkData(taskInfo.DbName, updateIsValidSql); err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Warn(\"kafka sink mysql failed\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime), zap.Error(err), zap.String(\"sql\", updateIsValidSql))\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，使用newest策略。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\n\tif resp != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries\", retryCount)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"CodeQwen2.5-7B":"taskInfo.Date),","copilot":"taskInfo.Date),","deepseek-chat-lite":"taskInfo.Date),","CodeQwen1.5-7B":" taskInfo.Date),"}},{"prefix":"package automatic\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"finance_etl/app/common\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/metrics\"\n\tmy_orm \"finance_etl/app/orm\"\n\t\"finance_etl/app/transform\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/google/go-querystring/query\"\n\t\"go.uber.org/zap\"\n)\n\n// F10KafkaTask 一个taskid可能对应多个数据库\ntype F10KafkaTask struct {\n\tTaskName    string\n\tDbName      string\n\tTableName   string\n\tCurrency    string\n\tMarketType  string\n\tMapItemName map[string]string // key是f10中提供的字段,val是本地sql库中定义的字段,一个映射关系\n\tDate        string            // 每个taskid对应一个日期字段\n\tHadReport   int               // 是否需要report字段\n}\n\ntype F10KafkaTaskInfo struct {\n\tTopic               string\n\tUrl                 string\n\tUnderlyingAssetUrl  string\n\tMapTaskIdIndicators map[string]string       // 一个topic存在多个task,一个task可以对应多个市场，key是taskid，保存taskid与指标关系\n\tMapTaskProcess      map[string]F10KafkaTask // 一个topic存在多个task,一个task对应多个市场，key是taskid+市场\n}\n\nfunc (d F10KafkaTaskInfo) GetTopic() string {\n\treturn d.Topic\n}\n\nfunc GetValid(valid interface{}) (int, error) {\n\tif result, ok := valid.(float64); ok {\n\t\treturn int(result), nil\n\t} else if result, ok := valid.(string); ok {\n\t\treturn strconv.Atoi(result)\n\t}\n\treturn 0, errors.New(\"get valid trans failed\")\n}\n\ntype ThsMarketCode struct {\n\tMarket string\n\tCode   string\n}\n\nfunc (d F10KafkaTaskInfo) GetMarketCodeFromQueryF10(f10TaskId string, subjects string, indicators string) (ThsMarketCode, error) {\n\t// 先根据 subjects 查询一下推送指标的市场\n\tjsonF10Rsp, err := d.RequestF10Data(indicators, subjects, \"\", \"\")\n\tif err != nil {\n\t\tleve","infill":"zap.Int64(\"affect_count\", affect_count))","suffix":"\n\t\treturn err\n\t} else {\n\t\tlevellog.Log.Info(\"update isvalid=0 in local mysql success\", zap.String(\"table\", taskInfo.TableName), zap.String(\"date\", dateStr), zap.String(\"code\", codeStr), zap.String(\"time_zone\", timeZone),\n\t\t\tzap.Int64(\"unixtime\", unixTime))\n\t\treturn nil\n\t}\n}\n\n// RequestF10Data 向指定的URL发送GET请求，并提供参数。\n// 如果失败，它会尝试重新请求最多3次。\n// 该函数接收四个参数：indicatorIDs，subjects，currency和date。\n// 如果date不是空字符串，请求策略将设置为\"period\"，并将date用作请求的\"From\"和\"To\"字段。\n// 该函数返回一个F10Responses对象和一个错误。\n// 如果响应的状态码不是0或响应的数据字段为空，函数将记录一条信息消息并返回错误。\n//\n// 参数：\n// - indicatorIDs：指标ID的字符串。可以包含的ID数量没有限制。\n// - subjects：主题的字符串。可以包含的主题数量没有限制。\n// - currency：表示货币的字符串。这应该是一个有效的货币代码（例如，\"USD\"，\"EUR\"）。\n// - date：表示日期的字符串。如果提供，应该是\"YYYY-MM-DD\"的格式。如果未提供或为空字符串，使用newest策略。\n//\n// 返回：\n// - F10Responses：包含F10 API响应数据的结构体。这包括状态码，状态消息和数据。\n// - error：错误对象。如果请求成功，这是nil，否则包含错误消息。\n//\n// 示例用法：\n// indicatorIDs := \"123,456,789\"\n// subjects := \"subject1,subject2,subject3\"\n// currency := \"USD\"\n// date := \"2022-01-01\"\n// response, err := d.RequestF10Data(indicatorIDs, subjects, currency, date)\n//\n//\tif err != nil {\n//\t    log.Fatalf(\"请求失败: %v\", err)\n//\t}\n//\n// fmt.Printf(\"响应: %v\", response)\nfunc (d F10KafkaTaskInfo) RequestF10Data(indicatorIDs string, subjects string, currency string, date string) (my_orm.F10Responses, error) {\n\tvar jsonF10Rsp my_orm.F10Responses\n\treq, err := http.NewRequest(\"GET\", d.Url, nil)\n\tif err != nil {\n\t\tlevellog.Log.Warn(\"new create http request failed in kafka consume\", zap.Error(err))\n\t\treturn jsonF10Rsp, err\n\t}\n\treq.Header.Add(\"Content-Type\", \"application/json\")\n\trequestBody := my_orm.F10Request{\n\t\tIds:      indicatorIDs,\n\t\tSubjects: subjects,\n\t\tCategory: \"indicator\",\n\t\tStrategy: \"newest\",\n\t\tCurrency: currency,\n\t}\n\n\tif date != \"\" {\n\t\trequestBody.Strategy = \"period\"\n\t\trequestBody.From = date\n\t\trequestBody.To = date\n\t}\n\n\treqVal, _ := query.Values(requestBody)\n\treq.URL.RawQuery = reqVal.Encode()\n\thttpRetryCount := 3 //3次重试\n\tresp, err := httpGetWithRetry(req, httpRetryCount)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrRequestF10ApiFailed)\n\t\treturn jsonF10Rsp, err\n\t}\n\tdefer resp.Body.Close()\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"read http f10 body failed in kafka thread\", zap.Error(err), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\n\terr = json.Unmarshal(body, &jsonF10Rsp)\n\tif err != nil {\n\t\tlevellog.Log.Error(\"unmarshal http req f10 failed in kafka thread\", zap.Error(err), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrJsonParseError)\n\t\treturn jsonF10Rsp, err\n\t}\n\tif jsonF10Rsp.StatusCode != 0 || len(jsonF10Rsp.Data.Data) == 0 {\n\t\t// 日志目前设置为Info,因为目前使用的是股票代码,接口建议使用机构id,第二期会改为机构id\n\t\tlevellog.Log.Info(\"rsp body status code != 0 in kafka thread\", zap.Int(\"StatusCode\", jsonF10Rsp.StatusCode), zap.String(\"status_msg\", jsonF10Rsp.StatusMsg), zap.String(\"data\", string(body)), zap.String(\"ReqArg\", reqVal.Encode()))\n\t\terr = fmt.Errorf(\"rsp body status code != 0 , StatusCode:%d, status_msg:%s\", jsonF10Rsp.StatusCode, jsonF10Rsp.StatusMsg)\n\t\treturn jsonF10Rsp, err\n\t}\n\treturn jsonF10Rsp, nil\n}\n\n// UpdateUnderlyingAssetData 解析财务数据\nfunc (d F10KafkaTaskInfo) UpdateUnderlyingAssetData(codeStr string, validInt int, dateStr string, jsonF10Rsp my_orm.F10Responses, taskInfo F10KafkaTask) {\n\tlevellog.Log.Debug(\"UpdateUnderlyingAssetData\", zap.String(\"codeStr\", codeStr), zap.String(\"dateStr\", dateStr),\n\t\tzap.Int(\"validInt\", validInt), zap.String(\"taskName\", taskInfo.TaskName), zap.String(\"table\", taskInfo.TableName), zap.String(\"db\", taskInfo.DbName))\n\n\tstartExtractTime := time.Now()\n\treport := common.GetReportData(jsonF10Rsp.Data.Data[0].Data[0], taskInfo.HadReport)\n\tunixTime := common.ConvertDateToUnixTime(dateStr, finance_info.GlobalFinanceInfoManager[taskInfo.DbName].TableBasic[taskInfo.TableName].TimeZone)\n\tmapDataFields := make(map[string]string)\n\tPrepareIndicatorsData(mapDataFields, report, jsonF10Rsp.Data.Data[0].Data[0], taskInfo)\n\n\tsql, err := transform.TransDataToSql(taskInfo.TableName, codeStr, validInt, report, unixTime, mapDataFields)\n\tif err != nil || sql == \"\" {\n\t\tlevellog.Log.Error(\"kafka trans sql failed\", zap.Error(err))\n\t\tmetrics.ErrorMetricsInc(metrics.ErrTrans2SqlFailed)\n\t\treturn\n\t}\n\tlevellog.Log.Debug(\"sink sql\", zap.String(\"sql\", sql))\n\t// 调用load进行持久化\n\ttriggerType := metrics.GetTriggerType(my_orm.UpdateByKafkaConsume)\n\t_, err = load.SinkData(taskInfo.DbName, sql)\n\tif err != nil {\n\t\tmetrics.ErrorMetricsInc(metrics.ErrSinkDatabaseFailed)\n\t\tlevellog.Log.Error(\"kafka sink mysql failed\", zap.Error(err))\n\t}\n\tmetrics.TrafficMetricsInc(taskInfo.DbName, taskInfo.TableName, triggerType, metrics.IncExport, metrics.SourceF10)\n\tmetrics.CostBucketMetricsObserve(taskInfo.DbName, taskInfo.TableName, metrics.StageAll, triggerType, metrics.IncExport, float64(time.Since(startExtractTime).Milliseconds()), metrics.SourceF10)\n\treturn\n}\n\n// http请求重试,http层状态码层的重试\nfunc httpGetWithRetry(req *http.Request, retryCount int) (resp *http.Response, err error) {\n\tclient := &http.Client{}\n\tfor i := 0; i < retryCount; i++ {\n\t\tresp, err = client.Do(req)\n\t\tif err == nil && resp.StatusCode == http.StatusOK {\n\t\t\treturn resp, nil\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\n\tif resp != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get response after %d retries, statuscode %d\", retryCount, resp.StatusCode)\n\t}\n\treturn nil, fmt.Errorf(\"failed to get response after %d retries\", retryCount)\n}\n\n// PrepareIndicatorsData 预先准备指标数据\nfunc PrepareIndicatorsData(mapDataFields map[string]string, report int, indicatorRecord map[string]string, taskInfo F10KafkaTask) {\n\tfor f10name, mysqlFieldName := range taskInfo.MapItemName {\n\t\tval, ok := indicatorRecord[f10name]\n\t\tif !ok {\n\t\t\tlevellog.Log.Error(\"f10 field not found in http rsp\", zap.String(\"field\", f10name))\n\t\t\tmetrics.ErrorMetricsInc(metrics.ErrF10ResponseMissingField)\n\t\t\tcontinue\n\t\t}\n\t\tif val == \"\" {\n\t\t\tval = \"NULL\"\n\t\t}\n\t\tmapDataFields[mysqlFieldName] = val\n\t}\n}\n","relevantFile":"<file_path>app/load/load.go\npackage load\n\nimport \"database/sql\"\n\nfunc DataLoaderInit() error {\n\treturn globalLoader.Init()\n}\n\nfunc DataLoaderClose() {\n\tglobalLoader.Close()\n}\n\n// SinkData 数据持久化\nfunc SinkData(dbname string, sql string) (int64, error) {\n\treturn globalLoader.SinkData(dbname, sql)\n}\n\n// FetchOneRowData 获取数据库中的一条数据\nfunc FetchOneRowData(dbname string, sql string) (*sql.Row, error) {\n\treturn globalLoader.FetchOneRowData(dbname, sql)\n}\n\n// FetchRowsData 获取数据库中的多条数据\nfunc FetchRowsData(dbname string, sql string) (*sql.Rows, error) {\n\treturn globalLoader.FetchRowsData(dbname, sql)\n}\n<file_path>app/metrics/metrics.go\n\tpg.OpCode:  \"code\",  //按代码导出\n\n}\n\n// 全量更新和增量更新的两个大类,适合f10等\nconst (\n\tAllExport = \"all\" // 全量\n\tIncExport = \"inc\" // 增量\n)\n\n// 触发方式\nvar trigTypeDict = map[int]string{\n\tmy_orm.UpdateByCron:         \"cron\",\n\tmy_orm.UpdateByKafkaConsume: \"push\",\n\tmy_orm.UpdateByManual:       \"manual\",\n}\n\n// 出错的一些类型枚举\n// 推送错误\nconst (\n\tErrKafkaConsumeError    = \"kafka_consume_error\"     // kafka消费出现问题\n\tErrKafkaMsgMissingField = \"kafka_msg_missing_field\" // kafka的消息体缺少字段\n\tErrKafkaMsgParseFailed  = \"kafka_msg_parse_failed\"  // kafka消息解析失败\n)\n\n// 客户端请求错误\nconst (\n\tErrClientRequestInvalid = \"client_request_invalid\" // 客户端请求错误\n)\n\n// F10接口错误\nconst (\n\tErrRequestF10ApiFailed     = \"request_f10_api_failed\"     // 请求f10api接口失败\n\tErrF10ResponseMissingField = \"f10_response_missing_field\" // f10返回值中\n)\n\n// 美股ETF复权因子接口错误\nconst (\n\tErrRequestFUsEtfAdjFactorApiFailed     = \"request_FUsEtfAdjFactor_api_failed\" // 请求接口失败\n\tErrFUsEtfAdjFactorResponseMissingField = \"FUsEtfAdjFactor_response_missing_field\"\n)\n\n// 内部错误\nconst (\n\tErrTrans2SqlFailed    = \"trans2sql_failed\"     // 转换成sql语句失败\n\tErrSinkDatabaseFailed = \"sink_database_failed\" // 写数据库失败\n\tErrCronTaskExecFailed = \"cron_task_failed\"     // 定时任务执行失败\n)\n\n// 通用错误\nconst (\n\tErrJsonParseError = \"json_parse_error\" // json解析失败\n)\n\n// 任务阶段stage\nconst (\n\tStageAll       = \"all\"       //完整阶段\n\tStageExtract   = \"extract\"   //抽取\n\tStageTransform = \"transform\" //转换\n\tStageLoad      = \"load\"      //持久化\n<file_path>main.go\n\nimport (\n\t\"finance_etl/app/config\"\n\t\"finance_etl/app/cron\"\n\t\"finance_etl/app/extract_manager\"\n\t\"finance_etl/app/finance_info\"\n\t\"finance_etl/app/load\"\n\tlevellog \"finance_etl/app/log\"\n\t\"finance_etl/app/security_code_change\"\n\t\"finance_etl/app/server\"\n\t\"flag\"\n\t\"log\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"go.uber.org/zap\"\n)\n\nvar version string\nvar configPath string\n\nfunc MetricsVersion() {\n\tpromauto.NewGauge(prometheus.GaugeOpts{\n\t\tName: \"gms_server_stable\",\n\t\tConstLabels: map[string]string{\n\t\t\t\"version\": version,\n\t\t},\n\t}).SetToCurrentTime()\n}\n\nfunc main() {\n\tflag.StringVar(&configPath, \"config\", \"/usr/local/conf/conf.yaml\", \"Path to the configuration file, default is /usr/local/conf/conf.yaml\")\n\tflag.Parse()\n\n\tlog.Printf(\"Finance ETL Server Started, version: %s\\n\", version)\n\tMetricsVersion()\n\n\tconfig.ConfigureInit(configPath)\n\tlevellog.InitLog()\n\tcron.GCronManager.Start()\n\terr := finance_info.Init()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init manager failed\", zap.String(\"error\", err.Error()))\n\t\treturn\n\t}\n\terr = load.DataLoaderInit()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Init load mysql failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n\t\treturn\n\t}\n\t// 开启数据自动更新\n\textract_manager.StartAutoExtract(config.GetMysql().SinkDb)\n\tdefer extract_manager.StopAutoExtract()\n\n\t// 开启代码变更（转板）定时任务。\n\terr = security_code_change.StartCodeChangeCron()\n\tif err != nil {\n\t\tlevellog.Log.Error(\"Start code change cron failed\", zap.String(\"error\", err.Error()))\n\t\tload.DataLoaderClose()\n<file_path>app/load/load_mysql.go\n\treturn nil\n}\n\nfunc (d *DataLoader) Close() {\n\tfor _, db := range d.MysqlDbs {\n\t\tif db == nil {\n\t\t\tcontinue\n\t\t}\n\t\terr := db.Close()\n\t\tif err != nil {\n\t\t\tlevellog.Log.Error(\"close mysql conn failed\", zap.String(\"error\", err.Error()))\n\t\t}\n\t}\n}\n\nfunc (d *DataLoader) GetConn(dbname string) (*sql.DB, error) {\n\tconn, ok := d.MysqlDbs[dbname]\n\tif !ok {\n\t\treturn nil, errors.New(\"Not support database\")\n\t}\n\tif conn.Ping() == nil {\n\t\treturn conn, nil\n\t}\n\tdb, err := mysql_dao.NewDbConn(mysql_dao.MakeDsn(config.GetMysql().Address, config.GetMysql().Params, dbname))\n\tif err != nil {\n\t\tlevellog.Log.Error(\"reconnect mysql connection failed\", zap.String(\"dbname\", dbname), zap.Error(err))\n\t\treturn nil, err\n\t}\n\tlevellog.Log.Info(\"reconnect mysql connection successfully\", zap.String(\"dbname\", dbname))\n\td.MysqlDbs[dbname] = db\n\treturn db, nil\n}\n\n// UpdateData mysql automatic 语句\n/*func (d *DataLoader) UpdateData(table string, dbname string, datas *MysqlLoadDatas) error {\n\tdb, err := d.GetConn(dbname)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsqlStr := fmt.Sprintf(\"automatic %s set \", table)\n\tfor index, id := range datas.Ids {\n\t\tif index != 0 {\n\t\t\tsqlStr += \"and \"\n\t\t}\n\t\tsqlStr += fmt.Sprintf(\"%s = ? \", id)\n\t}\n\tsqlStr += \"where zqdm = ? and bbrq = ?\"\n\tfor _, record := range datas.Data {\n\n\t}\n\tret, err := db.Exec(sqlStr)\n\treturn nil\n}\n\n// InsertData mysql insert语句\nfunc (d *DataLoader) InsertData(table string, dbname string, datas *MysqlLoadDatas) error {\n\treturn nil\n}*/\n\n// SinkData 数据持久化","relevantFileList":[],"filePath":"app/extract/automatic/update_kafka_f10.go","template":"go","multiRes":{"hipilot":"v.Error(\"get valid trans failed\", zap.Error(err))","CodeQwen2.5-7B":"llog.Log.Error(\"get market code from query f10 failed\", zap.Error(err))\n\t\treturn ThsMarketCode{}, err\n\t}\n\tif len(jsonF10Rsp.Data.Data) == 0 {\n\t\tlevellog.Log.Error(\"get market code from query f10 failed\", zap.String(\"f10TaskId\", f10TaskId), zap.String(\"subjects\", subjects), zap.String(\"indicators\", indicators))\n\t\treturn ThsMarketCode{}, errors.New(\"get market code from query f10 failed\")\n\t}\n\t// 从推送指标中获取市场\n\tmarket := jsonF10Rsp.Data.Data[0].Data[0][\"market\"]\n\tif market == \"\" {\n\t\tlevellog.Log.Error(\"get market code from query f10 failed\", zap.String(\"f10TaskId\", f10TaskId), zap.String(\"subjects\", subjects), zap.String(\"indicators\", indicators))\n\t\treturn ThsMarketCode{}, errors.New(\"get market code from query f10 failed\")\n\t}\n\t// 从推送指标中获取代码\n\tcode := jsonF10Rsp.Data.Data[0].Data[0][\"code\"]\n\tif code == \"\" {\n\t\tlevellog.Log.Error(\"get market code from query f10 failed\", zap.String(\"f10TaskId\", f10TaskId), zap.String(\"subjects\", subjects), zap.String(\"indicators\", indicators))\n\t\treturn Ths","copilot":"zap.Int64(\"affect_count\", affect_count))","deepseek-chat-lite":"llog.Log.Error(\"get market code from query f10 failed\", zap.Error(err))","CodeQwen1.5-7B":"llog.Log.Error(\"get market code from query f10 failed\", zap.Error(err))"}},{"prefix":"package operations\n\nimport (\n\t\"encoding/json\"\n\t\"hqops/models\"\n)\n\nconst OPAppendCode OPType = \"append_code\"\n\nfunc init() {\n\topTypeMap[OPAppendCode] = struct {\n\t\tName       string `json:\"name\"`\n\t\tDesc       string `json:\"desc\"`\n\t\tPermission string `json:\"permission\"`\n\t}{Name: \"RGM盘后追加代码\", Desc: \"RGM支持盘后追加代码,增量初始化\", Permission: models.SCRIPT_PERMISSION_OPS}\n\n\topImpCreatorMap[OPAppendCode] = func(str string) OPImplement {\n\t\timp := &OPAppendCodeImp{OPAppendCodeParams:OPAppendCodeParams{}}\n\t\t(&imp.OPAppendCodeParams).Parser(str)\n\t\treturn imp\n\t}\n}\n\ntype OPAppendCodeParams struct {\n\tPeriod         string            `json:\"period\"`           // 修复数据周期 一次仅支持修复一个周期\n\tMarket         string            `json:\"market\"`           // 修复数据市场 一次仅支持修复一个市场\n\tRepairCodeMode string            `json:\"repair_code_mode\"` // 修复代码模式 'all'表示全量修复 'manual'表示输入代码修复\n\tSourceMode     string            `json:\"source_mode\"`      // 修复K线模式 'backrepair'表示备份文件还原修复 'centerepair'表示中心站点下发修复\n\tCodes          []string          `json:\"codes\"`            // 修复代码,当repair_code_mode=manual用到\n\tBeginTime      string            `json:\"begin_time\"`       // 修复日期开始时间,source_mode=centerepair用到\n\tEndTime        string            `json:\"end_time\"`         // 修复日期结束时间,source_mode=centerepair用到\n\tSource         ServiceInstanceID `json:\"source\"`           // 数据源主机,source_mode=centerepair用到\n}\n\nfunc (o *OPAppendCodeParams) String() (string, error) {\n\tbt, err := json.Marshal(o)\n\treturn string(bt), err\n}\n\nfunc (o *OPAppendCodeParams) Parser(str string) error {\n\treturn json.Unmarshal([]byte(str), o)\n}\n\ntype OPAppendCodeImp struct {\n\tOPAppendCodeParams\n}\n\nfunc (*OPAppendCodeImp) Type() OPType {\n\treturn OPHisKlineRepair\n}\n\nfunc (o *OPAppendCodeImp) Params() string {\n\tstr, _ := o.OPAppendCodeParams.String()\n\treturn str\n}\n\n// Exec 执行操作\nfunc (o *OPAppendCodeImp) Exec(uuid string, instanceIDs ServiceInstanceIDList) error {\n\tretrun","infill":" nil","suffix":"\n}\n\nfunc (o *OPAppendCodeImp) GetOpPermission() string {\n\treturn opTypeMap[o.Type()].Permission\n}\n","relevantFile":"<file_path>main.go\n/*\nCopyright © 2020 NAME HERE <EMAIL ADDRESS>\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\npackage main\n\nimport (\n\t\"hqops/cmd\"\n\t\"runtime\"\n)\n\nfunc main() {\n\truntime.GOMAXPROCS(runtime.NumCPU())\n\tcmd.Execute()\n}\n<file_path>internal/operations/op_start_service.go\npackage operations\n\nimport (\n\t\"errors\"\n\t\"hqops/models\"\n\t\"net/http\"\n)\n\nconst OpStartService OPType = \"start_service\"\n\nfunc init() {\n\topTypeMap[OpStartService] = struct {\n\t\tName       string `json:\"name\"`\n\t\tDesc       string `json:\"desc\"`\n\t\tPermission string `json:\"permission\"`\n\t}{Name: \"启动服务操作\", Desc: \"启动服务操作，允许对外提供服务\", Permission: models.SCRIPT_PERMISSION_OPS}\n\n\topImpCreatorMap[OpStartService] = func(str string) OPImplement {\n\t\treturn &OPStartServiceImp{}\n\t}\n}\n\n// OPSetLogLevelImp 日志等级设置操作实例\ntype OPStartServiceImp struct{}\n\n// Type 获取操作类型\nfunc (*OPStartServiceImp) Type() OPType {\n\treturn OpStartService\n}\n\nfunc (*OPStartServiceImp) Params() string {\n\treturn \"\"\n}\n\n// Exec 执行操作\nfunc (o *OPStartServiceImp) Exec(uuid string, instanceIDs ServiceInstanceIDList) error {\n\t// 获取实例\n\tinstances := getServiceInstances(instanceIDs)\n\t// 主机执行指令\n\treturn instanceIDs.ForEach(uuid, func(id ServiceInstanceID) error {\n\t\thttpPort, ok := instances.HttpPort(id)\n\t\tif !ok {\n\t\t\treturn errors.New(\"service instance is not existed\")\n\t\t}\n\n\t\t_, err := httpRequest(id.HostName, httpPort, http.MethodPost, UrlOM, ReqTplOmStartService.Format(), false, Wait45Second)\n\t\treturn err\n\t})\n}\n\nfunc (o *OPStartServiceImp) GetOpPermission() string {\n\treturn opTypeMap[o.Type()].Permission\n}","relevantFileList":[],"filePath":"internal/operations/op_append_code.go","template":"go","multiRes":{"hipilot":"s := \"\"\n    return models.CreateOperation(uuid, o.Type(), o.Params(), retruns, models.OP_STATUS_OK, instanceIDs)","CodeQwen2.5-7B":", err := o.ExecWithRetrun(uuid, instanceIDs)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn retrun.GetError()<|file_sep|><|fim_prefix|>/ops/ops.go\npackage models","copilot":" nil","deepseek-chat-lite":"s, err := ExecAppendCode(o.Period, o.Market, o.RepairCodeMode, o.SourceMode, o.Codes, o.BeginTime, o.EndTime, o.Source, uuid, instanceIDs)","CodeQwen1.5-7B":" := &OPReturn{"}},{"prefix":"package operations\n\nimport (\n\t\"encoding/json\"\n\t\"hqops/models\"\n)\n\nconst OPAppendCode OPType = \"append_code\"\n\nfunc init() {\n\topTypeMap[OPAppendCode] = struct {\n\t\tName       string `json:\"name\"`\n\t\tDesc       string `json:\"desc\"`\n\t\tPermission string `json:\"permission\"`\n\t}{Name: \"RGM盘后追加代码\", Desc: \"RGM支持盘后追加代码,增量初始化\", Permission: models.SCRIPT_PERMISSION_OPS}\n\n\topImpCreatorMap[OPAppendCode] = func(str string) OPImplement {\n\t\timp := &OPAppendCodeImp{OPAppendCodeParams:OPAppendCodeParams{}}\n\t\t(&imp.OPAppendCodeParams).Parser(str)\n\t\treturn imp\n\t}\n}\n\ntype OPAppendCodeParams struct {\n\tPeriod         string            `json:\"period\"`           // 修复数据周期 一次仅支持修复一个周期\n\tMarket         string            `json:\"market\"`           // 修复数据市场 一次仅支持修复一个市场\n\tRepairCodeMode string            `json:\"repair_code_mode\"` // 修复代码模式 'all'表示全量修复 'manual'表示输入代码修复\n\tSourceMode     string            `json:\"source_mode\"`      // 修复K线模式 'backrepair'表示备份文件还原修复 'centerepair'表示中心站点下发修复\n\tCodes          []string          `json:\"codes\"`            // 修复代码,当repair_code_mode=manual用到\n\tBeginTime      string            `json:\"begin_time\"`       // 修复日期开始时间,source_mode=centerepair用到\n\tEndTime        string            `json:\"end_time\"`         // 修复日期结束时间,source_mode=centerepair用到\n\tSource         ServiceInstanceID `json:\"source\"`           // 数据源主机,source_mode=centerepair用到\n}\n\nfunc (o *OPAppendCodeParams) String() (string, error) {\n\tbt, err := json.Marshal(o)\n\treturn string(bt), err\n}\n\nfunc (o *OPAppendCodeParams) Parser(str string) error {\n\treturn json.Unmarshal([]byte(str), o)\n}\n\ntype OPAppendCodeImp struct {\n\tOPAppendCodeParams\n}\n\nfunc (*OPAppendCodeImp) Type() OPType {\n\treturn OPHisKlineRepair\n}\n\nfunc (o *OPAppendCodeImp) Params() string {\n\tstr, _ := o.OPAppendCodeParams.String()\n\treturn str\n}\n\n// Exec 执行操作\nfunc (o *OPAppendCodeImp) Exec(uuid string, instanceIDs ServiceInstanceIDList) error {\n\tretrun erro","infill":"rs.New(\"not implement\")","suffix":"\n}\n\nfunc (o *OPAppendCodeImp) GetOpPermission() string {\n\treturn opTypeMap[o.Type()].Permission\n}\n","relevantFile":"<file_path>main.go\n/*\nCopyright © 2020 NAME HERE <EMAIL ADDRESS>\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\npackage main\n\nimport (\n\t\"hqops/cmd\"\n\t\"runtime\"\n)\n\nfunc main() {\n\truntime.GOMAXPROCS(runtime.NumCPU())\n\tcmd.Execute()\n}\n<file_path>internal/operations/op_start_service.go\npackage operations\n\nimport (\n\t\"errors\"\n\t\"hqops/models\"\n\t\"net/http\"\n)\n\nconst OpStartService OPType = \"start_service\"\n\nfunc init() {\n\topTypeMap[OpStartService] = struct {\n\t\tName       string `json:\"name\"`\n\t\tDesc       string `json:\"desc\"`\n\t\tPermission string `json:\"permission\"`\n\t}{Name: \"启动服务操作\", Desc: \"启动服务操作，允许对外提供服务\", Permission: models.SCRIPT_PERMISSION_OPS}\n\n\topImpCreatorMap[OpStartService] = func(str string) OPImplement {\n\t\treturn &OPStartServiceImp{}\n\t}\n}\n\n// OPSetLogLevelImp 日志等级设置操作实例\ntype OPStartServiceImp struct{}\n\n// Type 获取操作类型\nfunc (*OPStartServiceImp) Type() OPType {\n\treturn OpStartService\n}\n\nfunc (*OPStartServiceImp) Params() string {\n\treturn \"\"\n}\n\n// Exec 执行操作\nfunc (o *OPStartServiceImp) Exec(uuid string, instanceIDs ServiceInstanceIDList) error {\n\t// 获取实例\n\tinstances := getServiceInstances(instanceIDs)\n\t// 主机执行指令\n\treturn instanceIDs.ForEach(uuid, func(id ServiceInstanceID) error {\n\t\thttpPort, ok := instances.HttpPort(id)\n\t\tif !ok {\n\t\t\treturn errors.New(\"service instance is not existed\")\n\t\t}\n\n\t\t_, err := httpRequest(id.HostName, httpPort, http.MethodPost, UrlOM, ReqTplOmStartService.Format(), false, Wait45Second)\n\t\treturn err\n\t})\n}\n\nfunc (o *OPStartServiceImp) GetOpPermission() string {\n\treturn opTypeMap[o.Type()].Permission\n}","relevantFileList":[],"filePath":"internal/operations/op_append_code.go","template":"go","multiRes":{"hipilot":"rs.New(\"\")","CodeQwen2.5-7B":"r","copilot":"rs.New(\"not implement\")","deepseek-chat-lite":"r","CodeQwen1.5-7B":"r"}},{"prefix":"package operation\n\nimport (\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hqops/internal/http/utils\"\n\t\"hqops/internal/operations\"\n\t\"hqops/models\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/gin-gonic/gin\"\n)\n\nfunc GetOperation(c *gin.Context) {\n\tform := struct {\n\t\tUUID string `form:\"uuid\" binding:\"required,uuid\"`\n\t}{}\n\n\tif err := c.BindQuery(&form); err != nil {\n\t\tc.JSON(http.StatusBadRequest, utils.ResponseForm{\n\t\t\tStatusCode:    -1,\n\t\t\tStatusMessage: err.Error(),\n\t\t})\n\t\treturn\n\t}\n\n\tuuid := form.UUID\n\n\top, err := (&models.Operation{UUID: uuid}).Query()\n\tif err != nil {\n\t\tc.JSON(http.StatusOK, utils.ResponseForm{\n\t\t\tStatusCode:    -1,\n\t\t\tStatusMessage: err.Error(),\n\t\t})\n\t\treturn\n\t}\n\n\tc.JSON(http.StatusOK, utils.ResponseForm{\n\t\tData: (*Operation)(op).JSON(),\n\t})\n}\n\ntype GetOperationListForm struct {\n\tPage      int    `form:\"page\"`\n\tLimit     int    `form:\"limit\"`\n\tOrder     string `form:\"order\"`\n\tField     string `form:\"field\" binding:\"sql_injection\"`\n\tName      string `form:\"name\"`\n\tUuid      string `form:\"uuid\"`\n\tStatus    string `form:\"status\"`\n\tStartTime string `form:\"start_time\"`\n\tEndTime   string `form:\"end_time\"`\n\tCreator   string `form:\"creator\"`\n}\n\nfunc (g *GetOperationListForm) Conditions() []models.FieldCondition {\n\tconditions := make([]models.FieldCondition, 0, 0)\n\n\tif len(g.Field) > 0 {\n\t\tfield := g.Field\n\t\tif g.Field == \"create_time\" {\n\t\t\tfield = \"created_at\"\n\t\t}\n\t\tconditions = append(conditions, models.FieldOrder{\n\t\t\tField: field,\n\t\t\tOrder: g.Order,\n\t\t})\n\t}\n\n\tif len(g.Name) > 0 {\n\t\tconditions = append(conditions, models.FieldOf{\n\t\t\tField: \"name\",\n\t\t\tCond:  strings.Split(g.Name, \",\"),\n\t\t})\n\t}\n\n\tif len(g.Status) > 0 {\n\t\tconditions = append(conditions, models.FieldOf{\n\t\t\tField: \"status\",\n\t\t\tCond:  strings.Split(g.Status, \",\"),\n\t\t})\n\t}\n\n\tif g.Page > 0 {\n\t\tconditions = append(conditions, models.FiledCount{})\n\t}\n\n\tif len(g.StartTime) > 0 && len(g.EndTime) > 0 {\n\t\tst, _ := time.Parse(\"2006-01-02\", g.StartTime)\n\t\tet, _ := time.Parse(\"2006-01-02\", g.EndTime)\n\t\tconditions = append(conditions, model","infill":"opTypeList","suffix":")\r\n\tsupportList := make([]map[string]string, 0, len(opTypeList))\r\n\tfor _, opType := range opTypeList {\r\n\t\tsupportList = append(supportList, map[string]string{\r\n\t\t\t\"type\":        string(opType),\r\n\t\t\t\"description\": opType.Desc(),\r\n\t\t\t\"name\":        opType.Name(),\r\n\t\t\t\"permission\":  opType.Permission(),\r\n\t\t})\r\n\t}\r\n\tc.JSON(http.StatusOK, utils.ResponseForm{\r\n\t\tData: supportList,\r\n\t})\r\n}\r\n\r\nfunc GetOperationCount(c *gin.Context) {\r\n\treqForm := GetOperationListForm{}\r\n\tif err := c.ShouldBindQuery(&reqForm); err != nil {\r\n\t\tc.JSON(http.StatusBadRequest, utils.ResponseForm{\r\n\t\t\tStatusCode:    -1,\r\n\t\t\tStatusMessage: err.Error(),\r\n\t\t})\r\n\t\treturn\r\n\t}\r\n\r\n\tls, err := (&models.Operation{}).Count(reqForm.Conditions()...)\r\n\tif err != nil {\r\n\t\tc.JSON(http.StatusOK, utils.ResponseForm{\r\n\t\t\tStatusCode:    -1,\r\n\t\t\tStatusMessage: err.Error(),\r\n\t\t})\r\n\t\treturn\r\n\t}\r\n\r\n\tc.JSON(http.StatusOK, utils.ResponseForm{\r\n\t\tData: ls,\r\n\t})\r\n}\r\n\r\nfunc PostOperationTerminate(ctx *gin.Context) {\r\n\topForm, ok := ctx.Get(\"store_opform\")\r\n\tif !ok {\r\n\t\tctx.AbortWithStatusJSON(http.StatusBadRequest, utils.ResponseForm{\r\n\t\t\tStatusCode:    http.StatusBadRequest,\r\n\t\t\tStatusMessage: \"invalid form\",\r\n\t\t})\r\n\t\treturn\r\n\t}\r\n\r\n\topParams := operations.OPTerminateParams{}\r\n\tbt, _ := json.Marshal(opForm.(utils.RequestForm).Params)\r\n\t_ = json.Unmarshal(bt, &opParams)\r\n\r\n\timp := &operations.OPTerminateImp{OPTerminateParams: opParams}\r\n\tif err := operations.Create(\r\n\t\timp,\r\n\t\toperations.ServiceInstanceIDList{},\r\n\t\toperations.Creator{\r\n\t\t\tMail: ctx.GetString(\"store_username\") + \"@myhexin.com\",\r\n\t\t\tIP:   ctx.GetString(\"store_client_addr\")},\r\n\t\topForm.(utils.RequestForm).AuditList,\r\n\t); errors.Is(err.Err, operations.ReturnCodeWaitAudit) {\r\n\t\tctx.JSON(http.StatusOK, utils.ResponseForm{StatusMessage: \"操作创建成功，等待审核人审核\", Data: gin.H{\"uuid\": err.UUID}})\r\n\t} else if err.Err == nil {\r\n\t\tctx.JSON(http.StatusOK, utils.ResponseForm{StatusMessage: \"操作创建成功，开始执行\", Data: gin.H{\"uuid\": err.UUID}})\r\n\t} else {\r\n\t\tctx.JSON(http.StatusOK, utils.ResponseForm{StatusCode: -1, StatusMessage: fmt.Sprintf(\"创建任务失败，提示信息：%v\", err.Err)})\r\n\t}\r\n}\r\n","relevantFile":"<file_path>main.go\n/*\nCopyright © 2020 NAME HERE <EMAIL ADDRESS>\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\npackage main\n\nimport (\n\t\"hqops/cmd\"\n\t\"runtime\"\n)\n\nfunc main() {\n\truntime.GOMAXPROCS(runtime.NumCPU())\n\tcmd.Execute()\n}\n<file_path>internal/operations/op_start_service.go\npackage operations\n\nimport (\n\t\"errors\"\n\t\"hqops/models\"\n\t\"net/http\"\n)\n\nconst OpStartService OPType = \"start_service\"\n\nfunc init() {\n\topTypeMap[OpStartService] = struct {\n\t\tName       string `json:\"name\"`\n\t\tDesc       string `json:\"desc\"`\n\t\tPermission string `json:\"permission\"`\n\t}{Name: \"启动服务操作\", Desc: \"启动服务操作，允许对外提供服务\", Permission: models.SCRIPT_PERMISSION_OPS}\n\n\topImpCreatorMap[OpStartService] = func(str string) OPImplement {\n\t\treturn &OPStartServiceImp{}\n\t}\n}\n\n// OPSetLogLevelImp 日志等级设置操作实例\ntype OPStartServiceImp struct{}\n\n// Type 获取操作类型\nfunc (*OPStartServiceImp) Type() OPType {\n\treturn OpStartService\n}\n\nfunc (*OPStartServiceImp) Params() string {\n\treturn \"\"\n}\n\n// Exec 执行操作\nfunc (o *OPStartServiceImp) Exec(uuid string, instanceIDs ServiceInstanceIDList) error {\n\t// 获取实例\n\tinstances := getServiceInstances(instanceIDs)\n\t// 主机执行指令\n\treturn instanceIDs.ForEach(uuid, func(id ServiceInstanceID) error {\n\t\thttpPort, ok := instances.HttpPort(id)\n\t\tif !ok {\n\t\t\treturn errors.New(\"service instance is not existed\")\n\t\t}\n\n\t\t_, err := httpRequest(id.HostName, httpPort, http.MethodPost, UrlOM, ReqTplOmStartService.Format(), false, Wait45Second)\n\t\treturn err\n\t})\n}\n\nfunc (o *OPStartServiceImp) GetOpPermission() string {\n\treturn opTypeMap[o.Type()].Permission\n}\n<file_path>internal/http/router/operation.go\npackage router\n\nimport (\n\t\"github.com/gin-gonic/gin\"\n\t\"hqops/internal/http/api/operation\"\n\t\"hqops/internal/http/middleware\"\n)\n\nfunc addOperationRouter(g *gin.RouterGroup) {\n\tgroup := g.Group(\"/operation\")\n\t{\n\t\tgroup.GET(\"\", operation.GetOperation)\n\t\tgroup.GET(\"list\", operation.GetOperationList)\n\t\tgroup.GET(\"progress\", operation.GetOperationProgress)\n\t\tgroup.GET(\"progress/list\", operation.GetOperationProgressList)\n\t\tgroup.GET(\"/node/list\", operation.GetOperationNodeList)\n\t\tgroup.GET(\"/support/list\", operation.GetOperationSupportList)\n\t\tgroup.GET(\"/count\", operation.GetOperationCount)\n\t\tprivilegeGroup := group.Group(\"\", middleware.IPWhiteListCheckMiddleware(), middleware.JWTAuthMiddleware(), middleware.GetUserName(), middleware.OpQueryCheckMiddleware(), middleware.ApplyCheck())\n\t\t{\n\t\t\tprivilegeGroup.POST(\"terminate\", operation.PostOperationTerminate)\n\t\t}\n\t}\n}\n<file_path>internal/operations/op_append_code.go\npackage operations\n\nimport (\n\t\"encoding/json\"\n\t\"errors\"\n\t\"hqops/models\"\n)\n\nconst OPAppendCode OPType = \"append_code\"\n\nfunc init() {\n\topTypeMap[OPAppendCode] = struct {\n\t\tName       string `json:\"name\"`\n\t\tDesc       string `json:\"desc\"`\n\t\tPermission string `json:\"permission\"`\n\t}{Name: \"RGM盘后追加代码\", Desc: \"RGM支持盘后追加代码,增量初始化\", Permission: models.SCRIPT_PERMISSION_OPS}\n\n\topImpCreatorMap[OPAppendCode] = func(str string) OPImplement {\n\t\timp := &OPAppendCodeImp{OPAppendCodeParams: OPAppendCodeParams{}}\n\t\t(&imp.OPAppendCodeParams).Parser(str)\n\t\treturn imp\n\t}\n}\n\ntype OPAppendCodeParams struct {\n\tPeriod         string            `json:\"period\"`           // 修复数据周期 一次仅支持修复一个周期\n\tMarket         string            `json:\"market\"`           // 修复数据市场 一次仅支持修复一个市场\n\tRepairCodeMode string            `json:\"repair_code_mode\"` // 修复代码模式 'all'表示全量修复 'manual'表示输入代码修复\n\tSourceMode     string            `json:\"source_mode\"`      // 修复K线模式 'backrepair'表示备份文件还原修复 'centerepair'表示中心站点下发修复\n\tCodes          []string          `json:\"codes\"`            // 修复代码,当repair_code_mode=manual用到\n\tBeginTime      string            `json:\"begin_time\"`       // 修复日期开始时间,source_mode=centerepair用到\n\tEndTime        string            `json:\"end_time\"`         // 修复日期结束时间,source_mode=centerepair用到\n\tSource         ServiceInstanceID `json:\"source\"`           // 数据源主机,source_mode=centerepair用到\n}\n\nfunc (o *OPAppendCodeParams) String() (string, error) {\n\tbt, err := json.Marshal(o)\n\treturn string(bt), err\n}\n\nfunc (o *OPAppendCodeParams) Parser(str string) error {\n\treturn json.Unmarshal([]byte(str), o)\n}\n\ntype OPAppendCodeImp struct {\n\tOPAppendCodeParams\n}\n\nfunc (*OPAppendCodeImp) Type() OPType {\n\treturn OPHisKlineRepair\n}\n\nfunc (o *OPAppendCodeImp) Params() string {\n\tstr, _ := o.OPAppendCodeParams.String()\n\treturn str\n}\n\n// Exec 执行操作\nfunc (o *OPAppendCodeImp) Exec(uuid string, instanceIDs ServiceInstanceIDList) error {\n\treturn errors.New(\"not implement\")","relevantFileList":[],"filePath":"internal/http/api/operation/operation.go","template":"go","multiRes":{"hipilot":"models.FieldBetween{\n            Field: \"created_at\",\n            Start: st,\n            End:   et,\n        }","CodeQwen2.5-7B":"s.FieldBetween{\r\n\t\t\tField: \"created_at\",\r\n\t\t\tCond:  []time.Time{st, et},\r\n\t\t})\r\n\t}\r\n\r\n\tif len(g.Uuid) > 0 {\r\n\t\tconditions = append(conditions, models.FieldOf{\r\n\t\t\tField: \"uuid\",\r\n\t\t\tCond:  strings.Split(g.Uuid, \",\")})\r\n\t}\r\n\r\n\tif len(g.Creator) > 0 {\r\n\t\tconditions = append(conditions, models.FieldOf{\r\n\t\t\tField: \"creator\",\r\n\t\t\tCond:  strings.Split(g.Creator, \",\")})\r\n\t}\r\n\r\n\treturn conditions\r\n}\r\n\r\nfunc GetOperationList(c *gin.Context) {\r\n\treqForm := GetOperationListForm{}\r\n\tif err := c.ShouldBindQuery(&reqForm); err != nil {\r\n\t\tc.JSON(http.StatusBadRequest, utils.ResponseForm{\r\n\t\t\tStatusCode:    -1,\r\n\t\t\tStatusMessage: err.Error(),\r\n\t\t})\r\n\t\treturn\r\n\t}\r\n\r\n\tls, err := (&models.Operation{}).Query(reqForm.Conditions()...)\r\n\tif err != nil {\r\n\t\tc.JSON(http.StatusOK, utils.ResponseForm{\r\n\t\t\tStatusCode:    -1,\r\n\t\t\tStatusMessage: err.Error(),\r\n\t\t})\r\n\t\treturn\r\n\t}\r\n\r\n\tc.JSON(http.StatusOK, utils.ResponseForm{\r\n\t\tData: (*OperationList)(ls).JSON(),\r\n\t})\r\n}\r\n\r\nfunc GetOperationSupport(c *gin.Context) {\r\n\topTypeList := operations.GetOperationTypeList()<|file_sep|><|fim_prefix|>/internal/operations/operation.go\npackage operations","copilot":"opTypeList","deepseek-chat-lite":"s.FieldBetween{","CodeQwen1.5-7B":"s.FieldBetween{\r"}},{"prefix":"package gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n//使用gesd算法检测异常值，参数 ","infill":"data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}\n","suffix":"","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>log.go\n}\n\n// Err starts a new message with error level with err as a field if not nil or\n// with info level if err is nil.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Err(err error) *zerolog.Event {\n\treturn Logger.Err(err)\n}\n\n// Trace starts a new message with trace level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Trace() *zerolog.Event {\n\treturn Logger.Trace()\n}\n\n// Debug starts a new message with debug level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Debug() *zerolog.Event {\n\treturn Logger.Debug()\n}\n\n// Info starts a new message with info level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Info() *zerolog.Event {\n\treturn Logger.Info()\n}\n\n// Warn starts a new message with warn level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Warn() *zerolog.Event {\n\treturn Logger.Warn()\n}\n\n// Error starts a new message with error level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Error() *zerolog.Event {\n\treturn Logger.Error()\n}\n\n// Fatal starts a new message with fatal level. The os.Exit(1) function\n// is called by the Msg method.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Fatal() *zerolog.Event {\n\treturn Logger.Fatal()\n}\n\n// Panic starts a new message with panic level. The message is also sent\n// to the panic function.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Panic() *zerolog.Event {\n\treturn Logger.Panic()\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}","relevantFileList":[],"filePath":"internal/gesd/gesd.go","template":"go","multiRes":{"hipilot":"data 是待检测数据，outlierRatio 是用于计算 k 的系数，一般取值为 1 或 2。\nfunc Gesd(data []float64, outlierRatio int) ([]float64, error) {\n    return findOut","CodeQwen2.5-7B":"k为检测的阈值，返回值为异常值的索引\nfunc GesdDetect(numbers []float64, k int) ([]int, error) {\n\toutliers, err := findOutliers(numbers, k)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\toutlierIndices := make([]int, 0)\n\tfor i, number := range numbers {\n\t\tfor _, outlier := range outliers {\n\t\t\tif number == outlier {\n\t\t\t\toutlierIndices = append(outlierIndices, i)\n\t\t\t}\n\t\t}\n\t}\n\treturn outlierIndices, nil\n}","copilot":"data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}\n","deepseek-chat-lite":"k 是窗口大小，n 是最大迭代次数，alpha 是显著性水平","CodeQwen1.5-7B":"k 默认为3"}},{"prefix":"package gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {","infill":"\n\treturn findOutliers(data, k)\n}\n","suffix":"","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>log.go\n}\n\n// Err starts a new message with error level with err as a field if not nil or\n// with info level if err is nil.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Err(err error) *zerolog.Event {\n\treturn Logger.Err(err)\n}\n\n// Trace starts a new message with trace level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Trace() *zerolog.Event {\n\treturn Logger.Trace()\n}\n\n// Debug starts a new message with debug level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Debug() *zerolog.Event {\n\treturn Logger.Debug()\n}\n\n// Info starts a new message with info level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Info() *zerolog.Event {\n\treturn Logger.Info()\n}\n\n// Warn starts a new message with warn level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Warn() *zerolog.Event {\n\treturn Logger.Warn()\n}\n\n// Error starts a new message with error level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Error() *zerolog.Event {\n\treturn Logger.Error()\n}\n\n// Fatal starts a new message with fatal level. The os.Exit(1) function\n// is called by the Msg method.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Fatal() *zerolog.Event {\n\treturn Logger.Fatal()\n}\n\n// Panic starts a new message with panic level. The message is also sent\n// to the panic function.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Panic() *zerolog.Event {\n\treturn Logger.Panic()\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}","relevantFileList":[],"filePath":"internal/gesd/gesd.go","template":"go","multiRes":{"hipilot":"\n    if k >= len(data) {\n        return nil, ErrInvalidK\n    }\n    return findOutliers(data, k)\n}\n","CodeQwen2.5-7B":"\n\tif k < 2 {\n\t\treturn nil, ErrInvalidK\n\t}\n\tif len(data) < k {\n\t\treturn nil, ErrInvalidData\n\t}","copilot":"\n\treturn findOutliers(data, k)\n}\n","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {","infill":"data := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n","suffix":"}","relevantFile":"<file_path>log.go\n}\n\n// Err starts a new message with error level with err as a field if not nil or\n// with info level if err is nil.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Err(err error) *zerolog.Event {\n\treturn Logger.Err(err)\n}\n\n// Trace starts a new message with trace level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Trace() *zerolog.Event {\n\treturn Logger.Trace()\n}\n\n// Debug starts a new message with debug level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Debug() *zerolog.Event {\n\treturn Logger.Debug()\n}\n\n// Info starts a new message with info level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Info() *zerolog.Event {\n\treturn Logger.Info()\n}\n\n// Warn starts a new message with warn level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Warn() *zerolog.Event {\n\treturn Logger.Warn()\n}\n\n// Error starts a new message with error level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Error() *zerolog.Event {\n\treturn Logger.Error()\n}\n\n// Fatal starts a new message with fatal level. The os.Exit(1) function\n// is called by the Msg method.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Fatal() *zerolog.Event {\n\treturn Logger.Fatal()\n}\n\n// Panic starts a new message with panic level. The message is also sent\n// to the panic function.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Panic() *zerolog.Event {\n\treturn Logger.Panic()\n}\n<file_path>internal/httpserver/httpserver_test.go\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/calculate\", compareReqBytes, 100*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestNewHttpServer(t *testing.T) {\n\tworkloadName := \"hexin-apm-hxredis\"\n\tindex := strings.LastIndexByte(workloadName, '-')\n\tif index > -1 {\n\t\tfmt.Println(workloadName[index+1:])\n\t}\n}\n\nfunc TestSyncWorkloadConfSync(t *testing.T) {\n\tsyncStr := \"[{\\\"workload_name\\\":\\\"hexin-apm-open-api\\\",\\\"namespace\\\":\\\"default\\\",\\\"stack\\\":\\\"java\\\"},{\\\"workload_name\\\":\\\"mobileweb-apm-api\\\",\\\"namespace\\\":\\\"default\\\",\\\"stack\\\":\\\"golang\\\"}]\"\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_workload_stack_all\", []byte(syncStr), 100*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/gesd/gesd_test.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\n\t// 测试用例数据\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5","copilot":"data := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\tdata = append(","infill":"d","suffix":"ata, 0)\n\tdata = append(\n\treturn findOutliers(data, k)\n}\n","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>log.go\n}\n\n// Err starts a new message with error level with err as a field if not nil or\n// with info level if err is nil.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Err(err error) *zerolog.Event {\n\treturn Logger.Err(err)\n}\n\n// Trace starts a new message with trace level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Trace() *zerolog.Event {\n\treturn Logger.Trace()\n}\n\n// Debug starts a new message with debug level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Debug() *zerolog.Event {\n\treturn Logger.Debug()\n}\n\n// Info starts a new message with info level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Info() *zerolog.Event {\n\treturn Logger.Info()\n}\n\n// Warn starts a new message with warn level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Warn() *zerolog.Event {\n\treturn Logger.Warn()\n}\n\n// Error starts a new message with error level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Error() *zerolog.Event {\n\treturn Logger.Error()\n}\n\n// Fatal starts a new message with fatal level. The os.Exit(1) function\n// is called by the Msg method.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Fatal() *zerolog.Event {\n\treturn Logger.Fatal()\n}\n\n// Panic starts a new message with panic level. The message is also sent\n// to the panic function.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Panic() *zerolog.Event {\n\treturn Logger.Panic()\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}","relevantFileList":[],"filePath":"internal/gesd/gesd.go","template":"go","multiRes":{"hipilot":"[]float64{0}, d","CodeQwen2.5-7B":"d","copilot":"d","deepseek-chat-lite":"d","CodeQwen1.5-7B":" d"}},{"prefix":"package gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\tdata = ","infill":"a","suffix":"ppend(data, 0)\n\tdata = append\n\treturn findOutliers(data, k)\n}\n","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>log.go\n}\n\n// Err starts a new message with error level with err as a field if not nil or\n// with info level if err is nil.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Err(err error) *zerolog.Event {\n\treturn Logger.Err(err)\n}\n\n// Trace starts a new message with trace level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Trace() *zerolog.Event {\n\treturn Logger.Trace()\n}\n\n// Debug starts a new message with debug level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Debug() *zerolog.Event {\n\treturn Logger.Debug()\n}\n\n// Info starts a new message with info level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Info() *zerolog.Event {\n\treturn Logger.Info()\n}\n\n// Warn starts a new message with warn level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Warn() *zerolog.Event {\n\treturn Logger.Warn()\n}\n\n// Error starts a new message with error level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Error() *zerolog.Event {\n\treturn Logger.Error()\n}\n\n// Fatal starts a new message with fatal level. The os.Exit(1) function\n// is called by the Msg method.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Fatal() *zerolog.Event {\n\treturn Logger.Fatal()\n}\n\n// Panic starts a new message with panic level. The message is also sent\n// to the panic function.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Panic() *zerolog.Event {\n\treturn Logger.Panic()\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}","relevantFileList":[],"filePath":"internal/gesd/gesd.go","template":"go","multiRes":{"hipilot":"a","CodeQwen2.5-7B":"a","copilot":"a","deepseek-chat-lite":"a","CodeQwen1.5-7B":" a"}},{"prefix":"package gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\tdata","infill":" = append(data, 0)\n\tdata = \n\tret","suffix":"urn findOutliers(data, k)\n}\n","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>log.go\n}\n\n// Err starts a new message with error level with err as a field if not nil or\n// with info level if err is nil.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Err(err error) *zerolog.Event {\n\treturn Logger.Err(err)\n}\n\n// Trace starts a new message with trace level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Trace() *zerolog.Event {\n\treturn Logger.Trace()\n}\n\n// Debug starts a new message with debug level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Debug() *zerolog.Event {\n\treturn Logger.Debug()\n}\n\n// Info starts a new message with info level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Info() *zerolog.Event {\n\treturn Logger.Info()\n}\n\n// Warn starts a new message with warn level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Warn() *zerolog.Event {\n\treturn Logger.Warn()\n}\n\n// Error starts a new message with error level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Error() *zerolog.Event {\n\treturn Logger.Error()\n}\n\n// Fatal starts a new message with fatal level. The os.Exit(1) function\n// is called by the Msg method.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Fatal() *zerolog.Event {\n\treturn Logger.Fatal()\n}\n\n// Panic starts a new message with panic level. The message is also sent\n// to the panic function.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Panic() *zerolog.Event {\n\treturn Logger.Panic()\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}","relevantFileList":[],"filePath":"internal/gesd/gesd.go","template":"go","multiRes":{"hipilot":"= append(make([]float64, k-1), data...) // 在数据前添加 k-1 个 NaN，用以计算第一组数据的前 k 个标准差\n    ret","CodeQwen2.5-7B":" = append(data, data[0:k]...)\n\tret","copilot":" = append(data, 0)\n\tdata = \n\tret","deepseek-chat-lite":" = append(make([]float64, k-1), data...)","CodeQwen1.5-7B":"  = append(data, 0)"}},{"prefix":"package gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\tdata =","infill":" appen","suffix":"d(data, 0)\n\tdata = append\n\treturn findOutliers(data, k)\n}\n","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>log.go\n}\n\n// Err starts a new message with error level with err as a field if not nil or\n// with info level if err is nil.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Err(err error) *zerolog.Event {\n\treturn Logger.Err(err)\n}\n\n// Trace starts a new message with trace level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Trace() *zerolog.Event {\n\treturn Logger.Trace()\n}\n\n// Debug starts a new message with debug level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Debug() *zerolog.Event {\n\treturn Logger.Debug()\n}\n\n// Info starts a new message with info level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Info() *zerolog.Event {\n\treturn Logger.Info()\n}\n\n// Warn starts a new message with warn level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Warn() *zerolog.Event {\n\treturn Logger.Warn()\n}\n\n// Error starts a new message with error level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Error() *zerolog.Event {\n\treturn Logger.Error()\n}\n\n// Fatal starts a new message with fatal level. The os.Exit(1) function\n// is called by the Msg method.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Fatal() *zerolog.Event {\n\treturn Logger.Fatal()\n}\n\n// Panic starts a new message with panic level. The message is also sent\n// to the panic function.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Panic() *zerolog.Event {\n\treturn Logger.Panic()\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}","relevantFileList":[],"filePath":"internal/gesd/gesd.go","template":"go","multiRes":{"hipilot":"appen","CodeQwen2.5-7B":" appen","copilot":" appen","deepseek-chat-lite":" appen","CodeQwen1.5-7B":" appen<file_sep> /README.md"}},{"prefix":"package gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\tdata = append(data, 0)\n\tdata = ","infill":"append(data, 0)\n\treturn","suffix":" findOutliers(data, k)\n}\n","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>log.go\n}\n\n// Err starts a new message with error level with err as a field if not nil or\n// with info level if err is nil.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Err(err error) *zerolog.Event {\n\treturn Logger.Err(err)\n}\n\n// Trace starts a new message with trace level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Trace() *zerolog.Event {\n\treturn Logger.Trace()\n}\n\n// Debug starts a new message with debug level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Debug() *zerolog.Event {\n\treturn Logger.Debug()\n}\n\n// Info starts a new message with info level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Info() *zerolog.Event {\n\treturn Logger.Info()\n}\n\n// Warn starts a new message with warn level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Warn() *zerolog.Event {\n\treturn Logger.Warn()\n}\n\n// Error starts a new message with error level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Error() *zerolog.Event {\n\treturn Logger.Error()\n}\n\n// Fatal starts a new message with fatal level. The os.Exit(1) function\n// is called by the Msg method.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Fatal() *zerolog.Event {\n\treturn Logger.Fatal()\n}\n\n// Panic starts a new message with panic level. The message is also sent\n// to the panic function.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Panic() *zerolog.Event {\n\treturn Logger.Panic()\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}","relevantFileList":[],"filePath":"internal/gesd/gesd.go","template":"go","multiRes":{"hipilot":"append([]float64{0}, data...)\n    return","CodeQwen2.5-7B":"append(data, 0)\n\treturn","copilot":"append(data, 0)\n\treturn","deepseek-chat-lite":"append(data, 0)","CodeQwen1.5-7B":" append(data, 0)"}},{"prefix":"package gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\tdata = append(data, 0)\n\tdata = append","infill":"(dat","suffix":"a, 0)\n\treturn findOutliers(data, k)\n}\n","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>log.go\n}\n\n// Err starts a new message with error level with err as a field if not nil or\n// with info level if err is nil.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Err(err error) *zerolog.Event {\n\treturn Logger.Err(err)\n}\n\n// Trace starts a new message with trace level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Trace() *zerolog.Event {\n\treturn Logger.Trace()\n}\n\n// Debug starts a new message with debug level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Debug() *zerolog.Event {\n\treturn Logger.Debug()\n}\n\n// Info starts a new message with info level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Info() *zerolog.Event {\n\treturn Logger.Info()\n}\n\n// Warn starts a new message with warn level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Warn() *zerolog.Event {\n\treturn Logger.Warn()\n}\n\n// Error starts a new message with error level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Error() *zerolog.Event {\n\treturn Logger.Error()\n}\n\n// Fatal starts a new message with fatal level. The os.Exit(1) function\n// is called by the Msg method.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Fatal() *zerolog.Event {\n\treturn Logger.Fatal()\n}\n\n// Panic starts a new message with panic level. The message is also sent\n// to the panic function.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Panic() *zerolog.Event {\n\treturn Logger.Panic()\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}","relevantFileList":[],"filePath":"internal/gesd/gesd.go","template":"go","multiRes":{"hipilot":"(dat","CodeQwen2.5-7B":"(dat","copilot":"(dat","deepseek-chat-lite":"(dat","CodeQwen1.5-7B":" (dat<file_sep> /README.md"}},{"prefix":"package gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutlier","infill":"s","suffix":"(data, k)\n}\n","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>log.go\n}\n\n// Err starts a new message with error level with err as a field if not nil or\n// with info level if err is nil.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Err(err error) *zerolog.Event {\n\treturn Logger.Err(err)\n}\n\n// Trace starts a new message with trace level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Trace() *zerolog.Event {\n\treturn Logger.Trace()\n}\n\n// Debug starts a new message with debug level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Debug() *zerolog.Event {\n\treturn Logger.Debug()\n}\n\n// Info starts a new message with info level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Info() *zerolog.Event {\n\treturn Logger.Info()\n}\n\n// Warn starts a new message with warn level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Warn() *zerolog.Event {\n\treturn Logger.Warn()\n}\n\n// Error starts a new message with error level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Error() *zerolog.Event {\n\treturn Logger.Error()\n}\n\n// Fatal starts a new message with fatal level. The os.Exit(1) function\n// is called by the Msg method.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Fatal() *zerolog.Event {\n\treturn Logger.Fatal()\n}\n\n// Panic starts a new message with panic level. The message is also sent\n// to the panic function.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Panic() *zerolog.Event {\n\treturn Logger.Panic()\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}","relevantFileList":[],"filePath":"internal/gesd/gesd.go","template":"go","multiRes":{"hipilot":"s","CodeQwen2.5-7B":"s","copilot":"s","deepseek-chat-lite":"s","CodeQwen1.5-7B":"s<file_sep> /README.md"}},{"prefix":"package zscore\n\n//","infill":" ZScore 计算数据集的Z分数\nfunc ZScore(data []float64) []float64 {\n\tmean, sd := calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n","suffix":"","relevantFile":"<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"----------","CodeQwen2.5-7B":" ZScore is a struct that holds the z-score of a value.","copilot":" ZScore 计算数据集的Z分数\nfunc ZScore(data []float64) []float64 {\n\tmean, sd := calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n","deepseek-chat-lite":" ZScore represents a z-score.","CodeQwen1.5-7B":"  ZScore calculates the z-score of a value in a slice of values."}},{"prefix":"package zscore\n\n// ZScore 计算数据集的Z分数\nfunc ZScore(data []float64) []float64 {\n\tmean, sd := calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc calculateMeanAndStandardDeviation(data []float64) (float64, float64) {","infill":"\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n","suffix":"","relevantFile":"<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"\n    var sum float64\n    for _, val := range data {\n        sum += val\n    }\n    mean := sum / float64(len(data))\n    var deviation float64\n    for _, val := range data {\n        deviation += (val -","CodeQwen2.5-7B":"\n\tmean := calculateMean(data)\n\tsd := calculateStandardDeviation(data, mean)\n\treturn mean, sd\n}","copilot":"\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package zscore\n\n// ZScore 计算数据集的Z分数\nfunc ZScore(data []float64) []float64 {\n\tmean, sd := calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n//","infill":" FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}","suffix":"","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"func main() {\n//    data := []float64{1.2, 2.3, 3.4, 1.7, 2.1, 3.8, 2.6}\n//    fmt.Println(ZScore(data))\n//","CodeQwen2.5-7B":" ZScoreWithMeanAndSD 计算数据集的Z分数，使用已知的均值和标准差\nfunc ZScoreWithMeanAndSD(data []float64, mean, sd float64) []float64 {\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}","copilot":" FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}","deepseek-chat-lite":" ZScore 计算数据集的Z分数","CodeQwen1.5-7B":"  ZScoreWithMeanAndSD 输入均值和标准差，计算数据集的Z分数"}},{"prefix":"package zscore\n\n// 创建一个zscore的结构","infill":"体\ntype ZScore struct {\n\t\n}\n\n// NewZScore 创建一个zscore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScore 计算数据","suffix":"集的Z分数\nfunc ZScore(data []float64) []float64 {\n\tmean, sd := calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}","relevantFile":"<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"体，保存均值和标准差\n// 可以根据这个结构体，创建一个计算zscore的函数\n\n// ZScore 计算给定数据集的Z分数\n// 返回一个切片，其中每个元素是对应 data 切片元素的Z分","CodeQwen2.5-7B":"体，包含数据集和阈值\ntype ZScore struct {\n\tData    []float64\n\tThreshold float64\n}","copilot":"体\ntype ZScore struct {\n\t\n}\n\n// NewZScore 创建一个zscore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScore 计算数据","deepseek-chat-lite":"体","CodeQwen1.5-7B":" "}},{"prefix":"package zscore\n\n// 创建一个zscore的结构体","infill":"type ZScore struct {\n\t\n}","suffix":"\n\n// NewZScore 创建一个zscore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScore 计算数据集的Z分数\nfunc ZScore(data []float64) []float64 {\n\tmean, sd := calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}","relevantFile":"<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\ntype ZScore struct {\n}","copilot":"type ZScore struct {\n\t\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package zscore\n\n// 创建一个zscore的结构体\ntype ZScore struct {\n\t\n}\n\n// NewZScore 创建一个zscore的实例","infill":"func NewZScore() *ZScore {\n\treturn &ZScore{}\n}","suffix":"\n\n// ZScore 计算数据集的Z分数\nfunc (z *ZScore) ZScore(data []float64) []float64 {\n\tmean, sd := calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}","relevantFile":"<file_path>internal/prom/metrics.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n)\n\nvar (\n\tinspectorCountTotalMetricName = \"ths_monitor_inspector_count_total\"\n\tinspectorPromCountMetricName  = \"ths_monitor_inspector_prom_count_total\"\n\tinspectorEsCountMetricName    = \"ths_monitor_inspector_es_count_total\"\n\tinspectorDelayMetricName      = \"ths_monitor_inspector_delay_time\"\n\tinspectorPromDelayMetricName  = \"ths_monitor_inspector_prom_delay_time\"\n\tinspectorEsDelayMetricName    = \"ths_monitor_inspector_es_delay_time\"\n\tinspectorPromFailedMetricName = \"ths_monitor_inspector_failed_count\"\n\n\tCountTotalMetric = promauto.NewCounter(prometheus.CounterOpts{\n\t\tName: inspectorCountTotalMetricName,\n\t})\n\n\tPromCountTotalMetric = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: inspectorPromCountMetricName,\n\t}, []string{\"status\", \"query_type\"})\n\tEsCountMetric = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: inspectorEsCountMetricName,\n\t}, []string{\"status\", \"query_type\"})\n\n\tDelayMetric = promauto.NewHistogramVec(prometheus.HistogramOpts{\n\t\tName:    inspectorDelayMetricName,\n\t\tBuckets: []float64{100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 20000, 25000, 30000, 40000, 50000, 60000, 120000},\n\t}, []string{\"stage\"})\n\n\tPromDelayMetric = promauto.NewHistogramVec(prometheus.HistogramOpts{\n\t\tName:    inspectorPromDelayMetricName,\n\t\tBuckets: []float64{100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 20000, 25000, 30000, 40000, 50000, 60000, 120000},\n\t}, []string{\"query_type\"})\n\n\tEsDelayMetric = promauto.NewHistogramVec(prometheus.HistogramOpts{\n\t\tName:    inspectorEsDelayMetricName,\n\t\tBuckets: []float64{100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 20000, 25000, 30000, 40000, 50000, 60000, 120000},\n\t}, []string{\"query_type\"})\n\n\tInspectorFailedMetric = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: inspectorPromFailedMetricName,\n\t}, []string{\"module\"})\n\n\tDBStatus = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tHelp: \"database connection status in gorm\",\n\t\tName: \"inspector_job_gorm_db_status\",\n\t}, []string{\"db_name\", \"status_key\"})\n)\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"函数","CodeQwen2.5-7B":"\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}","copilot":"func NewZScore() *ZScore {\n\treturn &ZScore{}\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package zscore\n\n// 创建一个zscore的结构体\ntype ZScore struct {\n\t\n}\n\n// NewZScore 创建一个zscore的实例\nfunc NewZScore() *ZScore {","infill":"\n\treturn &ZScore{}\n}\n\n// ZScore 计算数据集的Z分数\nfunc","suffix":" (z *ZScore) ZScore(data []float64) []float64 {\n\tmean, sd := calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}","relevantFile":"<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"z := &ZScore{}\n    return z\n}\n\n// ZScore 计算数据集的z分数\nfunc","CodeQwen2.5-7B":"\n\treturn &ZScore{}\n}","copilot":"\n\treturn &ZScore{}\n}\n\n// ZScore 计算数据集的Z分数\nfunc","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package zscore\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean flo\n}\n\n// NewZScore 创建一个zscore的实例\nfunc NewZScore() *ZScore {\n\t\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的Z分数\nfunc (z *ZScore) ZScoreFit(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// ZScoreDet ","infill":"计算数据集的Z分数\nfunc (z *ZSc","suffix":"ore) ZScoreDet(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc (z *ZScore) calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := z.ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}","relevantFile":"<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"计算数据集的Z分数\nfunc (z *ZSc","CodeQwen2.5-7B":"计算数据集的Z分数\nfunc (z *ZSc","copilot":"计算数据集的Z分数\nfunc (z *ZSc","deepseek-chat-lite":"计算数据集的Z分数","CodeQwen1.5-7B":" 检测数据集的Z分数"}},{"prefix":"package zscore\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   fl","infill":"oat64\n\tStdDev float64\n\tData   []float64\n}\n\n// N","suffix":"ewZScore 创建一个zscore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的Z分数\nfunc (z *ZScore) ZScoreFit(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc (z *ZScore) calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := z.ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}\n","relevantFile":"<file_path>internal/prom/metrics.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n)\n\nvar (\n\tinspectorCountTotalMetricName = \"ths_monitor_inspector_count_total\"\n\tinspectorPromCountMetricName  = \"ths_monitor_inspector_prom_count_total\"\n\tinspectorEsCountMetricName    = \"ths_monitor_inspector_es_count_total\"\n\tinspectorDelayMetricName      = \"ths_monitor_inspector_delay_time\"\n\tinspectorPromDelayMetricName  = \"ths_monitor_inspector_prom_delay_time\"\n\tinspectorEsDelayMetricName    = \"ths_monitor_inspector_es_delay_time\"\n\tinspectorPromFailedMetricName = \"ths_monitor_inspector_failed_count\"\n\n\tCountTotalMetric = promauto.NewCounter(prometheus.CounterOpts{\n\t\tName: inspectorCountTotalMetricName,\n\t})\n\n\tPromCountTotalMetric = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: inspectorPromCountMetricName,\n\t}, []string{\"status\", \"query_type\"})\n\tEsCountMetric = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: inspectorEsCountMetricName,\n\t}, []string{\"status\", \"query_type\"})\n\n\tDelayMetric = promauto.NewHistogramVec(prometheus.HistogramOpts{\n\t\tName:    inspectorDelayMetricName,\n\t\tBuckets: []float64{100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 20000, 25000, 30000, 40000, 50000, 60000, 120000},\n\t}, []string{\"stage\"})\n\n\tPromDelayMetric = promauto.NewHistogramVec(prometheus.HistogramOpts{\n\t\tName:    inspectorPromDelayMetricName,\n\t\tBuckets: []float64{100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 20000, 25000, 30000, 40000, 50000, 60000, 120000},\n\t}, []string{\"query_type\"})\n\n\tEsDelayMetric = promauto.NewHistogramVec(prometheus.HistogramOpts{\n\t\tName:    inspectorEsDelayMetricName,\n\t\tBuckets: []float64{100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 20000, 25000, 30000, 40000, 50000, 60000, 120000},\n\t}, []string{\"query_type\"})\n\n\tInspectorFailedMetric = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: inspectorPromFailedMetricName,\n\t}, []string{\"module\"})\n\n\tDBStatus = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tHelp: \"database connection status in gorm\",\n\t\tName: \"inspector_job_gorm_db_status\",\n\t}, []string{\"db_name\", \"status_key\"})\n)\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"o.Flat `json:\"mean\"`\n    Std    o.Flat `json:\"std\"`\n    zScores   []float64        `json:\"z_scores\"`\n}\n\n// N","CodeQwen2.5-7B":"oat64\n\tStdDev float64\n}","copilot":"oat64\n\tStdDev float64\n\tData   []float64\n}\n\n// N","deepseek-chat-lite":"oat64","CodeQwen1.5-7B":" oat64"}},{"prefix":"package zscore\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev fl","infill":"oat64\n\tData   []float64\n}\n\n// New","suffix":"ZScore 创建一个zscore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的Z分数\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\t\n\treturn zScores\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc (z *ZScore) calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := z.ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}\n","relevantFile":"<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/prom/metrics.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n)\n\nvar (\n\tinspectorCountTotalMetricName = \"ths_monitor_inspector_count_total\"\n\tinspectorPromCountMetricName  = \"ths_monitor_inspector_prom_count_total\"\n\tinspectorEsCountMetricName    = \"ths_monitor_inspector_es_count_total\"\n\tinspectorDelayMetricName      = \"ths_monitor_inspector_delay_time\"\n\tinspectorPromDelayMetricName  = \"ths_monitor_inspector_prom_delay_time\"\n\tinspectorEsDelayMetricName    = \"ths_monitor_inspector_es_delay_time\"\n\tinspectorPromFailedMetricName = \"ths_monitor_inspector_failed_count\"\n\n\tCountTotalMetric = promauto.NewCounter(prometheus.CounterOpts{\n\t\tName: inspectorCountTotalMetricName,\n\t})\n\n\tPromCountTotalMetric = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: inspectorPromCountMetricName,\n\t}, []string{\"status\", \"query_type\"})\n\tEsCountMetric = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: inspectorEsCountMetricName,\n\t}, []string{\"status\", \"query_type\"})\n\n\tDelayMetric = promauto.NewHistogramVec(prometheus.HistogramOpts{\n\t\tName:    inspectorDelayMetricName,\n\t\tBuckets: []float64{100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 20000, 25000, 30000, 40000, 50000, 60000, 120000},\n\t}, []string{\"stage\"})\n\n\tPromDelayMetric = promauto.NewHistogramVec(prometheus.HistogramOpts{\n\t\tName:    inspectorPromDelayMetricName,\n\t\tBuckets: []float64{100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 20000, 25000, 30000, 40000, 50000, 60000, 120000},\n\t}, []string{\"query_type\"})\n\n\tEsDelayMetric = promauto.NewHistogramVec(prometheus.HistogramOpts{\n\t\tName:    inspectorEsDelayMetricName,\n\t\tBuckets: []float64{100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 20000, 25000, 30000, 40000, 50000, 60000, 120000},\n\t}, []string{\"query_type\"})\n\n\tInspectorFailedMetric = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: inspectorPromFailedMetricName,\n\t}, []string{\"module\"})\n\n\tDBStatus = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tHelp: \"database connection status in gorm\",\n\t\tName: \"inspector_job_gorm_db_status\",\n\t}, []string{\"db_name\", \"status_key\"})\n)\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"oat64\n}\n\n// New","CodeQwen2.5-7B":"oat64\n}","copilot":"oat64\n\tData   []float64\n}\n\n// New","deepseek-chat-lite":"oat64","CodeQwen1.5-7B":" oat64"}},{"prefix":"package zscore\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []","infill":"float64\n}\n\n// N","suffix":"ewZScore 创建一个zscore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的Z分数\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tz.Mean = mean\n\treturn zScores\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc (z *ZScore) calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := z.ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}\n","relevantFile":"<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/prom/metrics.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n)\n\nvar (\n\tinspectorCountTotalMetricName = \"ths_monitor_inspector_count_total\"\n\tinspectorPromCountMetricName  = \"ths_monitor_inspector_prom_count_total\"\n\tinspectorEsCountMetricName    = \"ths_monitor_inspector_es_count_total\"\n\tinspectorDelayMetricName      = \"ths_monitor_inspector_delay_time\"\n\tinspectorPromDelayMetricName  = \"ths_monitor_inspector_prom_delay_time\"\n\tinspectorEsDelayMetricName    = \"ths_monitor_inspector_es_delay_time\"\n\tinspectorPromFailedMetricName = \"ths_monitor_inspector_failed_count\"\n\n\tCountTotalMetric = promauto.NewCounter(prometheus.CounterOpts{\n\t\tName: inspectorCountTotalMetricName,\n\t})\n\n\tPromCountTotalMetric = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: inspectorPromCountMetricName,\n\t}, []string{\"status\", \"query_type\"})\n\tEsCountMetric = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: inspectorEsCountMetricName,\n\t}, []string{\"status\", \"query_type\"})\n\n\tDelayMetric = promauto.NewHistogramVec(prometheus.HistogramOpts{\n\t\tName:    inspectorDelayMetricName,\n\t\tBuckets: []float64{100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 20000, 25000, 30000, 40000, 50000, 60000, 120000},\n\t}, []string{\"stage\"})\n\n\tPromDelayMetric = promauto.NewHistogramVec(prometheus.HistogramOpts{\n\t\tName:    inspectorPromDelayMetricName,\n\t\tBuckets: []float64{100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 20000, 25000, 30000, 40000, 50000, 60000, 120000},\n\t}, []string{\"query_type\"})\n\n\tEsDelayMetric = promauto.NewHistogramVec(prometheus.HistogramOpts{\n\t\tName:    inspectorEsDelayMetricName,\n\t\tBuckets: []float64{100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 20000, 25000, 30000, 40000, 50000, 60000, 120000},\n\t}, []string{\"query_type\"})\n\n\tInspectorFailedMetric = promauto.NewCounterVec(prometheus.CounterOpts{\n\t\tName: inspectorPromFailedMetricName,\n\t}, []string{\"module\"})\n\n\tDBStatus = promauto.NewGaugeVec(prometheus.GaugeOpts{\n\t\tHelp: \"database connection status in gorm\",\n\t\tName: \"inspector_job_gorm_db_status\",\n\t}, []string{\"db_name\", \"status_key\"})\n)\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"float64\n}\n\n// N","CodeQwen2.5-7B":"float64\n}","copilot":"float64\n}\n\n// N","deepseek-chat-lite":"float64","CodeQwen1.5-7B":" float64"}},{"prefix":"package zscore\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个zscore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的均值和标准差\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tz","infill":".Mean = mean\n\tz.StdDev = sd\n\tz.Data = data\n}","suffix":"\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc (z *ZScore) calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := z.ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}\n","relevantFile":"<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":".Mean = mean\n    z.StdDev = sd\n    z.Data = data","CodeQwen2.5-7B":".Mean = mean\n\tz.StdDev = sd\n}","copilot":".Mean = mean\n\tz.StdDev = sd\n\tz.Data = data\n}","deepseek-chat-lite":".Mean = mean","CodeQwen1.5-7B":" .Mean = mean"}},{"prefix":"package zscore\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个zscore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的均值和标准差\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tz.Mean = mean\n\tz.StdDev","infill":" = sd\n\tz.Data = data\n}\n\n// ZS","suffix":"coreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc (z *ZScore) calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := z.ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}\n","relevantFile":"<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"= sd\n    z.Data = data\n}\n\n// ZS","CodeQwen2.5-7B":" = sd\n\tz.Data = data\n}","copilot":" = sd\n\tz.Data = data\n}\n\n// ZS","deepseek-chat-lite":" = sd","CodeQwen1.5-7B":" = sd"}},{"prefix":"package zscore\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个zscore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的均值和标准差\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.Data","infill":" = data\n}\n\n// ","suffix":"ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc (z *ZScore) calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := z.ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}\n","relevantFile":"<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"Data = data\n}\n\n// ZScore 计算数据集的Z分数\nfunc (z *ZScore) ZScore(data []float64) []float64 {\n    zScores := make([]float64, len(data))\n    for i,","CodeQwen2.5-7B":" = data\n}","copilot":" = data\n}\n\n// ","deepseek-chat-lite":" = data","CodeQwen1.5-7B":"  = data"}},{"prefix":"package zscore\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个zscore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的","infill":"均值和标准差\nfunc (z *ZScore) Z","suffix":"ScoreFit(data []float64) {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.Data = data\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc (z *ZScore) calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := z.ZScore(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}\n","relevantFile":"<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"Z分数\nfunc (z *ZScore) Z","CodeQwen2.5-7B":"均值和标准差\nfunc (z *ZScore) Z","copilot":"均值和标准差\nfunc (z *ZScore) Z","deepseek-chat-lite":"均值和标准差","CodeQwen1.5-7B":" 均值和标准差"}},{"prefix":"package zscore\n\nimport \"gonum.org/v1/gonum","infill":"/stat\"\n\n// ZScore 创建一个ZScore的结构","suffix":"体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个ZScore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的均值和标准差\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.Data = data\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc (z *ZScore) calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) []float64 {\n\tzScores := z.ZScoreDet(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}\n","relevantFile":"<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}\n<file_path>compile.go\n// Licensed to Apache Software Foundation (ASF) under one or more contributor\n// license agreements. See the NOTICE file distributed with\n// this work for additional information regarding copyright\n// ownership. Apache Software Foundation (ASF) licenses this file to you under\n// the Apache License, Version 2.0 (the \"License\"); you may\n// not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\npackage operator\n<file_path>api.go\n// Licensed to Apache Software Foundation (ASF) under one or more contributor\n// license agreements. See the NOTICE file distributed with\n// this work for additional information regarding copyright\n// ownership. Apache Software Foundation (ASF) licenses this file to you under\n// the Apache License, Version 2.0 (the \"License\"); you may\n// not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\npackage skywalking\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"/stat\"\n\n// ZScore 计算Z分数","CodeQwen2.5-7B":"/stat\"","copilot":"/stat\"\n\n// ZScore 创建一个ZScore的结构","deepseek-chat-lite":"/stat\"","CodeQwen1.5-7B":"/stat\""}},{"prefix":"package zscore\n\nimport \"gonum.org/v1/gonum/stat\"\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个ZScore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的均值和标准差\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := stat.MeanStdDev(data, ","infill":"nil)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.D","suffix":"ata = data\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// calculateMeanAndStandardDeviation 计算数据集的均值和标准差\nfunc (z *ZScore) calculateMeanAndStandardDeviation(data []float64) (float64, float64) {\n\ttotal := 0.0\n\tfor _, val := range data {\n\t\ttotal += val\n\t}\n\tmean := total / float64(len(data))\n\n\ttotal = 0.0\n\tfor _, val := range data {\n\t\ttotal += (val - mean) * (val - mean)\n\t}\n\tsd := total / float64(len(data)-1)\n\treturn mean, sd\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) ([]float64 {\n\tzScores := z.ZScoreDet(data)\n\toutliers := make([]float64, 0)\n\tfor i, val := range zScores {\n\t\tif val > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t}\n\t}\n\treturn outliers\n}\n","relevantFile":"<file_path>logger.go\n// Licensed to Apache Software Foundation (ASF) under one or more contributor\n// license agreements. See the NOTICE file distributed with\n// this work for additional information regarding copyright\n// ownership. Apache Software Foundation (ASF) licenses this file to you under\n// the Apache License, Version 2.0 (the \"License\"); you may\n// not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\npackage log\n\ntype Logger interface {\n\tWithField(key string, value interface{}) Logger\n\t// Info logs to the INFO log.\n\tInfo(args ...interface{})\n\t// Infof logs to the INFO log.\n\tInfof(format string, args ...interface{})\n\t// Warn logs to the WARNING and INFO logs.\n\tWarn(args ...interface{})\n\t// Warnf logs to the WARNING and INFO logs.\n\tWarnf(format string, args ...interface{})\n\t// Error logs to the ERROR, WARNING, and INFO logs.\n\tError(args ...interface{})\n\t// Errorf logs to the ERROR, WARNING, and INFO logs.\n\tErrorf(format string, args ...interface{})\n}\n\n// SetLogger for connect the agent with program\nfunc SetLogger(logger Logger) {\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"nil)\n    z.Mean = mean\n    z.StdDev = sd\n    z.D","CodeQwen2.5-7B":"nil)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.D","copilot":"nil)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.D","deepseek-chat-lite":"nil)","CodeQwen1.5-7B":" nil)"}},{"prefix":"package zscore\n\nimport (\n\t\"gonum.org/v1/gonum/stat\"\n\t\"math\"\n)\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个ZScore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的均值和标准差\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := stat.MeanStdDev(data, nil)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.Data = data\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) ([]float64, []int) {\n\tzScores := z.ZScoreDet(data)\n\toutliers := make([]float64, 0)\n\toutlierIndexs := make([]int64, 0)\n\tfor i, val := range zScores {\n\t\tscore := math.Abs(","infill":"val)\n\t\tif score > threshol","suffix":"d {\n\t\t\toutliers = append(outliers, data[i])\n\t\t\toutlierIndexs = append(outlierIndexs, int64(i))\n\t\t}\n\t}\n\treturn outliers\n}\n","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"val)\n        if score > threshol","CodeQwen2.5-7B":"val)\n\t\tif score > threshol<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"val)\n\t\tif score > threshol","deepseek-chat-lite":"val)","CodeQwen1.5-7B":"val)"}},{"prefix":"package zscore\n\nimport (\n\t\"gonum.org/v1/gonum/stat\"\n\t\"math\"\n)\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个ZScore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的均值和标准差\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := stat.MeanStdDev(data, nil)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.Data = data\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tmean, sd := z.calculateMeanAndStandardDeviation(data)\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - mean) / sd\n\t}\n\treturn zScores\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) ([]float64, []int) {\n\tzScores := z.ZScoreDet(data)\n\toutliers := make([]float64, 0)\n\toutlierIndexes := make([]int, 0)\n\tfor i, val := range zScores {\n\t\tscore := math.Abs(val)\n\t\tif score > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t\toutlierIndexes","infill":" = append(outlierIndexes, i)\n\t\t}\n\t}\n\treturn ","suffix":"outliers, outlierIndexes\n}\n","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/gesd/gesd.go\npackage gesd\n\nimport (\n\t\"math\"\n)\n\nfunc calculateStandardDeviation(numbers []float64) (float64, error) {\n\ttotal := 0.0\n\tfor _, number := range numbers {\n\t\ttotal += number\n\t}\n\tmean := total / float64(len(numbers))\n\n\ttotal = 0.0\n\tfor _, number := range numbers {\n\t\ttotal += math.Pow(number-mean, 2)\n\t}\n\tvariance := total / float64(len(numbers)-1)\n\n\treturn math.Sqrt(variance), nil\n}\n\nfunc findOutliers(numbers []float64, k int) ([]float64, error) {\n\toutliers := make([]float64, 0)\n\tsd, err := calculateStandardDeviation(numbers[0:k])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i := k; i < len(numbers); i++ {\n\t\tif math.Abs(numbers[i]-numbers[i-k]) > float64(k)*sd {\n\t\t\toutliers = append(outliers, numbers[i])\n\t\t}\n\t}\n\treturn outliers, nil\n}\n\n// FindOutliers 使用gesd算法检测异常值，参数 data 为数据集，k 为窗口大小\nfunc FindOutliers(data []float64, k int) ([]float64, error) {\n\treturn findOutliers(data, k)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"append(outlierIndexes, i)\n        }\n    }\n    return ","CodeQwen2.5-7B":" = append(outlierIndexes, i)\n\t\t}\n\t}\n\treturn <|file_sep|><|fim_prefix|>/README.md\npackage zscore","copilot":" = append(outlierIndexes, i)\n\t\t}\n\t}\n\treturn ","deepseek-chat-lite":" = append(outlierIndexes, i)","CodeQwen1.5-7B":" = append(outlierIndexes, i)"}},{"prefix":"package zscore\n\n// 测试用例：根据给定数据集测试","infill":"ZScore的计算\n// TestZscore\n","suffix":"","relevantFile":"<file_path>log.go\npackage log\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\n\t\"github.com/rs/zerolog\"\n)\n\n// Logger is the global logger.\nvar Logger = zerolog.New(os.Stderr).With().Timestamp().Logger()\n\n// Output duplicates the global logger and sets w as its output.\nfunc Output(w io.Writer) zerolog.Logger {\n\treturn Logger.Output(w)\n}\n\n// With creates a child logger with the field added to its context.\nfunc With() zerolog.Context {\n\treturn Logger.With()\n}\n\n// Level creates a child logger with the minimum accepted level set to level.\nfunc Level(level zerolog.Level) zerolog.Logger {\n\treturn Logger.Level(level)\n}\n\n// Sample returns a logger with the s sampler.\nfunc Sample(s zerolog.Sampler) zerolog.Logger {\n\treturn Logger.Sample(s)\n}\n\n// Hook returns a logger with the h Hook.\nfunc Hook(h zerolog.Hook) zerolog.Logger {\n\treturn Logger.Hook(h)\n}\n\n// Err starts a new message with error level with err as a field if not nil or\n// with info level if err is nil.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Err(err error) *zerolog.Event {\n\treturn Logger.Err(err)\n}\n\n// Trace starts a new message with trace level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Trace() *zerolog.Event {\n\treturn Logger.Trace()\n}\n\n// Debug starts a new message with debug level.\n//\n// You must call Msg on the returned event in order to send the event.\nfunc Debug() *zerolog.Event {\n\treturn Logger.Debug()\n}\n<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/dynamic/zscore/zscore.go\npackage zscore\n\nimport (\n\t\"gonum.org/v1/gonum/stat\"\n\t\"math\"\n)\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个ZScore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的均值和标准差\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := stat.MeanStdDev(data, nil)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.Data = data\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - z.Mean) / z.StdDev\n\t}\n\treturn zScores\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) ([]float64, []int) {\n\tzScores := z.ZScoreDet(data)\n\toutliers := make([]float64, 0)\n\toutlierIndexes := make([]int, 0)\n\tfor i, val := range zScores {\n\t\tscore := math.Abs(val)\n\t\tif score > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t\toutlierIndexes = append(outlierIndexes, i)\n\t\t}\n\t}\n\treturn outliers, outlierIndexes\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore_test.go","template":"go","multiRes":{"hipilot":"zscore标准化算法的正确性。","CodeQwen2.5-7B":"函数\nfunc TestZScore(data []float64) {\n\t// 计算 Z-Score\n\tzScores := ZScore(data)","copilot":"ZScore的计算\n// TestZscore\n","deepseek-chat-lite":"Z-Score","CodeQwen1.5-7B":" "}},{"prefix":"package zscore\n\nimport \"testing\"\n\n// 测试用例：根据给定数据集测试ZScore的计算\n// TestZScoreFind","infill":"Outliers 测试ZScore的FindOutliers方法\nfunc TestZScoreFindOutliers(t *testing.T) {\n\t// 创建一个ZScore实例\n\tz := NewZScore()\n\t// 设置数据集\n\tdata := []float64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\t// 计算数据集的均值和标准差\n\tz.ZScoreFit(data)\n\t// 设置阈值\n\tthreshold := 2.0\n\t// 检测异常值\n\toutliers, _ := z.FindOutliers(data, threshold)\n\t// 预期结果\n\texpected := []float64{9, 10}\n\t// 检查结果是否符合预期\n\tfor i, val := range outliers {\n\t\tif val != expected[i] {\n\t\t\tt.Errorf(\"Test failed, expected: %v, got: %v\", expected, outliers)\n\t\t}\n\t}\n}\n","suffix":"","relevantFile":"<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>internal/dynamic/zscore/zscore.go\npackage zscore\n\nimport (\n\t\"gonum.org/v1/gonum/stat\"\n\t\"math\"\n)\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个ZScore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的均值和标准差\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := stat.MeanStdDev(data, nil)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.Data = data\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - z.Mean) / z.StdDev\n\t}\n\treturn zScores\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) ([]float64, []int) {\n\tzScores := z.ZScoreDet(data)\n\toutliers := make([]float64, 0)\n\toutlierIndexes := make([]int, 0)\n\tfor i, val := range zScores {\n\t\tscore := math.Abs(val)\n\t\tif score > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t\toutlierIndexes = append(outlierIndexes, i)\n\t\t}\n\t}\n\treturn outliers, outlierIndexes\n}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore_test.go","template":"go","multiRes":{"hipilot":"测试ZScore的计算","CodeQwen2.5-7B":"ZScore 测试ZScore的计算\nfunc TestZScoreFindZScore(t *testing.T) {\n\t// 测试用例\n\ttestCases := []struct {\n\t\tdata    []float64\n\t\tzScores []float64\n\t}{\n\t\t{\n\t\t\tdata:    []float64{1, 2, 3, 4, 5},\n\t\t\tzScores: []float64{-1.4142135623730951, -0.7071067811865475, 0, 0.7071067811865475, 1.4142135623730951},\n\t\t},\n\t\t{\n\t\t\tdata:    []float64{10, 20, 30, 40, 50},\n\t\t\tzScores: []float64{-1.4142135623730951, -0.7071067811865475, 0, 0.7071067811865475, 1.4142135623730951},\n\t\t},\n\t\t","copilot":"Outliers 测试ZScore的FindOutliers方法\nfunc TestZScoreFindOutliers(t *testing.T) {\n\t// 创建一个ZScore实例\n\tz := NewZScore()\n\t// 设置数据集\n\tdata := []float64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\t// 计算数据集的均值和标准差\n\tz.ZScoreFit(data)\n\t// 设置阈值\n\tthreshold := 2.0\n\t// 检测异常值\n\toutliers, _ := z.FindOutliers(data, threshold)\n\t// 预期结果\n\texpected := []float64{9, 10}\n\t// 检查结果是否符合预期\n\tfor i, val := range outliers {\n\t\tif val != expected[i] {\n\t\t\tt.Errorf(\"Test failed, expected: %v, got: %v\", expected, outliers)\n\t\t}\n\t}\n}\n","deepseek-chat-lite":" 测试ZScore的计算","CodeQwen1.5-7B":" "}},{"prefix":"package zscore\n\nimport \"testing\"\n\n// 测试用例：根据给定数据集测试ZScore的计算\n// TestZScoreFindOutliers 测试ZScore的FindOutliers方法\nfunc TestZScoreFindOutliers(t *testing.T) {\n\t// 创建一个ZScore实例\n\tz := NewZScore()\n\t// 设置数据集\n\tdata := []float64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\t// 计算数据集的均值和标准差\n\tz.ZScoreFit(data)\n\t// 设置阈值\n\tthreshold := 2.0\n\n\tcData := []float64{1, 2, 300, 4, 5, 6, 7, 8, 9, 10}\n\t// 检测异常值\n\toutliers, indexes := z.FindOutliers(cData, threshold)\n\t// 预期结果\n\texpectedOutliers := []float64{300}\n\texpectedIndexes := []int{2}\n\t// 检查结果是否符合预期\n\tfor i, val := range outliers {\n\t\tindex := indexes[i]\n\t\tif val != expectedOutliers[i] || index !","infill":"= expectedIndexes[i] {\n\t\t\tt.Errorf(\"Test failed, expectedOutliers: %v, g","suffix":"ot: %v\", expectedOutliers, outliers)\n\t\t}\n\t}\n}\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"flag\"\n\t\"fmt\"\n\t\"github.com/rs/zerolog\"\n\t\"github.com/rs/zerolog/log\"\n\t\"github.com/spf13/viper\"\n\tsentinel \"github.com/sunduoyou/sentinel-golang/api\"\n\t\"github.com/sunduoyou/sentinel-golang/core/flow\"\n\t\"gopkg.in/natefinch/lumberjack.v2\"\n\t\"io\"\n\t\"net/http\"\n\t_ \"net/http/pprof\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"ths-monitor-calculate-job/internal/application\"\n\t\"ths-monitor-calculate-job/internal/clickhouseclient\"\n\t\"ths-monitor-calculate-job/internal/config\"\n\t\"ths-monitor-calculate-job/internal/consts\"\n\t\"ths-monitor-calculate-job/internal/esclient\"\n\t\"ths-monitor-calculate-job/internal/k8s\"\n\t\"ths-monitor-calculate-job/internal/prom\"\n\t\"ths-monitor-calculate-job/internal/redis\"\n)\n\nconst PprofPort = \"127.0.0.1:5050\"\n\nfunc main() {\n\n\t// 开启pprof\n\tgo func() {\n\t\t_ = http.ListenAndServe(PprofPort, nil)\n\t}()\n\t// Initialize flags\n\tconfigPath := flag.String(\"config\", \"calculate-dev.yml\", \"Configuration file\")\n\tflag.Parse()\n\tv, err := ReadInConfig(*configPath)\n\tif err != nil {\n\t\tfmt.Printf(\"fail to read configuration: %v ,exit...\", err)\n\t\treturn\n\t}\n\t// 初始化日志配置\n\tInitLogConfig(v)\n\t// 初始化sentinel的配置\n\tInitSentinel(v)\n\t// 初始化redis\n\tInitRedis(v)\n\t// 初始化esClient\n\tInitEsClient(v)\n\t// 初始化prom\n\tInitProm(v)\n\t// 初始化clickhouse\n\tInitClickhouse(v)\n\t// 初始化k8s meta cache\n\tk8s.InitK8sCache()\n\tapp, err := application.New(v)\n\tif err != nil {\n\t\tlog.Err(err).Msg(\"application new failed!\")\n<file_path>internal/httpserver/httpserver_test.go\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/calculate\", compareReqBytes, 100*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestNewHttpServer(t *testing.T) {\n\tworkloadName := \"hexin-apm-hxredis\"\n\tindex := strings.LastIndexByte(workloadName, '-')\n\tif index > -1 {\n\t\tfmt.Println(workloadName[index+1:])\n\t}\n}\n\nfunc TestSyncWorkloadConfSync(t *testing.T) {\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/dynamic/zscore/zscore.go\npackage zscore\n\nimport (\n\t\"gonum.org/v1/gonum/stat\"\n\t\"math\"\n)\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个ZScore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的均值和标准差\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := stat.MeanStdDev(data, nil)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.Data = data\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - z.Mean) / z.StdDev\n\t}\n\treturn zScores\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) ([]float64, []int) {\n\tzScores := z.ZScoreDet(data)\n\toutliers := make([]float64, 0)\n\toutlierIndexes := make([]int, 0)\n\tfor i, val := range zScores {\n\t\tscore := math.Abs(val)\n\t\tif score > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t\toutlierIndexes = append(outlierIndexes, i)\n\t\t}\n\t}\n\treturn outliers, outlierIndexes\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore_test.go","template":"go","multiRes":{"hipilot":"expectedIndexes[i] {\n            t.Errorf(\"Expected: %v, g","CodeQwen2.5-7B":"= expectedIndexes[i] {\n\t\t\tt.Errorf(\"Expected outliers: %v, got: %v\", expectedOutliers, outliers)\n\t\t\tt.Errorf(\"Expected indexes: %v, got: %v\", expectedIndexes, indexes)\n\t\t}\n\t}\n}","copilot":"= expectedIndexes[i] {\n\t\t\tt.Errorf(\"Test failed, expectedOutliers: %v, g","deepseek-chat-lite":"= expectedIndexes[i] {","CodeQwen1.5-7B":"= expectedIndexes[i] {"}},{"prefix":"package zscore\n\nimport \"testing\"\n\n// 测试用例：根据给定数据集测试ZScore的计算\n// TestZScoreFindOutliers 测试ZScore的FindOutliers方法\nfunc TestZScoreFindOutliers(t *testing.T) {\n\t// 创建一个ZScore实例\n\tz := NewZScore()\n\t// 设置数据集\n\tdata := []float64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\t// 计算数据集的均值和标准差\n\tz.ZScoreFit(data)\n\t// 设置阈值\n\tthreshold := 2.0\n\n\tcData := []float64{1, 2, 300, 4, 5, 6, 7, 8, 9, 10}\n\t// 检测异常值\n\toutliers, indexes := z.FindOutliers(cData, threshold)\n\t// 预期结果\n\texpectedOutliers := []float64{300}\n\texpectedIndexes := []float64{2}\n\t// 检查结果是否符合预期\n\tfor i, val := range outliers {\n\t\tindex := ","infill":"indexes[i]\n\t\tif val !","suffix":"= expectedOutliers[i] || index != expectedIndexes[i] {\n\t\t\tt.Errorf(\"Test failed, expected: %v, got: %v\", expectedOutliers, outliers)\n\t\t}\n\t}\n}\n","relevantFile":"<file_path>main.go\npackage main\n\nimport (\n\t\"flag\"\n\t\"fmt\"\n\t\"github.com/rs/zerolog\"\n\t\"github.com/rs/zerolog/log\"\n\t\"github.com/spf13/viper\"\n\tsentinel \"github.com/sunduoyou/sentinel-golang/api\"\n\t\"github.com/sunduoyou/sentinel-golang/core/flow\"\n\t\"gopkg.in/natefinch/lumberjack.v2\"\n\t\"io\"\n\t\"net/http\"\n\t_ \"net/http/pprof\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"ths-monitor-calculate-job/internal/application\"\n\t\"ths-monitor-calculate-job/internal/clickhouseclient\"\n\t\"ths-monitor-calculate-job/internal/config\"\n\t\"ths-monitor-calculate-job/internal/consts\"\n\t\"ths-monitor-calculate-job/internal/esclient\"\n\t\"ths-monitor-calculate-job/internal/k8s\"\n\t\"ths-monitor-calculate-job/internal/prom\"\n\t\"ths-monitor-calculate-job/internal/redis\"\n)\n\nconst PprofPort = \"127.0.0.1:5050\"\n\nfunc main() {\n\n\t// 开启pprof\n\tgo func() {\n\t\t_ = http.ListenAndServe(PprofPort, nil)\n\t}()\n\t// Initialize flags\n\tconfigPath := flag.String(\"config\", \"calculate-dev.yml\", \"Configuration file\")\n\tflag.Parse()\n\tv, err := ReadInConfig(*configPath)\n\tif err != nil {\n\t\tfmt.Printf(\"fail to read configuration: %v ,exit...\", err)\n\t\treturn\n\t}\n\t// 初始化日志配置\n\tInitLogConfig(v)\n\t// 初始化sentinel的配置\n\tInitSentinel(v)\n\t// 初始化redis\n\tInitRedis(v)\n\t// 初始化esClient\n\tInitEsClient(v)\n\t// 初始化prom\n\tInitProm(v)\n\t// 初始化clickhouse\n\tInitClickhouse(v)\n\t// 初始化k8s meta cache\n\tk8s.InitK8sCache()\n\tapp, err := application.New(v)\n\tif err != nil {\n\t\tlog.Err(err).Msg(\"application new failed!\")\n<file_path>internal/httpserver/httpserver_test.go\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/calculate\", compareReqBytes, 100*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestNewHttpServer(t *testing.T) {\n\tworkloadName := \"hexin-apm-hxredis\"\n\tindex := strings.LastIndexByte(workloadName, '-')\n\tif index > -1 {\n\t\tfmt.Println(workloadName[index+1:])\n\t}\n}\n\nfunc TestSyncWorkloadConfSync(t *testing.T) {\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/dynamic/zscore/zscore.go\npackage zscore\n\nimport (\n\t\"gonum.org/v1/gonum/stat\"\n\t\"math\"\n)\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个ZScore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的均值和标准差\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := stat.MeanStdDev(data, nil)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.Data = data\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - z.Mean) / z.StdDev\n\t}\n\treturn zScores\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) ([]float64, []int) {\n\tzScores := z.ZScoreDet(data)\n\toutliers := make([]float64, 0)\n\toutlierIndexes := make([]int, 0)\n\tfor i, val := range zScores {\n\t\tscore := math.Abs(val)\n\t\tif score > threshold {\n\t\t\toutliers = append(outliers, data[i])\n\t\t\toutlierIndexes = append(outlierIndexes, i)\n\t\t}\n\t}\n\treturn outliers, outlierIndexes\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore_test.go","template":"go","multiRes":{"hipilot":"indexes[i]\n        if val !","CodeQwen2.5-7B":"indexes[i]\n\t\tif val !","copilot":"indexes[i]\n\t\tif val !","deepseek-chat-lite":"indexes[i]","CodeQwen1.5-7B":" indexes[i]"}},{"prefix":"package zscore\n\nimport (\n\t\"gonum.org/v1/gonum/stat\"\n\t\"math\"\n)\n\n// ZScore 创建一个ZScore的结构体\ntype ZScore struct {\n\tMean   float64\n\tStdDev float64\n\tData   []float64\n}\n\n// NewZScore 创建一个ZScore的实例\nfunc NewZScore() *ZScore {\n\treturn &ZScore{}\n}\n\n// ZScoreFit 计算数据集的均值和标准差\nfunc (z *ZScore) ZScoreFit(data []float64) {\n\tmean, sd := stat.MeanStdDev(data, nil)\n\tz.Mean = mean\n\tz.StdDev = sd\n\tz.Data = data\n}\n\n// ZScoreDet 计算数据集的Z分数\nfunc (z *ZScore) ZScoreDet(data []float64) []float64 {\n\tzScores := make([]float64, len(data))\n\tfor i, val := range data {\n\t\tzScores[i] = (val - z.Mean) / z.StdDev\n\t}\n\treturn zScores\n}\n\n// FindOutliers 使用Z分数检测异常值，参数 data 为数据集，threshold 为阈值\nfunc (z *ZScore) FindOutliers(data []float64, threshold float64) ([]float64, []float64, []int) {\n\tzScores := z.ZScoreDet(data)\n\toutliers := make([]float64, 0)\n\toutlierScores := make([]float64, 0)\n\toutlierIndexes := make([]int, 0)\n\tfor i, val := range zScores {\n\t\tscore := math.Abs(val)\n\t\tif score > threshold {\n\t\t\toutlierScores","infill":" = append(outlierScores, score)\n\t\t\toutliers = a","suffix":"ppend(outliers, data[i])\n\t\t\toutlierIndexes = append(outlierIndexes, i)\n\t\t}\n\t}\n\treturn outliers, outlierScores, outlierIndexes\n}\n","relevantFile":"<file_path>internal/httpserver/httpserver_test.go\npackage httpserver\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"ths-monitor-calculate-job/internal/module\"\n\t\"ths-monitor-calculate-job/internal/util\"\n\t\"time\"\n)\n\nfunc TestSyncExprConfig(t *testing.T) {\n\tvar stackExprConfig []module.StackExprConfig\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"http平均响应时间\",\n\t\tExpr:             \"sum(rate(http_server_requests_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name) / sum(rate(http_server_requests_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\",uri!~\\\".*actuator.*\\\",uri!~\\\".*/readiness\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tstackExprConfig = append(stackExprConfig, module.StackExprConfig{\n\t\tExprName:         \"java平均gc时间\",\n\t\tExpr:             \"sum(rate(jvm_gc_pause_seconds_sum{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)/sum(rate(jvm_gc_pause_seconds_count{pod_name=~\\\"($deploy(-grayscale)?-[^-]+(-[^-][^-][^-][^-]+)?)\\\"}[$range])) by (pod_name)\",\n\t\tThreshold:        0.01,\n\t\tCompareThreshold: 1.5,\n\t\tCompareExpr:      \">\",\n\t})\n\tvar stackExpr []module.StackExpr\n\tstackExpr = append(stackExpr, module.StackExpr{\n\t\tStack:       \"java\",\n\t\tStackConfig: stackExprConfig,\n\t})\n\tstackExprBytes, err := json.Marshal(stackExpr)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n\trespByte, err := util.HttpPost(\"http://localhost/api/v1/sync_expr_conf\", stackExprBytes, 10*time.Second)\n\tif err != nil {\n\t\tt.Error(\"http post failed !\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"resp: %s\", string(respByte))\n}\n\nfunc TestCalculateCompare(t *testing.T) {\n\tcompareReq := module.CompareReq{\n\t\tCluster:        \"prod\",\n\t\tWorkloadName:   \"hexin-apm-open-api\",\n\t\tNamespace:      \"default\",\n\t\tPreTradeDaySec: 1680833907,\n\t\tStartTimeSec:   1680834123,\n\t\tEndTimeSec:     1680834123,\n\t}\n\tcompareReqBytes, err := json.Marshal(compareReq)\n\tif err != nil {\n\t\tt.Error(\"json marshal failed !\", err)\n\t\treturn\n\t}\n<file_path>internal/cache/cache_test.go\npackage cache\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestCache(t *testing.T) {\n\tvar cacheKey = \"test-key\"\n\terr := Cache.SetString(cacheKey, \"test-value\")\n\tif err != nil {\n\t\tt.Errorf(\"set cache val failed , err is %v\", err)\n\t}\n\tval, err := Cache.GetString(cacheKey)\n\tif err != nil {\n\t\tt.Errorf(\"get cache val failed , err is %v\", err)\n\t}\n\tfmt.Println(val)\n}\n<file_path>internal/dynamic/gesd/gesd_test.go\npackage gesd\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGesd(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := findOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n\n// 创建一个gesd异常检测的测试用例\nfunc TestFindOutliers(t *testing.T) {\n\tdata := []float64{10, 5, 8, 12, 15, 30, 20, 17, 9, 22, 25, 28, 30, 15, 32, 18, 9, 20, 22}\n\tk := 5\n\toutliers, err := FindOutliers(data, k)\n\tif err != nil {\n\t\tfmt.Println(\"Error calculating outliers:\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n}\n<file_path>internal/dynamic/zscore/zscore_test.go\npackage zscore\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n)\n\n// 测试用例：根据给定数据集测试ZScore的计算\n// TestZScoreFindOutliers 测试ZScore的FindOutliers方法\nfunc TestZScoreFindOutliers(t *testing.T) {\n\t// 创建一个ZScore实例\n\tz := NewZScore()\n\t// 设置数据集\n\tdata := []float64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\t// 计算数据集的均值和标准差\n\tz.ZScoreFit(data)\n\t// 设置阈值\n\tthreshold := 3.0\n\n\t// 待检测的数据集\n\tcData := []float64{1, 2, 50, 4, 5, 6, 7, 8, 9, 10}\n\t// 检测异常值\n\toutliers, outindexes := z.FindOutliers(cData, threshold)\n\t// 预期结果\n\texpectedOutliers := []float64{300}\n\texpectedIndexes := []int{2}\n\t// 检查结果是否符合预期\n\tfor i, val := range outliers {\n\t\tindex := indexes[i]\n\t\tif val != expectedOutliers[i] || index != expectedIndexes[i] {\n\t\t\tt.Errorf(\"Test failed, expectedOutliers: %v, got: %v\", expectedOutliers, outliers)\n\t\t}\n\t}\n\tfmt.Println(\"Outliers:\", outliers)\n\tfmt.Println(\"Indexes:\", indexes)\n}","relevantFileList":[],"filePath":"internal/dynamic/zscore/zscore.go","template":"go","multiRes":{"hipilot":"a","CodeQwen2.5-7B":" = append(outlierScores, score)\n\t\t\toutliers = a","copilot":" = append(outlierScores, score)\n\t\t\toutliers = a","deepseek-chat-lite":" = append(outlierScores, score)","CodeQwen1.5-7B":" = append(outlierScores, score)"}},{"prefix":"package onlineconsumer\n\n// Config 创建配置","infill":"结构体\ntype Config struct {\n\tKafkaConfig KafkaConfig `mapstructure:\"kafka\"`\n}\n","suffix":"","relevantFile":"<file_path>internal/config/redisconfig.go\npackage config\n\ntype RedisConfig struct {\n\tAddr     string `mapstructure:\"addr\"`\n\tPassword string `mapstructure:\"password\"`\n\tPoolSize int    `mapstructure:\"pool_size\"`\n}\n<file_path>internal/util/stringutil.go\npackage util\n\nfunc GetFixedLengthStr(str string, length int) string {\n\tif len(str) > length {\n\t\treturn str[0:length] + \"...\"\n\t}\n\treturn str\n}\n<file_path>internal/onlineconsumer/onlineconsumer.go\npackage onlineconsumer\n\nimport \"github.com/spf13/viper\"\n\n// OnlineConsumer 创建OnlineConsumer结构体\ntype OnlineConsumer struct {\n}\n\n// NewOnlineConsumer 创建消费者实例\nfunc NewOnlineConsumer(v *viper.Viper) *OnlineConsumer {\n\treturn &OnlineConsumer{}\n}\n<file_path>internal/prom/config.go\npackage prom\n\ntype Config struct {\n\tPort string `mapstructure:\"port\"`\n}","relevantFileList":[],"filePath":"internal/onlineconsumer/config.go","template":"go","multiRes":{"hipilot":"中心\ntype Config struct {\n}\n","CodeQwen2.5-7B":"\ntype Config struct {\n\t// 消费者组\n\tGroup string\n\t// 消费者ID\n\tID string\n\t// 消费者地址\n\tAddr string\n\t// 消费者地址\n\tTopic string\n\t// 消费者地址\n\tPartition int32\n\t// 消费者地址\n\tOffset int64\n}","copilot":"结构体\ntype Config struct {\n\tKafkaConfig KafkaConfig `mapstructure:\"kafka\"`\n}\n","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package onlineconsumer\n\n// Config 创建配置结构体\nty","infill":"pe Config struct {\n\tKafkaConfig KafkaConfig `mapstructure:\"kafkaconfig\"`\n}\n","suffix":"","relevantFile":"<file_path>internal/config/redisconfig.go\npackage config\n\ntype RedisConfig struct {\n\tAddr     string `mapstructure:\"addr\"`\n\tPassword string `mapstructure:\"password\"`\n\tPoolSize int    `mapstructure:\"pool_size\"`\n}\n<file_path>internal/util/stringutil.go\npackage util\n\nfunc GetFixedLengthStr(str string, length int) string {\n\tif len(str) > length {\n\t\treturn str[0:length] + \"...\"\n\t}\n\treturn str\n}\n<file_path>internal/onlineconsumer/onlineconsumer.go\npackage onlineconsumer\n\nimport \"github.com/spf13/viper\"\n\n// OnlineConsumer 创建OnlineConsumer结构体\ntype OnlineConsumer struct {\n}\n\n// NewOnlineConsumer 创建消费者实例\nfunc NewOnlineConsumer(v *viper.Viper) *OnlineConsumer {\n\treturn &OnlineConsumer{}\n}\n<file_path>internal/prom/config.go\npackage prom\n\ntype Config struct {\n\tPort string `mapstructure:\"port\"`\n}","relevantFileList":[],"filePath":"internal/onlineconsumer/config.go","template":"go","multiRes":{"hipilot":"type Config struct {\n}\n","CodeQwen2.5-7B":"pe Config struct {\n\t// 消费者组名称\n\tGroup string\n\t// 消费者名称\n\tName string\n\t// 消费者地址\n\tAddr string\n\t// 消费者端口\n\tPort int\n\t// 消费者日志级别\n\tLogLevel string\n\t// 消费者日志文件\n\tLogFile string\n\t// 消费者日志文件大小\n\tLogSize int\n\t// 消费者日志文件个数\n\tLogNum int\n\t// 消费者日志文件路径\n\tLogPath string\n\t// 消费者日志文件名\n\tLogName string\n\t// 消费者日志文件后缀\n\tLogSuffix string\n\t// 消费者日志文件编码\n\tLogEncoding string\n\t// 消费者日志文件格式\n\tLogFormat string\n\t// 消费者日志文件时间格式\n\tLogTimeFormat string\n\t// 消费者日志文件时间间隔\n\tLogTimeInterval int\n\t// 消费者日志文件时间间隔单位\n\tLogTimeIntervalUnit string\n\t// 消费者日志文件时间间隔单位\n\tLogTimeIntervalUnit string\n\t// 消费者日志文件时间间隔单位\n\tLogTimeIntervalUnit string\n\t// 消费者日志文件时间间隔单位\n\tLogTimeIntervalUnit string\n\t// 消费者日","copilot":"pe Config struct {\n\tKafkaConfig KafkaConfig `mapstructure:\"kafkaconfig\"`\n}\n","deepseek-chat-lite":"pe Config struct {","CodeQwen1.5-7B":" pe Config struct {"}},{"prefix":"package onlineconsumer\n\n// Config 创建配置结构体\ntype Config struct {\n\tKafkaConfig ","infill":"KafkaConfig `mapstructure:\"kafkaconfig\"`\n}\n","suffix":"","relevantFile":"<file_path>internal/config/loggerconfig.go\npackage config\n\ntype LoggerConfig struct {\n\tProfile           string `mapstructure:\"profile\"`\n\tLogFileName       string `mapstructure:\"log_file_name\"`\n\tLogFileMaxSize    int    `mapstructure:\"log_file_max_size\"`\n\tLogFileMaxBackups int    `mapstructure:\"log_file_max_backups\"`\n\tLogFileMaxAge     int    `mapstructure:\"log_file_max_age\"`\n\tLogLevel          string `mapstructure:\"log_level\"`\n}\n<file_path>internal/config/redisconfig.go\npackage config\n\ntype RedisConfig struct {\n\tAddr     string `mapstructure:\"addr\"`\n\tPassword string `mapstructure:\"password\"`\n\tPoolSize int    `mapstructure:\"pool_size\"`\n}\n<file_path>internal/onlineconsumer/onlineconsumer.go\npackage onlineconsumer\n\nimport \"github.com/spf13/viper\"\n\n// OnlineConsumer 创建OnlineConsumer结构体\ntype OnlineConsumer struct {\n}\n\n// NewOnlineConsumer 创建消费者实例\nfunc NewOnlineConsumer(v *viper.Viper) *OnlineConsumer {\n\treturn &OnlineConsumer{}\n}\n<file_path>internal/prom/config.go\npackage prom\n\ntype Config struct {\n\tPort string `mapstructure:\"port\"`\n}","relevantFileList":[],"filePath":"internal/onlineconsumer/config.go","template":"go","multiRes":{"hipilot":"string `mapstructure:\"kafka_config\" json:\"kafka_config\" yaml:\"kafka_config\"`\n    TopicName  string `mapstructure:\"topic_name\" json:\"topic_name\" yaml:\"topic_name\"`\n    GroupName  string","CodeQwen2.5-7B":"KafkaConfig\n\tRedisConfig RedisConfig\n}","copilot":"KafkaConfig `mapstructure:\"kafkaconfig\"`\n}\n","deepseek-chat-lite":"*KafkaConfig","CodeQwen1.5-7B":" KafkaConfig `json:\"kafka_config\"`"}},{"prefix":"package onlineconsumer\n\nimport (\n\t\"context\"\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/rs/zerolog/log\"\n\t\"github.com/spf13/viper\"\n\t\"sync\"\n)\n\n// OnlineConsumer 创建OnlineConsumer结构体\ntype OnlineConsumer struct {\n\tcfg               *Config\n\tshutdownWG        sync.WaitGroup\n\tshutdownState     bool\n\tconsumerGroup     sarama.ConsumerGroup\n\ttopics            []string\n\tcancelConsumeLoop context.CancelFunc\n\tautocommitEnabled bool\n}\n\n// NewOnlineConsumer 创建消费者实例\nfunc NewOnlineConsumer(v *viper.Viper) *OnlineConsumer {\n\tcfg, ok := getConfig(v)\n\tif !ok {\n\t\tlog.Panic().Msgf(\"Cannot convert [%s] config\", Kafka)\n\t\treturn nil\n\t}\n\tclient, err := getConsumerGroup(cfg.KafkaConfig)\n\tif err != nil {\n\t\tlog.Panic().Msgf(\"Cannot create [%s] consumer\", Kafka)\n\t\treturn nil\n\t}\n\treturn &OnlineConsumer{\n\t\tcfg:               cfg,\n\t\tconsumerGroup:     client,\n\t\ttopics:            cfg.KafkaConfig.Topics,\n\t\tautocommitEnabled: cfg.KafkaConfig.AutocommitEnabled,\n\t}\n}\n\nfunc getConfig(v *viper.Viper) (*Config, bool) {","infill":"var cfg Config\n\tif err := v.UnmarshalKey(&cfg); err != nil {\n\t\tlog.Panic().Msgf(\"Cannot convert [%s] config\", Kafka)\n\t\treturn nil, false\n\t}\n\treturn &cfg, true\n","suffix":"}\n\nfunc getConsumerGroup(config *KafkaConfig) (sarama.ConsumerGroup, error) {\n\tkafkaCfg := DefaultConfig()\n\tkafkaCfg.Hosts = config.Hosts\n\tif config.MaxMessageBytes != nil {\n\t\tkafkaCfg.MaxMessageBytes = config.MaxMessageBytes\n\t}\n\tif config.RequiredACKs != nil {\n\t\tkafkaCfg.RequiredACKs = config.RequiredACKs\n\t}\n\tif config.Compression != \"\" {\n\t\tkafkaCfg.Compression = config.Compression\n\t}\n\n\tif config.BulkMaxSize > 0 {\n\t\tkafkaCfg.BulkMaxSize = config.BulkMaxSize\n\t}\n\tif config.MaxRetries > 0 {\n\t\tkafkaCfg.MaxRetries = config.MaxRetries\n\t}\n\tif config.ClientID != \"\" {\n\t\tkafkaCfg.ClientID = config.ClientID\n\t}\n\tif config.ChanBufferSize > 0 {\n\t\tkafkaCfg.ChanBufferSize = config.ChanBufferSize\n\t}\n\tif config.SaslEnable {\n\t\tkafkaCfg.Username = config.Username\n\t\tkafkaCfg.Password = config.Password\n\t\tif config.SaslType == \"\" {\n\t\t\tkafkaCfg.Sasl.SaslMechanism = sarama.SASLTypePlaintext\n\t\t} else {\n\t\t\tkafkaCfg.Sasl.SaslMechanism = config.SaslType\n\t\t}\n\t}\n\tkafkaCfg.AutocommitEnabled = config.AutocommitEnabled\n\tcfg, err := NewSaramaConfig(&kafkaCfg)\n\tif err != nil {\n\t\ttelemetry.Logger.Sugar().Panicf(\"Cannot create [%s] config\", Kafka)\n\t}\n\treturn sarama.NewConsumerGroup(config.Hosts, config.GroupId, cfg)\n}\n","relevantFile":"<file_path>internal/util/timeutil.go\npackage util\n\nimport (\n\t\"github.com/rs/zerolog/log\"\n\t\"time\"\n)\n\nfunc GetTimeDuration(duration string) time.Duration {\n\tdur, err := time.ParseDuration(duration)\n\tif err != nil {\n\t\tlog.Err(err).Msgf(\"duration parse failed ! return default 1 hour; durationStr: %s\", duration)\n\t\treturn time.Duration(1) * time.Minute\n\t}\n\treturn dur\n}\n<file_path>internal/prom/prom.go\npackage prom\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/zerolog/log\"\n\t\"net/http\"\n)\n\nfunc InitProm(config Config) {\n\tgo StartPromServer(config.Port)\n}\n\nfunc StartPromServer(port string) {\n\tserveMux := http.ServeMux{}\n\tserveMux.Handle(\"/metrics\", promhttp.Handler())\n\tsrv := http.Server{\n\t\tAddr:    port,\n\t\tHandler: &serveMux,\n\t}\n\n\tlog.Info().Msgf(\"Prometheus Server listening at port: [%s]\", port)\n\terr := srv.ListenAndServe()\n\n\tif err != nil && err != http.ErrServerClosed {\n\t\tlog.Err(err).Msg(\"Prometheus Server listening failed!\")\n\t}\n\tlog.Info().Msgf(\"Prometheus gracefully shutdown the http server...\\n\")\n}\n<file_path>internal/util/viper.go\npackage util\n\nimport (\n\t\"fmt\"\n\t\"github.com/spf13/viper\"\n)\n\n// ReadInConfig 将配置读取到viper中\nfunc ReadInConfig(path string) (*viper.Viper, error) {\n\tv := viper.New()\n\tv.SetConfigFile(path)\n\tv.SetConfigType(\"yaml\")\n\terr := v.ReadInConfig()\n\tif err != nil { // Handle errors reading the config file\n\t\treturn nil, fmt.Errorf(\"error happened while reading config file: %v\", err)\n\t}\n\treturn v, nil\n}\n<file_path>internal/onlineconsumer/kafkareceiver.go\npackage onlineconsumer\n\nimport (\n\t\"context\"\n\t\"github.com/Shopify/sarama\"\n\t\"go.uber.org/zap\"\n\t\"go.uber.org/zap/zapcore\"\n\t\"sync\"\n\t\"time\"\n)\n\nconst (\n\tKafka = \"kafkareceiver\"\n)\n\n\nfunc NewKafkaReceiver(config interface{}, telemetry *component.TelemetryTools, analyzerManager analyzerpackage.Manager) receiver.Receiver {\n\tcfg, ok := config.(*Config)\n\tif !ok {\n\t\ttelemetry.Logger.Sugar().Panicf(\"Cannot convert [%s] config\", Kafka)\n\t}\n\tclient, err := getConsumerGroup(cfg, telemetry)\n\tif err != nil {\n\t\ttelemetry.Logger.Sugar().Panicf(\"Cannot create [%s] consumer\", Kafka)\n\t}\n\treturn &KafkaReceiver{\n\t\tcfg:               cfg,\n\t\tconsumerGroup:     client,\n\t\ttopics:            cfg.Topics,\n\t\tautocommitEnabled: cfg.AutocommitEnabled,\n\t\tanalyzerManager:   analyzerManager,\n\t\ttelemetry:         telemetry,\n\t}\n}\n\nfunc getConsumerGroup(config *Config, telemetry *component.TelemetryTools) (sarama.ConsumerGroup, error) {\n\tkafkaCfg := DefaultConfig()\n\tkafkaCfg.Hosts = config.Hosts\n\tif config.MaxMessageBytes != nil {\n\t\tkafkaCfg.MaxMessageBytes = config.MaxMessageBytes\n\t}\n\tif config.RequiredACKs != nil {\n\t\tkafkaCfg.RequiredACKs = config.RequiredACKs\n\t}\n\tif config.Compression != \"\" {\n\t\tkafkaCfg.Compression = config.Compression\n\t}\n\n\tif config.BulkMaxSize > 0 {\n\t\tkafkaCfg.BulkMaxSize = config.BulkMaxSize\n\t}\n\tif config.MaxRetries > 0 {\n\t\tkafkaCfg.MaxRetries = config.MaxRetries\n\t}\n\tif config.ClientID != \"\" {\n\t\tkafkaCfg.ClientID = config.ClientID\n\t}\n\tif config.ChanBufferSize > 0 {\n\t\tkafkaCfg.ChanBufferSize = config.ChanBufferSize\n\t}","relevantFileList":[],"filePath":"internal/onlineconsumer/onlineconsumer.go","template":"go","multiRes":{"hipilot":", ok := v.Get(Kafka).(*Config)\n    return cfg, ok","CodeQwen2.5-7B":"\n\tcfg := &Config{}\n\tif err := v.UnmarshalKey(Kafka, cfg); err != nil {\n\t\treturn nil, false\n\t}\n\treturn cfg, true\n<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"var cfg Config\n\tif err := v.UnmarshalKey(&cfg); err != nil {\n\t\tlog.Panic().Msgf(\"Cannot convert [%s] config\", Kafka)\n\t\treturn nil, false\n\t}\n\treturn &cfg, true\n","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package onlineconsumer\n\n// Config 创建配置结构体\ntype Config struct {\n\tBucketNum   int  `map","infill":"structure:\"bucket_num\"`\n\tKafkaConfig *KafkaC","suffix":"onfig `mapstructure:\"kafkaconfig\"`\n}\n\ntype KafkaConfig struct {\n\tMaxConcurrent     int      `mapstructure:\"max_concurrent\"`\n\tHosts             []string `mapstructure:\"hosts\"`\n\tGroupId           string   `mapstructure:\"group_id\"`\n\tTopics            []string `mapstructure:\"topics\"`\n\tAutocommitEnabled bool     `mapstructure:\"auto_commit_enable\"`\n\tMaxMessageBytes   *int     `mapstructure:\"max_message_bytes,optional\"`\n\tRequiredACKs      *int     `mapstructure:\"required_acks,optional\"`\n\tCompression       string   `mapstructure:\"compression,optional\"`\n\tBulkMaxSize       int      `mapstructure:\"bulk_max_size,optional\"`\n\tMaxRetries        int      `mapstructure:\"max_retries,optional\"`\n\tClientID          string   `mapstructure:\"client_id,optional\"`\n\tChanBufferSize    int      `mapstructure:\"channel_buffer_size,optional\"`\n\tSaslEnable        bool     `mapstructure:\"sasl_enable\"`\n\tUsername          string   `mapstructure:\"username,optional\"`\n\tPassword          string   `mapstructure:\"password,optional\"`\n\tSaslType          string   `mapstructure:\"sasl_type,optional\"`\n}\n","relevantFile":"<file_path>internal/config/loggerconfig.go\npackage config\n\ntype LoggerConfig struct {\n\tProfile           string `mapstructure:\"profile\"`\n\tLogFileName       string `mapstructure:\"log_file_name\"`\n\tLogFileMaxSize    int    `mapstructure:\"log_file_max_size\"`\n\tLogFileMaxBackups int    `mapstructure:\"log_file_max_backups\"`\n\tLogFileMaxAge     int    `mapstructure:\"log_file_max_age\"`\n\tLogLevel          string `mapstructure:\"log_level\"`\n}\n<file_path>internal/util/stringutil.go\npackage util\n\nfunc GetFixedLengthStr(str string, length int) string {\n\tif len(str) > length {\n\t\treturn str[0:length] + \"...\"\n\t}\n\treturn str\n}\n<file_path>internal/config/redisconfig.go\npackage config\n\ntype RedisConfig struct {\n\tAddr     string `mapstructure:\"addr\"`\n\tPassword string `mapstructure:\"password\"`\n\tPoolSize int    `mapstructure:\"pool_size\"`\n}\n<file_path>internal/prom/config.go\npackage prom\n\ntype Config struct {\n\tPort string `mapstructure:\"port\"`\n}","relevantFileList":[],"filePath":"internal/onlineconsumer/config.go","template":"go","multiRes":{"hipilot":"structure:\"bucket_num\"`\n    OnlineSize   int  `mapstructure:\"online_size\"`\n    CleanInterval int  `mapstructure:\"clean_interval\"`\n    IsTesting     bool `mapstructure:\"is_testing\"`\n    KafkaConfig  KafkaC","CodeQwen2.5-7B":"structure:\"bucket_num\"`\n\tMaxConcurrent int  `mapstructure:\"max_concurrent\"`\n\tKafkaConfig   KafkaC<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"structure:\"bucket_num\"`\n\tKafkaConfig *KafkaC","deepseek-chat-lite":"structure:\"bucket_num\"`","CodeQwen1.5-7B":" structure:\"bucket_num\"`"}},{"prefix":"package onlineconsumer\n\n// OnlineUs","infill":"erHandler 结构体创建\ntype OnlineUserHandler struct {\n\t\n}","suffix":"","relevantFile":"<file_path>internal/consts/const.go\npackage consts\n\nconst (\n\tStackExprPrefix       string = \"stack_expr_conf_\"\n\tWorkloadStack         string = \"workload_stack_conf\"\n\tPaasCmdbContainerInfo string = \"paas_cmdb_container_info\"\n\tStackPrefix           string = \"stack_conf_\"\n)\n<file_path>internal/config/redisconfig.go\npackage config\n\ntype RedisConfig struct {\n\tAddr     string `mapstructure:\"addr\"`\n\tPassword string `mapstructure:\"password\"`\n\tPoolSize int    `mapstructure:\"pool_size\"`\n}\n<file_path>internal/util/stringutil.go\npackage util\n\nfunc GetFixedLengthStr(str string, length int) string {\n\tif len(str) > length {\n\t\treturn str[0:length] + \"...\"\n\t}\n\treturn str\n}\n<file_path>internal/prom/config.go\npackage prom\n\ntype Config struct {\n\tPort string `mapstructure:\"port\"`\n}","relevantFileList":[],"filePath":"internal/onlineconsumer/onlineuserhandler.go","template":"go","multiRes":{"hipilot":"erHandler 用户上线处理","CodeQwen2.5-7B":"er is a user that is online\ntype OnlineUser struct {\n\tUsername string\n}","copilot":"erHandler 结构体创建\ntype OnlineUserHandler struct {\n\t\n}","deepseek-chat-lite":"er struct","CodeQwen1.5-7B":" er is a user that is online"}},{"prefix":"package onlineconsumer\n\n// OnlineUserHandler 结构体创建\ntype ","infill":"OnlineUserHandler struct {\n\t\n}","suffix":"","relevantFile":"<file_path>internal/config/loggerconfig.go\npackage config\n\ntype LoggerConfig struct {\n\tProfile           string `mapstructure:\"profile\"`\n\tLogFileName       string `mapstructure:\"log_file_name\"`\n\tLogFileMaxSize    int    `mapstructure:\"log_file_max_size\"`\n\tLogFileMaxBackups int    `mapstructure:\"log_file_max_backups\"`\n\tLogFileMaxAge     int    `mapstructure:\"log_file_max_age\"`\n\tLogLevel          string `mapstructure:\"log_level\"`\n}\n<file_path>internal/cache/types.go\npackage cache\n\ntype CmdbContainerInfo struct {\n\tBusinessName      string `json:\"business_name\"`\n\tBusinessCode      string `json:\"business_code\"`\n\tBusinessGroupCode string `json:\"business_group_code\"`\n\tBusinessGroupName string `json:\"business_group_name\"`\n\tProductName       string `json:\"product_name\"`\n\tProductCode       string `json:\"product_code\"`\n}\n<file_path>internal/config/redisconfig.go\npackage config\n\ntype RedisConfig struct {\n\tAddr     string `mapstructure:\"addr\"`\n\tPassword string `mapstructure:\"password\"`\n\tPoolSize int    `mapstructure:\"pool_size\"`\n}\n<file_path>internal/prom/config.go\npackage prom\n\ntype Config struct {\n\tPort string `mapstructure:\"port\"`\n}","relevantFileList":[],"filePath":"internal/onlineconsumer/onlineuserhandler.go","template":"go","multiRes":{"hipilot":"OnlineUserHandler struct {\n}\n","CodeQwen2.5-7B":"OnlineUserHandler struct {\n\t// 用户id\n\tUserId int64 `json:\"user_id,omitempty\" xml:\"user_id,omitempty\"`\n\t// 用户昵称\n\tNick string `json:\"nick,omitempty\" xml:\"nick,omitempty\"`\n\t// 用户头像\n\tAvatar string `json:\"avatar,omitempty\" xml:\"avatar,omitempty\"`\n\t// 用户性别\n\tGender int64 `json:\"gender,omitempty\" xml:\"gender,omitempty\"`\n\t// 用户等级\n\tLevel int64 `json:\"level,omitempty\" xml:\"level,omitempty\"`\n\t// 用户等级名称\n\tLevelName string `json:\"level_name,omitempty\" xml:\"level_name,omitempty\"`\n\t// 用户等级图标\n\tLevelIcon string `json:\"level_icon,omitempty\" xml:\"level_icon,omitempty\"`\n\t// 用户等级背景\n\tLevelBg string `json:\"level_bg,omitempty\" xml:\"level_bg,omitempty\"`\n\t// 用户等级背景颜色\n\tLevelBgColor string `json:\"level_bg_color,omitempty\" xml:\"level_bg_color,omitempty\"`\n\t// 用户等级背景图片\n\tLevelBgImage string `json:\"level_bg_image,omitempty\" xml:\"level_bg_image,omitempty\"`\n\t// 用户等级背景图片\n\tLevelBgImage2 string `json:\"level_bg_image2,omitempty\" xml:\"level_bg_image2,omitempty\"`\n\t// 用户等级背景图片\n\tLevelBgImage3 string `json:\"level_bg_image3,omitempty\" xml:\"level_bg_image3,omitempty\"`\n\t// 用户等级背景图片\n","copilot":"OnlineUserHandler struct {\n\t\n}","deepseek-chat-lite":"OnlineUserHandler struct {","CodeQwen1.5-7B":" OnlineUserHandler struct {"}},{"prefix":"package onlineconsumer\n\n// Config 创建配置结构体\ntype Config struct {\n\tOnlineUserConfig ","infill":"*OnlineUserConfig `mapstructure:\"onlineuserconfig\"`\n\tKafkaConfig      *Kaf","suffix":"kaConfig      `mapstructure:\"kafkaconfig\"`\n}\n\ntype KafkaConfig struct {\n\tMaxConcurrent     int      `mapstructure:\"max_concurrent\"`\n\tHosts             []string `mapstructure:\"hosts\"`\n\tGroupId           string   `mapstructure:\"group_id\"`\n\tTopics            []string `mapstructure:\"topics\"`\n\tAutocommitEnabled bool     `mapstructure:\"auto_commit_enable\"`\n\tMaxMessageBytes   *int     `mapstructure:\"max_message_bytes,optional\"`\n\tRequiredACKs      *int     `mapstructure:\"required_acks,optional\"`\n\tCompression       string   `mapstructure:\"compression,optional\"`\n\tBulkMaxSize       int      `mapstructure:\"bulk_max_size,optional\"`\n\tMaxRetries        int      `mapstructure:\"max_retries,optional\"`\n\tClientID          string   `mapstructure:\"client_id,optional\"`\n\tChanBufferSize    int      `mapstructure:\"channel_buffer_size,optional\"`\n\tSaslEnable        bool     `mapstructure:\"sasl_enable\"`\n\tUsername          string   `mapstructure:\"username,optional\"`\n\tPassword          string   `mapstructure:\"password,optional\"`\n\tSaslType          string   `mapstructure:\"sasl_type,optional\"`\n}\n\ntype OnlineUserConfig struct {\n\tBucketNum int `mapstructure:\"bucket_num\"`\n}\n","relevantFile":"<file_path>internal/cache/types.go\npackage cache\n\ntype CmdbContainerInfo struct {\n\tBusinessName      string `json:\"business_name\"`\n\tBusinessCode      string `json:\"business_code\"`\n\tBusinessGroupCode string `json:\"business_group_code\"`\n\tBusinessGroupName string `json:\"business_group_name\"`\n\tProductName       string `json:\"product_name\"`\n\tProductCode       string `json:\"product_code\"`\n}\n<file_path>internal/config/redisconfig.go\npackage config\n\ntype RedisConfig struct {\n\tAddr     string `mapstructure:\"addr\"`\n\tPassword string `mapstructure:\"password\"`\n\tPoolSize int    `mapstructure:\"pool_size\"`\n}\n<file_path>internal/prom/config.go\npackage prom\n\ntype Config struct {\n\tPort string `mapstructure:\"port\"`\n}\n<file_path>internal/onlineconsumer/onlineuserhandler.go\npackage onlineconsumer\n\n// OnlineUserHandler 结构体创建\ntype OnlineUserHandler struct {\n\tcfg *OnlineUserConfig\n}","relevantFileList":[],"filePath":"internal/onlineconsumer/config.go","template":"go","multiRes":{"hipilot":"OnlineUserConfig `mapstructure:\"online_user_config\"`\n    KafkaConfig         Kaf","CodeQwen2.5-7B":"OnlineUserConfig `mapstructure:\"online_user_config\"`\n\tKaf<|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|><|fim_pad|>","copilot":"*OnlineUserConfig `mapstructure:\"onlineuserconfig\"`\n\tKafkaConfig      *Kaf","deepseek-chat-lite":"`mapstructure:\"online_user_config\"`","CodeQwen1.5-7B":" OnlineUserConfig `mapstructure:\"onlineuserconfig\"`"}},{"prefix":"package onlineconsumer\n\n// OnlineUserHandler 结构体创建\ntype OnlineUserHandler struct {\n\tcfg *OnlineUserConfig\n}\n\n// NewOnlineUserHandler 创建OnlineUserHandler实例\nfun","infill":"c NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {\n\treturn &OnlineUserHandler{cfg: cfg}\n}\n\n// Cac","suffix":"","relevantFile":"<file_path>internal/config/loggerconfig.go\npackage config\n\ntype LoggerConfig struct {\n\tProfile           string `mapstructure:\"profile\"`\n\tLogFileName       string `mapstructure:\"log_file_name\"`\n\tLogFileMaxSize    int    `mapstructure:\"log_file_max_size\"`\n\tLogFileMaxBackups int    `mapstructure:\"log_file_max_backups\"`\n\tLogFileMaxAge     int    `mapstructure:\"log_file_max_age\"`\n\tLogLevel          string `mapstructure:\"log_level\"`\n}\n<file_path>internal/cache/types.go\npackage cache\n\ntype CmdbContainerInfo struct {\n\tBusinessName      string `json:\"business_name\"`\n\tBusinessCode      string `json:\"business_code\"`\n\tBusinessGroupCode string `json:\"business_group_code\"`\n\tBusinessGroupName string `json:\"business_group_name\"`\n\tProductName       string `json:\"product_name\"`\n\tProductCode       string `json:\"product_code\"`\n}\n<file_path>internal/config/redisconfig.go\npackage config\n\ntype RedisConfig struct {\n\tAddr     string `mapstructure:\"addr\"`\n\tPassword string `mapstructure:\"password\"`\n\tPoolSize int    `mapstructure:\"pool_size\"`\n}\n<file_path>internal/prom/config.go\npackage prom\n\ntype Config struct {\n\tPort string `mapstructure:\"port\"`\n}","relevantFileList":[],"filePath":"internal/onlineconsumer/onlineuserhandler.go","template":"go","multiRes":{"hipilot":"c NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {\n    return &OnlineUserHandler{\n        cfg: cfg,\n    }\n}\n","CodeQwen2.5-7B":"c NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {\n\treturn &OnlineUserHandler{\n\t\tcfg: cfg,\n\t}\n}","copilot":"c NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {\n\treturn &OnlineUserHandler{cfg: cfg}\n}\n\n// Cac","deepseek-chat-lite":"c NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {","CodeQwen1.5-7B":" c NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {"}},{"prefix":"package onlineconsumer\n\n// OnlineUserHandler 结构体创建\ntype OnlineUserHandler struct {\n\tcfg *OnlineUserConfig\n}\n\n//","infill":" NewOnlineUserHandler 创建OnlineUserHandler实例\nfunc NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {\n\treturn &OnlineUserHandler{cfg: cfg}\n}\n\n// ca","suffix":"","relevantFile":"<file_path>internal/config/loggerconfig.go\npackage config\n\ntype LoggerConfig struct {\n\tProfile           string `mapstructure:\"profile\"`\n\tLogFileName       string `mapstructure:\"log_file_name\"`\n\tLogFileMaxSize    int    `mapstructure:\"log_file_max_size\"`\n\tLogFileMaxBackups int    `mapstructure:\"log_file_max_backups\"`\n\tLogFileMaxAge     int    `mapstructure:\"log_file_max_age\"`\n\tLogLevel          string `mapstructure:\"log_level\"`\n}\n<file_path>internal/cache/types.go\npackage cache\n\ntype CmdbContainerInfo struct {\n\tBusinessName      string `json:\"business_name\"`\n\tBusinessCode      string `json:\"business_code\"`\n\tBusinessGroupCode string `json:\"business_group_code\"`\n\tBusinessGroupName string `json:\"business_group_name\"`\n\tProductName       string `json:\"product_name\"`\n\tProductCode       string `json:\"product_code\"`\n}\n<file_path>internal/config/redisconfig.go\npackage config\n\ntype RedisConfig struct {\n\tAddr     string `mapstructure:\"addr\"`\n\tPassword string `mapstructure:\"password\"`\n\tPoolSize int    `mapstructure:\"pool_size\"`\n}\n<file_path>internal/prom/config.go\npackage prom\n\ntype Config struct {\n\tPort string `mapstructure:\"port\"`\n}","relevantFileList":[],"filePath":"internal/onlineconsumer/onlineuserhandler.go","template":"go","multiRes":{"hipilot":"初始化\nfunc NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {\n    return &OnlineUserHandler{\n        cfg: cfg,\n    }\n}\n","CodeQwen2.5-7B":" NewOnlineUserHandler 创建OnlineUserHandler\nfunc NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {\n\treturn &OnlineUserHandler{\n\t\tcfg: cfg,\n\t}\n}","copilot":" NewOnlineUserHandler 创建OnlineUserHandler实例\nfunc NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {\n\treturn &OnlineUserHandler{cfg: cfg}\n}\n\n// ca","deepseek-chat-lite":" NewOnlineUserHandler 创建OnlineUserHandler实例","CodeQwen1.5-7B":"  NewOnlineUserHandler 创建结构体"}},{"prefix":"package onlineconsumer\n\n// OnlineUserHandler 结构体创建\ntype OnlineUserHandler struct {\n\tcfg *OnlineUserConfig\n}\n\n// NewOnlineUserHandler 创建OnlineUserHandler实例\nfunc NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {\n\treturn &OnlineUserHandler{cfg: cfg}\n}\n\n// CalculateO","infill":"nlineUser 计算在线用户\nfunc (h *OnlineUserHandler) CalculateOnlineUser(kafkaMsg *KafkaMsg) {\n\t// 业务逻辑\n}","suffix":"","relevantFile":"<file_path>internal/cache/types.go\npackage cache\n\ntype CmdbContainerInfo struct {\n\tBusinessName      string `json:\"business_name\"`\n\tBusinessCode      string `json:\"business_code\"`\n\tBusinessGroupCode string `json:\"business_group_code\"`\n\tBusinessGroupName string `json:\"business_group_name\"`\n\tProductName       string `json:\"product_name\"`\n\tProductCode       string `json:\"product_code\"`\n}\n<file_path>internal/config/redisconfig.go\npackage config\n\ntype RedisConfig struct {\n\tAddr     string `mapstructure:\"addr\"`\n\tPassword string `mapstructure:\"password\"`\n\tPoolSize int    `mapstructure:\"pool_size\"`\n}\n<file_path>internal/util/stringutil.go\npackage util\n\nfunc GetFixedLengthStr(str string, length int) string {\n\tif len(str) > length {\n\t\treturn str[0:length] + \"...\"\n\t}\n\treturn str\n}\n<file_path>internal/prom/config.go\npackage prom\n\ntype Config struct {\n\tPort string `mapstructure:\"port\"`\n}","relevantFileList":[],"filePath":"internal/onlineconsumer/onlineuserhandler.go","template":"go","multiRes":{"hipilot":"nlineUser 计算在线用户\n//func (ouh *OnlineUserHandler) CalculateOnlineUser(ctx context.Context, req *userpb.CalcOnlineUserReq) (*userpb.CalcOnlineUserResp, error) {\n//\n//}\n","CodeQwen2.5-7B":"nlineUser 计算在线用户\nfunc (o *OnlineUserHandler) CalculateOnlineUser() (int, error) {\n\treturn 0, nil\n}","copilot":"nlineUser 计算在线用户\nfunc (h *OnlineUserHandler) CalculateOnlineUser(kafkaMsg *KafkaMsg) {\n\t// 业务逻辑\n}","deepseek-chat-lite":"nlineUser 计算在线用户","CodeQwen1.5-7B":" nlineUsers 计算在线用户数"}},{"prefix":"package onlineconsumer\n\n// OnlineUserHandler 结构体创建\ntype OnlineUserHandler struct {\n\tcfg *OnlineUserConfig\n}\n\n// NewOnlineUserHandler 创建OnlineUserHandler实例\nfunc NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {\n\treturn &OnlineUserHandler{cfg: cfg}\n}\n\n// CalculateOnlineUser 计算在线用户\nfunc (h *OnlineUserHandler) CalculateOnlineUser(kafkaMs","infill":"g *KafkaMsg) {\n\t// 业务逻辑\n}","suffix":"","relevantFile":"<file_path>internal/cache/types.go\npackage cache\n\ntype CmdbContainerInfo struct {\n\tBusinessName      string `json:\"business_name\"`\n\tBusinessCode      string `json:\"business_code\"`\n\tBusinessGroupCode string `json:\"business_group_code\"`\n\tBusinessGroupName string `json:\"business_group_name\"`\n\tProductName       string `json:\"product_name\"`\n\tProductCode       string `json:\"product_code\"`\n}\n<file_path>internal/config/redisconfig.go\npackage config\n\ntype RedisConfig struct {\n\tAddr     string `mapstructure:\"addr\"`\n\tPassword string `mapstructure:\"password\"`\n\tPoolSize int    `mapstructure:\"pool_size\"`\n}\n<file_path>internal/util/stringutil.go\npackage util\n\nfunc GetFixedLengthStr(str string, length int) string {\n\tif len(str) > length {\n\t\treturn str[0:length] + \"...\"\n\t}\n\treturn str\n}\n<file_path>internal/prom/config.go\npackage prom\n\ntype Config struct {\n\tPort string `mapstructure:\"port\"`\n}","relevantFileList":[],"filePath":"internal/onlineconsumer/onlineuserhandler.go","template":"go","multiRes":{"hipilot":"g []byte) error {\n    // 在此处实现...\n    return nil\n}\n","CodeQwen2.5-7B":"g *KafkaMsg) (int, error) {\n\t// TODO: 实现在线用户计算逻辑\n\treturn 0, nil\n}","copilot":"g *KafkaMsg) {\n\t// 业务逻辑\n}","deepseek-chat-lite":"g []byte) (int, error) {","CodeQwen1.5-7B":" g []byte) (int, error) {"}},{"prefix":"package onlineconsumer\n\nimport \"github.com/Shopify/sarama\"\n\n// OnlineUserHandler 结构体创建\ntype OnlineUserHandler struct {\n\tcfg *OnlineUserConfig\n}\n\n// NewOnlineUserHandler 创建OnlineUserHandler实例\nfunc NewOnlineUserHandler(cfg *OnlineUserConfig) *OnlineUserHandler {\n\treturn &OnlineUserHandler{cfg: cfg}\n}\n\n// CalculateOnlineUser 计算在线用户\nfunc (h *OnlineUserHandler) CalculateOnlineUser(kafkaMsg *sarama.ConsumerMessage) error {\n\t// ","infill":"TODO: 实现在线用户计算逻辑\n\treturn nil\n}","suffix":"\n","relevantFile":"<file_path>internal/util/timeutil.go\npackage util\n\nimport (\n\t\"github.com/rs/zerolog/log\"\n\t\"time\"\n)\n\nfunc GetTimeDuration(duration string) time.Duration {\n\tdur, err := time.ParseDuration(duration)\n\tif err != nil {\n\t\tlog.Err(err).Msgf(\"duration parse failed ! return default 1 hour; durationStr: %s\", duration)\n\t\treturn time.Duration(1) * time.Minute\n\t}\n\treturn dur\n}\n<file_path>internal/onlineconsumer/sasl.go\npackage onlineconsumer\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/Shopify/sarama\"\n)\n\ntype SaslConfig struct {\n\tSaslMechanism string `Config:\"mechanism\"`\n}\n\nconst (\n\tSaslTypePlaintext   = sarama.SASLTypePlaintext\n\tSaslTypeSCRAMSHA256 = sarama.SASLTypeSCRAMSHA256\n\tSaslTypeSCRAMSHA512 = sarama.SASLTypeSCRAMSHA512\n)\n\nfunc (c *SaslConfig) ConfigureSarama(config *sarama.Config) {\n\tswitch strings.ToUpper(c.SaslMechanism) { // try not to force users to use all upper case\n\tcase \"\":\n\t\t// SASL is not enabled\n\t\treturn\n\tcase SaslTypePlaintext:\n\t\tconfig.Net.SASL.Mechanism = sarama.SASLMechanism(sarama.SASLTypePlaintext)\n\tcase SaslTypeSCRAMSHA256:\n\t\tconfig.Net.SASL.Handshake = true\n\t\tconfig.Net.SASL.Mechanism = sarama.SASLMechanism(sarama.SASLTypeSCRAMSHA256)\n\t\tconfig.Net.SASL.SCRAMClientGeneratorFunc = func() sarama.SCRAMClient {\n\t\t\treturn &XDGSCRAMClient{HashGeneratorFcn: SHA256}\n\t\t}\n\tcase SaslTypeSCRAMSHA512:\n\t\tconfig.Net.SASL.Handshake = true\n\t\tconfig.Net.SASL.Mechanism = sarama.SASLMechanism(sarama.SASLTypeSCRAMSHA512)\n\t\tconfig.Net.SASL.SCRAMClientGeneratorFunc = func() sarama.SCRAMClient {\n\t\t\treturn &XDGSCRAMClient{HashGeneratorFcn: SHA512}\n\t\t}\n\tdefault:\n\t\t// This should never happen because `SaslMechanism` is checked on `Validate()`, keeping a panic to detect it earlier if it happens.\n\t\tpanic(fmt.Sprintf(\"not valid SASL mechanism '%v', only supported with PLAIN|SCRAM-SHA-512|SCRAM-SHA-256\", c.SaslMechanism))\n\t}\n}\n\nfunc (c *SaslConfig) Validate() error {\n\tswitch strings.ToUpper(c.SaslMechanism) { // try not to force users to use all upper case\n\tcase \"\", SaslTypePlaintext, SaslTypeSCRAMSHA256, SaslTypeSCRAMSHA512:\n\tdefault:\n\t\treturn fmt.Errorf(\"not valid SASL mechanism '%v', only supported with PLAIN|SCRAM-SHA-512|SCRAM-SHA-256\", c.SaslMechanism)\n\t}\n\treturn nil\n}\n<file_path>internal/util/viper.go\npackage util\n\nimport (\n\t\"fmt\"\n\t\"github.com/spf13/viper\"\n)\n\n// ReadInConfig 将配置读取到viper中\nfunc ReadInConfig(path string) (*viper.Viper, error) {\n\tv := viper.New()\n\tv.SetConfigFile(path)\n\tv.SetConfigType(\"yaml\")\n\terr := v.ReadInConfig()\n\tif err != nil { // Handle errors reading the config file\n\t\treturn nil, fmt.Errorf(\"error happened while reading config file: %v\", err)\n\t}\n\treturn v, nil\n}\n<file_path>internal/onlineconsumer/onlineconsumer.go\npackage onlineconsumer\n\nimport (\n\t\"context\"\n\t\"github.com/Shopify/sarama\"\n\t\"github.com/rs/zerolog/log\"\n\t\"github.com/spf13/viper\"\n\t\"go.uber.org/zap\"\n\t\"go.uber.org/zap/zapcore\"\n\t\"sync\"\n\t\"time\"\n)\n\nconst (\n\tKafka = \"online consumer\"\n)\n\n// OnlineConsumer 创建OnlineConsumer结构体\ntype OnlineConsumer struct {\n\tcfg               *Config\n\tshutdownWG        sync.WaitGroup\n\tshutdownState     bool\n\tconsumerGroup     sarama.ConsumerGroup\n\ttopics            []string\n\tcancelConsumeLoop context.CancelFunc\n\tautocommitEnabled bool\n\thandler           *OnlineUserHandler\n}\n\n// NewOnlineConsumer 创建消费者实例\nfunc NewOnlineConsumer(v *viper.Viper) *OnlineConsumer {\n\tcfg, ok := getConfig(v)\n\tif !ok {\n\t\tlog.Panic().Msgf(\"Cannot convert [%s] config\", Kafka)\n\t\treturn nil\n\t}\n\tclient, err := getConsumerGroup(cfg.KafkaConfig)\n\tif err != nil {\n\t\tlog.Panic().Msgf(\"Cannot create [%s] consumer\", Kafka)\n\t\treturn nil\n\t}\n\treturn &OnlineConsumer{\n\t\tcfg:               cfg,\n\t\tconsumerGroup:     client,\n\t\ttopics:            cfg.KafkaConfig.Topics,\n\t\tautocommitEnabled: cfg.KafkaConfig.AutocommitEnabled,\n\t\thandler:           NewOnlineUserHandler(cfg.OnlineUserConfig),\n\t}\n}\n\nfunc getConfig(v *viper.Viper) (*Config, bool) {\n\tvar cfg Config\n\tif err := v.UnmarshalKey(\"onlineconsumer\", &cfg); err != nil {\n\t\tlog.Panic().Msgf(\"Cannot convert [%s] config\", Kafka)\n\t\treturn nil, false\n\t}\n\treturn &cfg, true\n}\n\nfunc getConsumerGroup(config *KafkaConfig) (sarama.ConsumerGroup, error) {","relevantFileList":[],"filePath":"internal/onlineconsumer/onlineuserhandler.go","template":"go","multiRes":{"hipilot":"TODO 根据需求对接收到的kafka消息进行处理","CodeQwen2.5-7B":"TODO: 实现在线用户计算逻辑","copilot":"TODO: 实现在线用户计算逻辑\n\treturn nil\n}","deepseek-chat-lite":"TODO: 实现计算在线用户的逻辑","CodeQwen1.5-7B":" 获取消息内容"}},{"prefix":"package redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/go-redis/redis/v8\"\n\t\"github.com/rs/zerolog/log\"\n\t\"ths-monitor-apm-onlineuser/internal/config\"\n\t\"time\"\n)\n\nvar RedisClient *Client\n\ntype Client struct {\n\tclient *redis.Client\n\tctx    context.Context\n}\n\nfunc InitRedis(conf config.RedisConfig) {\n\tRedisClient = NewRedisClient(conf)\n}\n\nfunc NewRedisClient(conf config.RedisConfig) *Client {\n\tclient := redis.NewClient(&redis.Options{\n\t\tAddr:     conf.Addr,\n\t\tPassword: conf.Password,\n\t\tPoolSize: conf.PoolSize,\n\t})\n\tpong, err := client.Ping(context.Background()).Result()\n\tif err != nil {\n\t\tlog.Err(err).Msg(\"redis connect failed! exiting...\")\n\t\tpanic(fmt.Sprintf(\"redis connect failed! exiting... , err:%s \", err.Error()))\n\t}\n\tlog.Info().Msgf(\"redis ping result : %s\", pong)\n\treturn &Client{\n\t\tclient: client,\n\t\tctx:    context.Background(),\n\t}\n}\n\nfunc (c *Client) Set(key string, value string) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetExpire(key string, value string, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) Expire(key string, expiration time.Duration) error {\n\treturn c.client.Expire(c.ctx, key, expiration).Err()\n}\n\nfunc (c *Client) HSetExpire(key string, value map[string]interface{}, expiration time.Duration) error {\n\n\terr := c.client.HSet(c.ctx, key, value).Err()\n\tif err != nil {\n\t\treturn err\n\t} else {\n\t\treturn c.client.Expire(c.ctx, key, expiration).Err()\n\t}\n}\n\nfunc (c *Client) HSet(key string, value map[string]interface{}) error {\n\treturn c.client.HSet(c.ctx, key, value).Err()\n}\n\nfunc (c *Client) HGetAll(key string) (map[string]string, error) {\n\n\treturn c.client.HGetAll(c.ctx, key).Result()\n}\n\nfunc (c *Client) HGet(key string, field string) (string, error) {\n\n\treturn c.client.HGet(c.ctx, key, field).Result()\n}\n\nfunc (c *Client) Get(key string) (string, error) {\n\n\treturn c.client.Get(c.ctx, key).Result()\n}\n\nfunc (c *Client) SetBytes(key string, value []byte) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetBytesExpire(key string, value []byte, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) GetBytes(key string) ([]byte, error) {\n\n\treturn c.client.Get(c.ctx, key).Bytes()\n}\n\nfunc (c *Client) SetNX(key string, value string, expiration time.Duration) bool {\n\treturn c.client.SetNX(c.ctx, key, value, expiration).Val()\n}\n\nfunc (c *Client) GetDel(key string) (string, error) {\n\n\treturn c.client.GetDel(c.ctx, key).Result()\n}\n\nfunc (c *Client) Del(key string) error {\n\treturn c.client.Del(c.ctx, key).Err()\n}\n\nfunc (c *Client) RPush(key string, val string) error {\n\treturn c.client.RPush(c.ctx, key, val).Err()\n}\n\nfunc (c *Client) LPop(key string) (string, error) {\n\treturn c.client.LPop(c.ctx, key).Result()\n}\n\n// ZAdd ","infill":"adds the specified member with the specified score to the sorted set stored at key.\nfunc (c *Client) ZAdd(key string, member string, score float64) error {\n\treturn c.client.ZAdd(c.ctx, key, &redis.Z{Score: score, Member: member}).Err()\n}\n","suffix":"","relevantFile":"<file_path>internal/util/slicepool.go\npackage util\n\nimport (\n\t\"bytes\"\n\t\"sync\"\n)\n\nvar slicePool sync.Pool\n\nfunc init() {\n\tslicePool = sync.Pool{New: func() interface{} {\n\t\tnewSlice := bytes.NewBuffer(make([]byte, 4096))\n\t\treturn newSlice\n\t}}\n}\n\nfunc GetBuffer() *bytes.Buffer {\n\tbuffer := slicePool.Get().(*bytes.Buffer)\n\tbuffer.Reset()\n\treturn buffer\n}\n\nfunc PutBuffer(buffer *bytes.Buffer) {\n\tif buffer != nil {\n\t\tslicePool.Put(buffer)\n\t}\n}\n<file_path>internal/util/timeutil.go\npackage util\n\nimport (\n\t\"github.com/rs/zerolog/log\"\n\t\"time\"\n)\n\nfunc GetTimeDuration(duration string) time.Duration {\n\tdur, err := time.ParseDuration(duration)\n\tif err != nil {\n\t\tlog.Err(err).Msgf(\"duration parse failed ! return default 1 hour; durationStr: %s\", duration)\n\t\treturn time.Duration(1) * time.Minute\n\t}\n\treturn dur\n}\n<file_path>internal/util/md5.go\npackage util\n\nimport (\n\t\"crypto/md5\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"sort\"\n)\n\nfunc GetMd5Str(str string) string {\n\thas := md5.Sum([]byte(str))\n\tmd5Str := fmt.Sprintf(\"%x\", has)\n\treturn md5Str\n}\n\nfunc GetMd5Sign(params map[string]string) string {\n\tkeys := make([]string, 0, len(params))\n\tfor k := range params {\n\t\tkeys = append(keys, k)\n\t}\n\tsort.Strings(keys)\n\tsign := \"metric\"\n\tfor _, key := range keys {\n\t\tsign += key\n\t\tsign += params[key]\n\t}\n\thash := md5.Sum([]byte(sign))\n\treturn hex.EncodeToString(hash[:])\n}\n<file_path>internal/util/http.go\n\t\"time\"\n)\n\nconst (\n\tPromSource    = \"Source\"\n\tPromSourceVal = \"inspector\"\n)\n\nvar urlBuilderPool sync.Pool\n\nfunc init() {\n\turlBuilderPool = sync.Pool{New: func() interface{} {\n\t\treturn make([]byte, 0, 150)\n\t}}\n}\nfunc HttpGet(url string) ([]byte, error) {\n\tc := http.Client{\n\t\tTimeout: 30 * time.Second,\n\t}\n\tresp, err := c.Get(url)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPostForm(policyurl string, data url.Values, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.PostForm(policyurl, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPost(url string, bodyBytes []byte, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.Post(url, \"application/json\", bytes.NewBuffer(bodyBytes))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trespBytes, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn respBytes, err\n\t}","relevantFileList":[],"filePath":"internal/redis/client.go","template":"go","multiRes":{"hipilot":"if member not exist will create a new sorted set key","CodeQwen2.5-7B":"向有序集合中添加一个或多个成员，或者更新已存在成员的分数\nfunc (c *Client) ZAdd(key string, members ...*redis.Z) error {\n\treturn c.client.ZAdd(c.ctx, key, members...).Err()\n}","copilot":"adds the specified member with the specified score to the sorted set stored at key.\nfunc (c *Client) ZAdd(key string, member string, score float64) error {\n\treturn c.client.ZAdd(c.ctx, key, &redis.Z{Score: score, Member: member}).Err()\n}\n","deepseek-chat-lite":"添加一个或多个成员到有序集合,或者如果它已经存在更新分数","CodeQwen1.5-7B":" 添加有序集合"}},{"prefix":"package redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/go-redis/redis/v8\"\n\t\"github.com/rs/zerolog/log\"\n\t\"ths-monitor-apm-onlineuser/internal/config\"\n\t\"time\"\n)\n\nvar RedisClient *Client\n\ntype Client struct {\n\tclient *redis.Client\n\tctx    context.Context\n}\n\nfunc InitRedis(conf config.RedisConfig) {\n\tRedisClient = NewRedisClient(conf)\n}\n\nfunc NewRedisClient(conf config.RedisConfig) *Client {\n\tclient := redis.NewClient(&redis.Options{\n\t\tAddr:     conf.Addr,\n\t\tPassword: conf.Password,\n\t\tPoolSize: conf.PoolSize,\n\t})\n\tpong, err := client.Ping(context.Background()).Result()\n\tif err != nil {\n\t\tlog.Err(err).Msg(\"redis connect failed! exiting...\")\n\t\tpanic(fmt.Sprintf(\"redis connect failed! exiting... , err:%s \", err.Error()))\n\t}\n\tlog.Info().Msgf(\"redis ping result : %s\", pong)\n\treturn &Client{\n\t\tclient: client,\n\t\tctx:    context.Background(),\n\t}\n}\n\nfunc (c *Client) Set(key string, value string) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetExpire(key string, value string, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) Expire(key string, expiration time.Duration) error {\n\treturn c.client.Expire(c.ctx, key, expiration).Err()\n}\n\nfunc (c *Client) HSetExpire(key string, value map[string]interface{}, expiration time.Duration) error {\n\n\terr := c.client.HSet(c.ctx, key, value).Err()\n\tif err != nil {\n\t\treturn err\n\t} else {\n\t\treturn c.client.Expire(c.ctx, key, expiration).Err()\n\t}\n}\n\nfunc (c *Client) HSet(key string, value map[string]interface{}) error {\n\treturn c.client.HSet(c.ctx, key, value).Err()\n}\n\nfunc (c *Client) HGetAll(key string) (map[string]string, error) {\n\n\treturn c.client.HGetAll(c.ctx, key).Result()\n}\n\nfunc (c *Client) HGet(key string, field string) (string, error) {\n\n\treturn c.client.HGet(c.ctx, key, field).Result()\n}\n\nfunc (c *Client) Get(key string) (string, error) {\n\n\treturn c.client.Get(c.ctx, key).Result()\n}\n\nfunc (c *Client) SetBytes(key string, value []byte) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetBytesExpire(key string, value []byte, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) GetBytes(key string) ([]byte, error) {\n\n\treturn c.client.Get(c.ctx, key).Bytes()\n}\n\nfunc (c *Client) SetNX(key string, value string, expiration time.Duration) bool {\n\treturn c.client.SetNX(c.ctx, key, value, expiration).Val()\n}\n\nfunc (c *Client) GetDel(key string) (string, error) {\n\n\treturn c.client.GetDel(c.ctx, key).Result()\n}\n\nfunc (c *Client) Del(key string) error {\n\treturn c.client.Del(c.ctx, key).Err()\n}\n\nfunc (c *Client) RPush(key string, val string) error {\n\treturn c.client.RPush(c.ctx, key, val).Err()\n}\n\nfunc (c *Client) LPop(key string) (string, error) {\n\treturn c.client.LPop(c.ctx, key).Result()\n}\n\n// ZAdd adds the specified member with the specified score to t","infill":"\n\treturn c.client.ZAdd(c.ctx, key, &redis.Z{Score: score, Member: member}).Err()\n}\n","suffix":"","relevantFile":"<file_path>internal/onlineconsumer/onlineconsumer.go\n\tif config.ChanBufferSize > 0 {\n\t\tkafkaCfg.ChanBufferSize = config.ChanBufferSize\n\t}\n\tif config.SaslEnable {\n\t\tkafkaCfg.Username = config.Username\n\t\tkafkaCfg.Password = config.Password\n\t\tif config.SaslType == \"\" {\n\t\t\tkafkaCfg.Sasl.SaslMechanism = sarama.SASLTypePlaintext\n\t\t} else {\n\t\t\tkafkaCfg.Sasl.SaslMechanism = config.SaslType\n\t\t}\n\t}\n\tkafkaCfg.AutocommitEnabled = config.AutocommitEnabled\n\tcfg, err := NewSaramaConfig(&kafkaCfg)\n\tif err != nil {\n\t}\n\treturn sarama.NewConsumerGroup(config.Hosts, config.GroupId, cfg)\n}\n\ntype metricsConsumerGroupHandler struct {\n\tready             chan bool\n\treadyCloser       sync.Once\n\tlogger            *zap.Logger\n\tautocommitEnabled bool\n\tselfMetrics       *SelfMetrics\n\tgoroutineChan     chan struct{}\n\thandler           *OnlineUserHandler\n}\n\nfunc (c *metricsConsumerGroupHandler) Setup(session sarama.ConsumerGroupSession) error {\n\tc.readyCloser.Do(func() {\n\t\tclose(c.ready)\n\t})\n\treturn nil\n}\n\nfunc (c *metricsConsumerGroupHandler) Cleanup(session sarama.ConsumerGroupSession) error {\n\treturn nil\n}\n\nfunc (c *metricsConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {\n\tc.logger.Info(\"Starting consumer group\", zap.Int32(\"partition\", claim.Partition()))\n\tif !c.autocommitEnabled {\n\t\tdefer session.Commit()\n\t}\n\n\tfor message := range claim.Messages() {\n\t\tc.logger.Debug(\"Kafka message claimed\",\n\t\t\tzap.String(\"value\", string(message.Value)),\n\t\t\tzap.Time(\"timestamp\", message.Timestamp),\n\t\t\tzap.String(\"topic\", message.Topic))\n\t\tc.selfMetrics.eventReceivedCounter.WithLabelValues(message.Topic).Inc()\n\t\tif ce := c.logger.Check(zapcore.DebugLevel, \"Receive Event\"); ce != nil {\n\t\t\tce.Write(\n\t\t\t\tzap.String(\"message\", string(message.Value)),\n\t\t\t)\n\t\t}\n\t\tsession.MarkMessage(message, \"\")\n\t\tc.selfMetrics.eventReceivedBytesCounter.WithLabelValues(message.Topic).Add(float64(len(message.Value)))\n\t\tstart := time.Now().UnixMilli()\n<file_path>internal/util/timeutil.go\npackage util\n\nimport (\n\t\"github.com/rs/zerolog/log\"\n\t\"time\"\n)\n\nfunc GetTimeDuration(duration string) time.Duration {\n\tdur, err := time.ParseDuration(duration)\n\tif err != nil {\n\t\tlog.Err(err).Msgf(\"duration parse failed ! return default 1 hour; durationStr: %s\", duration)\n\t\treturn time.Duration(1) * time.Minute\n\t}\n\treturn dur\n}\n<file_path>internal/util/md5.go\npackage util\n\nimport (\n\t\"crypto/md5\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"sort\"\n)\n\nfunc GetMd5Str(str string) string {\n\thas := md5.Sum([]byte(str))\n\tmd5Str := fmt.Sprintf(\"%x\", has)\n\treturn md5Str\n}\n\nfunc GetMd5Sign(params map[string]string) string {\n\tkeys := make([]string, 0, len(params))\n\tfor k := range params {\n\t\tkeys = append(keys, k)\n\t}\n\tsort.Strings(keys)\n\tsign := \"metric\"\n\tfor _, key := range keys {\n\t\tsign += key\n\t\tsign += params[key]\n\t}\n\thash := md5.Sum([]byte(sign))\n\treturn hex.EncodeToString(hash[:])\n}\n<file_path>internal/util/http.go\n\t\"time\"\n)\n\nconst (\n\tPromSource    = \"Source\"\n\tPromSourceVal = \"inspector\"\n)\n\nvar urlBuilderPool sync.Pool\n\nfunc init() {\n\turlBuilderPool = sync.Pool{New: func() interface{} {\n\t\treturn make([]byte, 0, 150)\n\t}}\n}\nfunc HttpGet(url string) ([]byte, error) {\n\tc := http.Client{\n\t\tTimeout: 30 * time.Second,\n\t}\n\tresp, err := c.Get(url)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPostForm(policyurl string, data url.Values, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.PostForm(policyurl, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPost(url string, bodyBytes []byte, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.Post(url, \"application/json\", bytes.NewBuffer(bodyBytes))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trespBytes, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn respBytes, err\n\t}","relevantFileList":[],"filePath":"internal/redis/client.go","template":"go","multiRes":{"hipilot":"he sorted set stored at key.\nfunc (c *Client) ZAdd(key string, members ...*redis.Z) (int64, error) {\n    return c.client.ZAdd(c.ctx, key, members).Result()\n}\n\n// ZRangeBy","CodeQwen2.5-7B":"he sorted set stored at key.\n// If member is already a member of the sorted set the score is updated, and the element reinserted in the right position to ensure sorting. If key does not exist a new sorted set with the specified member as sole member is created. If the key exists but does not hold a sorted set value an error is returned.\n// The score value can be the string representation of a double precision floating point number.\n// The precision is limited to 15 digits after the decimal point.\n// The number of elements added, not including elements already existing for which the score was updated.\nfunc (c *Client) ZAdd(key string, score float64, member string) (int64, error) {\n\treturn c.client.ZAdd(c.ctx, key, &redis.Z{\n\t\tScore:  score,\n\t\tMember: member,\n\t}).Result()\n}","copilot":"\n\treturn c.client.ZAdd(c.ctx, key, &redis.Z{Score: score, Member: member}).Err()\n}\n","deepseek-chat-lite":"he sorted set stored at key.","CodeQwen1.5-7B":" he sorted set stored at key."}},{"prefix":"package redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/go-redis/redis/v8\"\n\t\"github.com/rs/zerolog/log\"\n\t\"ths-monitor-apm-onlineuser/internal/config\"\n\t\"time\"\n)\n\nvar RedisClient *Client\n\ntype Client struct {\n\tclient *redis.Client\n\tctx    context.Context\n}\n\nfunc InitRedis(conf config.RedisConfig) {\n\tRedisClient = NewRedisClient(conf)\n}\n\nfunc NewRedisClient(conf config.RedisConfig) *Client {\n\tclient := redis.NewClient(&redis.Options{\n\t\tAddr:     conf.Addr,\n\t\tPassword: conf.Password,\n\t\tPoolSize: conf.PoolSize,\n\t})\n\tpong, err := client.Ping(context.Background()).Result()\n\tif err != nil {\n\t\tlog.Err(err).Msg(\"redis connect failed! exiting...\")\n\t\tpanic(fmt.Sprintf(\"redis connect failed! exiting... , err:%s \", err.Error()))\n\t}\n\tlog.Info().Msgf(\"redis ping result : %s\", pong)\n\treturn &Client{\n\t\tclient: client,\n\t\tctx:    context.Background(),\n\t}\n}\n\nfunc (c *Client) Set(key string, value string) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetExpire(key string, value string, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) Expire(key string, expiration time.Duration) error {\n\treturn c.client.Expire(c.ctx, key, expiration).Err()\n}\n\nfunc (c *Client) HSetExpire(key string, value map[string]interface{}, expiration time.Duration) error {\n\n\terr := c.client.HSet(c.ctx, key, value).Err()\n\tif err != nil {\n\t\treturn err\n\t} else {\n\t\treturn c.client.Expire(c.ctx, key, expiration).Err()\n\t}\n}\n\nfunc (c *Client) HSet(key string, value map[string]interface{}) error {\n\treturn c.client.HSet(c.ctx, key, value).Err()\n}\n\nfunc (c *Client) HGetAll(key string) (map[string]string, error) {\n\n\treturn c.client.HGetAll(c.ctx, key).Result()\n}\n\nfunc (c *Client) HGet(key string, field string) (string, error) {\n\n\treturn c.client.HGet(c.ctx, key, field).Result()\n}\n\nfunc (c *Client) Get(key string) (string, error) {\n\n\treturn c.client.Get(c.ctx, key).Result()\n}\n\nfunc (c *Client) SetBytes(key string, value []byte) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetBytesExpire(key string, value []byte, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) GetBytes(key string) ([]byte, error) {\n\n\treturn c.client.Get(c.ctx, key).Bytes()\n}\n\nfunc (c *Client) SetNX(key string, value string, expiration time.Duration) bool {\n\treturn c.client.SetNX(c.ctx, key, value, expiration).Val()\n}\n\nfunc (c *Client) GetDel(key string) (string, error) {\n\n\treturn c.client.GetDel(c.ctx, key).Result()\n}\n\nfunc (c *Client) Del(key string) error {\n\treturn c.client.Del(c.ctx, key).Err()\n}\n\nfunc (c *Client) RPush(key string, val string) error {\n\treturn c.client.RPush(c.ctx, key, val).Err()\n}\n\nfunc (c *Client) LPop(key string) (string, error) {\n\treturn c.client.LPop(c.ctx, key).Result()\n}\n\n// ZAdd adds the specified member with the specified score to the sorted set stored at key.","infill":"func (c *Client) ZAdd(key string, member string, score float64) error {\n\treturn c.client.ZAdd(c.ctx, key, &redis.Z{Score: score, Member: member}).Err()\n}","suffix":"\n","relevantFile":"<file_path>internal/onlineconsumer/onlineconsumer.go\n\t}\n\n\tif config.BulkMaxSize > 0 {\n\t\tkafkaCfg.BulkMaxSize = config.BulkMaxSize\n\t}\n\tif config.MaxRetries > 0 {\n\t\tkafkaCfg.MaxRetries = config.MaxRetries\n\t}\n\tif config.ClientID != \"\" {\n\t\tkafkaCfg.ClientID = config.ClientID\n\t}\n\tif config.ChanBufferSize > 0 {\n\t\tkafkaCfg.ChanBufferSize = config.ChanBufferSize\n\t}\n\tif config.SaslEnable {\n\t\tkafkaCfg.Username = config.Username\n\t\tkafkaCfg.Password = config.Password\n\t\tif config.SaslType == \"\" {\n\t\t\tkafkaCfg.Sasl.SaslMechanism = sarama.SASLTypePlaintext\n\t\t} else {\n\t\t\tkafkaCfg.Sasl.SaslMechanism = config.SaslType\n\t\t}\n\t}\n\tkafkaCfg.AutocommitEnabled = config.AutocommitEnabled\n\tcfg, err := NewSaramaConfig(&kafkaCfg)\n\tif err != nil {\n\t}\n\treturn sarama.NewConsumerGroup(config.Hosts, config.GroupId, cfg)\n}\n\ntype metricsConsumerGroupHandler struct {\n\tready             chan bool\n\treadyCloser       sync.Once\n\tlogger            *zap.Logger\n\tautocommitEnabled bool\n\tselfMetrics       *SelfMetrics\n\tgoroutineChan     chan struct{}\n\thandler           *OnlineUserHandler\n}\n\nfunc (c *metricsConsumerGroupHandler) Setup(session sarama.ConsumerGroupSession) error {\n\tc.readyCloser.Do(func() {\n\t\tclose(c.ready)\n\t})\n\treturn nil\n}\n\nfunc (c *metricsConsumerGroupHandler) Cleanup(session sarama.ConsumerGroupSession) error {\n\treturn nil\n}\n\nfunc (c *metricsConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {\n\tc.logger.Info(\"Starting consumer group\", zap.Int32(\"partition\", claim.Partition()))\n\tif !c.autocommitEnabled {\n\t\tdefer session.Commit()\n\t}\n\n\tfor message := range claim.Messages() {\n\t\tc.logger.Debug(\"Kafka message claimed\",\n\t\t\tzap.String(\"value\", string(message.Value)),\n<file_path>internal/util/timeutil.go\npackage util\n\nimport (\n\t\"github.com/rs/zerolog/log\"\n\t\"time\"\n)\n\nfunc GetTimeDuration(duration string) time.Duration {\n\tdur, err := time.ParseDuration(duration)\n\tif err != nil {\n\t\tlog.Err(err).Msgf(\"duration parse failed ! return default 1 hour; durationStr: %s\", duration)\n\t\treturn time.Duration(1) * time.Minute\n\t}\n\treturn dur\n}\n<file_path>internal/util/md5.go\npackage util\n\nimport (\n\t\"crypto/md5\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"sort\"\n)\n\nfunc GetMd5Str(str string) string {\n\thas := md5.Sum([]byte(str))\n\tmd5Str := fmt.Sprintf(\"%x\", has)\n\treturn md5Str\n}\n\nfunc GetMd5Sign(params map[string]string) string {\n\tkeys := make([]string, 0, len(params))\n\tfor k := range params {\n\t\tkeys = append(keys, k)\n\t}\n\tsort.Strings(keys)\n\tsign := \"metric\"\n\tfor _, key := range keys {\n\t\tsign += key\n\t\tsign += params[key]\n\t}\n\thash := md5.Sum([]byte(sign))\n\treturn hex.EncodeToString(hash[:])\n}\n<file_path>internal/util/http.go\n\t\"time\"\n)\n\nconst (\n\tPromSource    = \"Source\"\n\tPromSourceVal = \"inspector\"\n)\n\nvar urlBuilderPool sync.Pool\n\nfunc init() {\n\turlBuilderPool = sync.Pool{New: func() interface{} {\n\t\treturn make([]byte, 0, 150)\n\t}}\n}\nfunc HttpGet(url string) ([]byte, error) {\n\tc := http.Client{\n\t\tTimeout: 30 * time.Second,\n\t}\n\tresp, err := c.Get(url)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPostForm(policyurl string, data url.Values, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.PostForm(policyurl, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPost(url string, bodyBytes []byte, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.Post(url, \"application/json\", bytes.NewBuffer(bodyBytes))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trespBytes, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn respBytes, err\n\t}","relevantFileList":[],"filePath":"internal/redis/client.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\nfunc (c *Client) ZAdd(key string, score float64, member string) error {\n\treturn c.client.ZAdd(c.ctx, key, &redis.Z{Score: score, Member: member}).Err()\n}","copilot":"func (c *Client) ZAdd(key string, member string, score float64) error {\n\treturn c.client.ZAdd(c.ctx, key, &redis.Z{Score: score, Member: member}).Err()\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/go-redis/redis/v8\"\n\t\"github.com/rs/zerolog/log\"\n\t\"ths-monitor-apm-onlineuser/internal/config\"\n\t\"time\"\n)\n\nvar RedisClient *Client\n\ntype Client struct {\n\tclient *redis.Client\n\tctx    context.Context\n}\n\nfunc InitRedis(conf config.RedisConfig) {\n\tRedisClient = NewRedisClient(conf)\n}\n\nfunc NewRedisClient(conf config.RedisConfig) *Client {\n\tclient := redis.NewClient(&redis.Options{\n\t\tAddr:     conf.Addr,\n\t\tPassword: conf.Password,\n\t\tPoolSize: conf.PoolSize,\n\t})\n\tpong, err := client.Ping(context.Background()).Result()\n\tif err != nil {\n\t\tlog.Err(err).Msg(\"redis connect failed! exiting...\")\n\t\tpanic(fmt.Sprintf(\"redis connect failed! exiting... , err:%s \", err.Error()))\n\t}\n\tlog.Info().Msgf(\"redis ping result : %s\", pong)\n\treturn &Client{\n\t\tclient: client,\n\t\tctx:    context.Background(),\n\t}\n}\n\nfunc (c *Client) Set(key string, value string) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetExpire(key string, value string, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) Expire(key string, expiration time.Duration) error {\n\treturn c.client.Expire(c.ctx, key, expiration).Err()\n}\n\nfunc (c *Client) HSetExpire(key string, value map[string]interface{}, expiration time.Duration) error {\n\n\terr := c.client.HSet(c.ctx, key, value).Err()\n\tif err != nil {\n\t\treturn err\n\t} else {\n\t\treturn c.client.Expire(c.ctx, key, expiration).Err()\n\t}\n}\n\nfunc (c *Client) HSet(key string, value map[string]interface{}) error {\n\treturn c.client.HSet(c.ctx, key, value).Err()\n}\n\nfunc (c *Client) HGetAll(key string) (map[string]string, error) {\n\n\treturn c.client.HGetAll(c.ctx, key).Result()\n}\n\nfunc (c *Client) HGet(key string, field string) (string, error) {\n\n\treturn c.client.HGet(c.ctx, key, field).Result()\n}\n\nfunc (c *Client) Get(key string) (string, error) {\n\n\treturn c.client.Get(c.ctx, key).Result()\n}\n\nfunc (c *Client) SetBytes(key string, value []byte) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetBytesExpire(key string, value []byte, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) GetBytes(key string) ([]byte, error) {\n\n\treturn c.client.Get(c.ctx, key).Bytes()\n}\n\nfunc (c *Client) SetNX(key string, value string, expiration time.Duration) bool {\n\treturn c.client.SetNX(c.ctx, key, value, expiration).Val()\n}\n\nfunc (c *Client) GetDel(key string) (string, er","infill":"\n\treturn c.client.ZCount(c.ctx, key, min, max).Result()\n}\n\n// ZRe\n","suffix":"","relevantFile":"<file_path>internal/onlineconsumer/onlineconsumer.go\n\tif config.ChanBufferSize > 0 {\n\t\tkafkaCfg.ChanBufferSize = config.ChanBufferSize\n\t}\n\tif config.SaslEnable {\n\t\tkafkaCfg.Username = config.Username\n\t\tkafkaCfg.Password = config.Password\n\t\tif config.SaslType == \"\" {\n\t\t\tkafkaCfg.Sasl.SaslMechanism = sarama.SASLTypePlaintext\n\t\t} else {\n\t\t\tkafkaCfg.Sasl.SaslMechanism = config.SaslType\n\t\t}\n\t}\n\tkafkaCfg.AutocommitEnabled = config.AutocommitEnabled\n\tcfg, err := NewSaramaConfig(&kafkaCfg)\n\tif err != nil {\n\t}\n\treturn sarama.NewConsumerGroup(config.Hosts, config.GroupId, cfg)\n}\n\ntype metricsConsumerGroupHandler struct {\n\tready             chan bool\n\treadyCloser       sync.Once\n\tlogger            *zap.Logger\n\tautocommitEnabled bool\n\tselfMetrics       *SelfMetrics\n\tgoroutineChan     chan struct{}\n\thandler           *OnlineUserHandler\n}\n\nfunc (c *metricsConsumerGroupHandler) Setup(session sarama.ConsumerGroupSession) error {\n\tc.readyCloser.Do(func() {\n\t\tclose(c.ready)\n\t})\n\treturn nil\n}\n\nfunc (c *metricsConsumerGroupHandler) Cleanup(session sarama.ConsumerGroupSession) error {\n\treturn nil\n}\n\nfunc (c *metricsConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {\n\tc.logger.Info(\"Starting consumer group\", zap.Int32(\"partition\", claim.Partition()))\n\tif !c.autocommitEnabled {\n\t\tdefer session.Commit()\n\t}\n\n\tfor message := range claim.Messages() {\n\t\tc.logger.Debug(\"Kafka message claimed\",\n\t\t\tzap.String(\"value\", string(message.Value)),\n\t\t\tzap.Time(\"timestamp\", message.Timestamp),\n\t\t\tzap.String(\"topic\", message.Topic))\n\t\tc.selfMetrics.eventReceivedCounter.WithLabelValues(message.Topic).Inc()\n\t\tif ce := c.logger.Check(zapcore.DebugLevel, \"Receive Event\"); ce != nil {\n\t\t\tce.Write(\n\t\t\t\tzap.String(\"message\", string(message.Value)),\n\t\t\t)\n\t\t}\n\t\tsession.MarkMessage(message, \"\")\n\t\tc.selfMetrics.eventReceivedBytesCounter.WithLabelValues(message.Topic).Add(float64(len(message.Value)))\n\t\tstart := time.Now().UnixMilli()\n<file_path>internal/util/timeutil.go\npackage util\n\nimport (\n\t\"github.com/rs/zerolog/log\"\n\t\"time\"\n)\n\nfunc GetTimeDuration(duration string) time.Duration {\n\tdur, err := time.ParseDuration(duration)\n\tif err != nil {\n\t\tlog.Err(err).Msgf(\"duration parse failed ! return default 1 hour; durationStr: %s\", duration)\n\t\treturn time.Duration(1) * time.Minute\n\t}\n\treturn dur\n}\n<file_path>internal/util/md5.go\npackage util\n\nimport (\n\t\"crypto/md5\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"sort\"\n)\n\nfunc GetMd5Str(str string) string {\n\thas := md5.Sum([]byte(str))\n\tmd5Str := fmt.Sprintf(\"%x\", has)\n\treturn md5Str\n}\n\nfunc GetMd5Sign(params map[string]string) string {\n\tkeys := make([]string, 0, len(params))\n\tfor k := range params {\n\t\tkeys = append(keys, k)\n\t}\n\tsort.Strings(keys)\n\tsign := \"metric\"\n\tfor _, key := range keys {\n\t\tsign += key\n\t\tsign += params[key]\n\t}\n\thash := md5.Sum([]byte(sign))\n\treturn hex.EncodeToString(hash[:])\n}\n<file_path>internal/util/http.go\n\tprom.PromDelayMetric.WithLabelValues(\"sentinel\").Observe(float64(delay))\n\ttimeoutD := 60 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = time.Duration(timeout) * time.Second\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tdata := url.Values{\n\t\t\"query\":   {query},\n\t\t\"time\":    {strconv.Itoa(promTime)},\n\t\t\"timeout\": {timeoutD.String()},\n\t}\n\trequest, err := http.NewRequest(http.MethodPost, promUrl, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trequest.Header.Add(PromSource, PromSourceVal)\n\trequest.URL.RawQuery = data.Encode()\n\tresp, err := client.Do(request)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc StructToQueryParams(data interface{}) string {\n\tvalues := url.Values{}\n\tv := reflect.ValueOf(data)\n\tt := v.Type()\n\tfor i := 0; i < v.NumField(); i++ {\n\t\tfieldName := t.Field(i).Tag.Get(\"url\")\n\t\tif fieldName == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tfieldValue := fmt.Sprintf(\"%v\", v.Field(i))\n\t\tvalues.Set(fieldName, fieldValue)\n\t}\n\treturn values.Encode()\n}\n\nfunc GetWholeUrl(address string, secret string, args map[string]string) string {\n\tsign := GetSign(secret, args)\n\turlBuilder := urlBuilderPool.Get()\n\tdefer urlBuilderPool.Put(urlBuilder)\n\turlBytes, _ := urlBuilder.([]byte)\n\turlBytes = urlBytes[:0]\n\n\turlBytes = append(urlBytes, address...)\n\turlBytes = append(urlBytes, \"?sign=\"...)\n\turlBytes = append(urlBytes, sign...)\n\tfor key, value := range args {\n\t\turlBytes = append(urlBytes, \"&\"...)\n\t\turlBytes = append(urlBytes, key...)\n\t\turlBytes = append(urlBytes, \"=\"...)\n\t\turlBytes = append(urlBytes, value...)\n\t}\n\treturn string(urlBytes)\n}","relevantFileList":[],"filePath":"internal/redis/client.go","template":"go","multiRes":{"hipilot":"ror) {\n\n    return c.client.GetDel(c.ctx, key).Result()\n}\n\nfunc (c *Client) Del(key string) (int64, error) {\n\n    return c.client.Del(c.ctx, key).Result()\n","CodeQwen2.5-7B":"ror) {\n\treturn c.client.GetDel(c.ctx, key).Result()\n}","copilot":"\n\treturn c.client.ZCount(c.ctx, key, min, max).Result()\n}\n\n// ZRe\n","deepseek-chat-lite":"ror) {","CodeQwen1.5-7B":"ror) {"}},{"prefix":"package redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/go-redis/redis/v8\"\n\t\"github.com/rs/zerolog/log\"\n\t\"ths-monitor-apm-onlineuser/internal/config\"\n\t\"time\"\n)\n\nvar RedisClient *Client\n\ntype Client struct {\n\tclient *redis.Client\n\tctx    context.Context\n}\n\nfunc InitRedis(conf config.RedisConfig) {\n\tRedisClient = NewRedisClient(conf)\n}\n\nfunc NewRedisClient(conf config.RedisConfig) *Client {\n\tclient := redis.NewClient(&redis.Options{\n\t\tAddr:     conf.Addr,\n\t\tPassword: conf.Password,\n\t\tPoolSize: conf.PoolSize,\n\t})\n\tpong, err := client.Ping(context.Background()).Result()\n\tif err != nil {\n\t\tlog.Err(err).Msg(\"redis connect failed! exiting...\")\n\t\tpanic(fmt.Sprintf(\"redis connect failed! exiting... , err:%s \", err.Error()))\n\t}\n\tlog.Info().Msgf(\"redis ping result : %s\", pong)\n\treturn &Client{\n\t\tclient: client,\n\t\tctx:    context.Background(),\n\t}\n}\n\nfunc (c *Client) Set(key string, value string) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetExpire(key string, value string, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) Expire(key string, expiration time.Duration) error {\n\treturn c.client.Expire(c.ctx, key, expiration).Err()\n}\n\nfunc (c *Client) HSetExpire(key string, value map[string]interface{}, expiration time.Duration) error {\n\n\terr := c.client.HSet(c.ctx, key, value).Err()\n\tif err != nil {\n\t\treturn err\n\t} else {\n\t\treturn c.client.Expire(c.ctx, key, expiration).Err()\n\t}\n}\n\nfunc (c *Client) HSet(key string, value map[string]interface{}) error {\n\treturn c.client.HSet(c.ctx, key, value).Err()\n}\n\nfunc (c *Client) HGetAll(key string) (map[string]string, error) {\n\n\treturn c.client.HGetAll(c.ctx, key).Result()\n}\n\nfunc (c *Client) HGet(key string, field string) (string, error) {\n\n\treturn c.client.HGet(c.ctx, key, field).Result()\n}\n\nfunc (c *Client) Get(key string) (string, error) {\n\n\treturn c.client.Get(c.ctx, key).Result()\n}\n\nfunc (c *Client) SetBytes(key string, value []byte) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetBytesExpire(key string, value []byte, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) GetBytes(key string) ([]byte, error) {\n\n\treturn c.client.Get(c.ctx, key).Bytes()\n}\n\nfunc (c *Client) SetNX(key string, value string, expiration time.Duration) bool {\n\treturn c.client.SetNX(c.ctx, key, value, expiration).Val()\n}\n\nfunc (c *Client) GetDel(key string) (string, error) {\n\n\treturn c.client.GetDel(c.ctx, key).Result()\n}\n\nfunc (c *Client) Del(key string) error {\n\treturn c.client.Del(c.ctx, key).Err()\n}\n\nfunc (c *Client) RPush(key string, val string) error {\n\treturn c.client.RPush(c.ctx, key, val).Err()\n}\n\nfunc (c *Client) LPop(key string) (string, error) {\n\treturn c.client.LPop(c.ctx, key).Result()\n}\n\n// ZAdd adds the specified member with the specified score to t","infill":"unt returns the number of elements in the sorted set at key with a score between min and max.\nfunc (c *Client) ZCount(key string, min, max string) (int64, error) {\n\treturn c.client.ZCount(c.ctx, key, min, max).Result()\n}\n\n// ZR\n","suffix":"","relevantFile":"<file_path>internal/onlineconsumer/onlineconsumer.go\n\tif config.ChanBufferSize > 0 {\n\t\tkafkaCfg.ChanBufferSize = config.ChanBufferSize\n\t}\n\tif config.SaslEnable {\n\t\tkafkaCfg.Username = config.Username\n\t\tkafkaCfg.Password = config.Password\n\t\tif config.SaslType == \"\" {\n\t\t\tkafkaCfg.Sasl.SaslMechanism = sarama.SASLTypePlaintext\n\t\t} else {\n\t\t\tkafkaCfg.Sasl.SaslMechanism = config.SaslType\n\t\t}\n\t}\n\tkafkaCfg.AutocommitEnabled = config.AutocommitEnabled\n\tcfg, err := NewSaramaConfig(&kafkaCfg)\n\tif err != nil {\n\t}\n\treturn sarama.NewConsumerGroup(config.Hosts, config.GroupId, cfg)\n}\n\ntype metricsConsumerGroupHandler struct {\n\tready             chan bool\n\treadyCloser       sync.Once\n\tlogger            *zap.Logger\n\tautocommitEnabled bool\n\tselfMetrics       *SelfMetrics\n\tgoroutineChan     chan struct{}\n\thandler           *OnlineUserHandler\n}\n\nfunc (c *metricsConsumerGroupHandler) Setup(session sarama.ConsumerGroupSession) error {\n\tc.readyCloser.Do(func() {\n\t\tclose(c.ready)\n\t})\n\treturn nil\n}\n\nfunc (c *metricsConsumerGroupHandler) Cleanup(session sarama.ConsumerGroupSession) error {\n\treturn nil\n}\n\nfunc (c *metricsConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {\n\tc.logger.Info(\"Starting consumer group\", zap.Int32(\"partition\", claim.Partition()))\n\tif !c.autocommitEnabled {\n\t\tdefer session.Commit()\n\t}\n\n\tfor message := range claim.Messages() {\n\t\tc.logger.Debug(\"Kafka message claimed\",\n\t\t\tzap.String(\"value\", string(message.Value)),\n\t\t\tzap.Time(\"timestamp\", message.Timestamp),\n\t\t\tzap.String(\"topic\", message.Topic))\n\t\tc.selfMetrics.eventReceivedCounter.WithLabelValues(message.Topic).Inc()\n\t\tif ce := c.logger.Check(zapcore.DebugLevel, \"Receive Event\"); ce != nil {\n\t\t\tce.Write(\n\t\t\t\tzap.String(\"message\", string(message.Value)),\n\t\t\t)\n\t\t}\n\t\tsession.MarkMessage(message, \"\")\n\t\tc.selfMetrics.eventReceivedBytesCounter.WithLabelValues(message.Topic).Add(float64(len(message.Value)))\n\t\tstart := time.Now().UnixMilli()\n<file_path>internal/util/timeutil.go\npackage util\n\nimport (\n\t\"github.com/rs/zerolog/log\"\n\t\"time\"\n)\n\nfunc GetTimeDuration(duration string) time.Duration {\n\tdur, err := time.ParseDuration(duration)\n\tif err != nil {\n\t\tlog.Err(err).Msgf(\"duration parse failed ! return default 1 hour; durationStr: %s\", duration)\n\t\treturn time.Duration(1) * time.Minute\n\t}\n\treturn dur\n}\n<file_path>internal/util/md5.go\npackage util\n\nimport (\n\t\"crypto/md5\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"sort\"\n)\n\nfunc GetMd5Str(str string) string {\n\thas := md5.Sum([]byte(str))\n\tmd5Str := fmt.Sprintf(\"%x\", has)\n\treturn md5Str\n}\n\nfunc GetMd5Sign(params map[string]string) string {\n\tkeys := make([]string, 0, len(params))\n\tfor k := range params {\n\t\tkeys = append(keys, k)\n\t}\n\tsort.Strings(keys)\n\tsign := \"metric\"\n\tfor _, key := range keys {\n\t\tsign += key\n\t\tsign += params[key]\n\t}\n\thash := md5.Sum([]byte(sign))\n\treturn hex.EncodeToString(hash[:])\n}\n<file_path>internal/util/http.go\n\t\"time\"\n)\n\nconst (\n\tPromSource    = \"Source\"\n\tPromSourceVal = \"inspector\"\n)\n\nvar urlBuilderPool sync.Pool\n\nfunc init() {\n\turlBuilderPool = sync.Pool{New: func() interface{} {\n\t\treturn make([]byte, 0, 150)\n\t}}\n}\nfunc HttpGet(url string) ([]byte, error) {\n\tc := http.Client{\n\t\tTimeout: 30 * time.Second,\n\t}\n\tresp, err := c.Get(url)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPostForm(policyurl string, data url.Values, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.PostForm(policyurl, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPost(url string, bodyBytes []byte, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.Post(url, \"application/json\", bytes.NewBuffer(bodyBytes))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trespBytes, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn respBytes, err\n\t}","relevantFileList":[],"filePath":"internal/redis/client.go","template":"go","multiRes":{"hipilot":"he sorted set stored at key.\nfunc (c *Client) ZAdd(key string, member string, score float64) error {\n    return c.client.ZAdd(c.ctx, key, &redis.Z{\n        Score:  score,\n        Member:","CodeQwen2.5-7B":"he sorted set stored at key.\n// If member is already a member of the sorted set the score is updated, and the element reinserted in the right position to ensure sorting. If key does not exist a new sorted set with the specified member as sole member is created. If the key exists but does not hold a sorted set value an error is returned.\n// The score value can be the string representation of a double precision floating point number.\n// The precision is limited to 15 digits after the decimal point.\n// The number of elements added, not including elements already existing for which the score was updated.\nfunc (c *Client) ZAdd(key string, score float64, member string) (int64, error) {\n\treturn c.client.ZAdd(c.ctx, key, &redis.Z{\n\t\tScore:  score,\n\t\tMember: member,\n\t}).Result()\n}","copilot":"unt returns the number of elements in the sorted set at key with a score between min and max.\nfunc (c *Client) ZCount(key string, min, max string) (int64, error) {\n\treturn c.client.ZCount(c.ctx, key, min, max).Result()\n}\n\n// ZR\n","deepseek-chat-lite":"he sorted set stored at key.","CodeQwen1.5-7B":" he sorted set stored at key."}},{"prefix":"package redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/go-redis/redis/v8\"\n\t\"github.com/rs/zerolog/log\"\n\t\"ths-monitor-apm-onlineuser/internal/config\"\n\t\"time\"\n)\n\nvar RedisClient *Client\n\ntype Client struct {\n\tclient *redis.Client\n\tctx    context.Context\n}\n\nfunc InitRedis(conf config.RedisConfig) {\n\tRedisClient = NewRedisClient(conf)\n}\n\nfunc NewRedisClient(conf config.RedisConfig) *Client {\n\tclient := redis.NewClient(&redis.Options{\n\t\tAddr:     conf.Addr,\n\t\tPassword: conf.Password,\n\t\tPoolSize: conf.PoolSize,\n\t})\n\tpong, err := client.Ping(context.Background()).Result()\n\tif err != nil {\n\t\tlog.Err(err).Msg(\"redis connect failed! exiting...\")\n\t\tpanic(fmt.Sprintf(\"redis connect failed! exiting... , err:%s \", err.Error()))\n\t}\n\tlog.Info().Msgf(\"redis ping result : %s\", pong)\n\treturn &Client{\n\t\tclient: client,\n\t\tctx:    context.Background(),\n\t}\n}\n\nfunc (c *Client) Set(key string, value string) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetExpire(key string, value string, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) Expire(key string, expiration time.Duration) error {\n\treturn c.client.Expire(c.ctx, key, expiration).Err()\n}\n\nfunc (c *Client) HSetExpire(key string, value map[string]interface{}, expiration time.Duration) error {\n\n\terr := c.client.HSet(c.ctx, key, value).Err()\n\tif err != nil {\n\t\treturn err\n\t} else {\n\t\treturn c.client.Expire(c.ctx, key, expiration).Err()\n\t}\n}\n\nfunc (c *Client) HSet(key string, value map[string]interface{}) error {\n\treturn c.client.HSet(c.ctx, key, value).Err()\n}\n\nfunc (c *Client) HGetAll(key string) (map[string]string, error) {\n\n\treturn c.client.HGetAll(c.ctx, key).Result()\n}\n\nfunc (c *Client) HGet(key string, field string) (string, error) {\n\n\treturn c.client.HGet(c.ctx, key, field).Result()\n}\n\nfunc (c *Client) Get(key string) (string, error) {\n\n\treturn c.client.Get(c.ctx, key).Result()\n}\n\nfunc (c *Client) SetBytes(key string, value []byte) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetBytesExpire(key string, value []byte, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) GetBytes(key string) ([]byte, error) {\n\n\treturn c.client.Get(c.ctx, key).Bytes()\n}\n\nfunc (c *Client) SetNX(key string, value string, expiration time.Duration) bool {\n\treturn c.client.SetNX(c.ctx, key, value, expiration).Val()\n}\n\nfunc (c *Client) GetDel(key string) (string, er","infill":"func (c *Client) ZCount(key string, min, max string) (int64, error) {\n\treturn c.client.ZCount(c.ctx, key, min, max).Result()\n}\n\n// ZRe","suffix":"\n","relevantFile":"<file_path>internal/onlineconsumer/onlineconsumer.go\n\tif config.ChanBufferSize > 0 {\n\t\tkafkaCfg.ChanBufferSize = config.ChanBufferSize\n\t}\n\tif config.SaslEnable {\n\t\tkafkaCfg.Username = config.Username\n\t\tkafkaCfg.Password = config.Password\n\t\tif config.SaslType == \"\" {\n\t\t\tkafkaCfg.Sasl.SaslMechanism = sarama.SASLTypePlaintext\n\t\t} else {\n\t\t\tkafkaCfg.Sasl.SaslMechanism = config.SaslType\n\t\t}\n\t}\n\tkafkaCfg.AutocommitEnabled = config.AutocommitEnabled\n\tcfg, err := NewSaramaConfig(&kafkaCfg)\n\tif err != nil {\n\t}\n\treturn sarama.NewConsumerGroup(config.Hosts, config.GroupId, cfg)\n}\n\ntype metricsConsumerGroupHandler struct {\n\tready             chan bool\n\treadyCloser       sync.Once\n\tlogger            *zap.Logger\n\tautocommitEnabled bool\n\tselfMetrics       *SelfMetrics\n\tgoroutineChan     chan struct{}\n\thandler           *OnlineUserHandler\n}\n\nfunc (c *metricsConsumerGroupHandler) Setup(session sarama.ConsumerGroupSession) error {\n\tc.readyCloser.Do(func() {\n\t\tclose(c.ready)\n\t})\n\treturn nil\n}\n\nfunc (c *metricsConsumerGroupHandler) Cleanup(session sarama.ConsumerGroupSession) error {\n\treturn nil\n}\n\nfunc (c *metricsConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {\n\tc.logger.Info(\"Starting consumer group\", zap.Int32(\"partition\", claim.Partition()))\n\tif !c.autocommitEnabled {\n\t\tdefer session.Commit()\n\t}\n\n\tfor message := range claim.Messages() {\n\t\tc.logger.Debug(\"Kafka message claimed\",\n\t\t\tzap.String(\"value\", string(message.Value)),\n\t\t\tzap.Time(\"timestamp\", message.Timestamp),\n\t\t\tzap.String(\"topic\", message.Topic))\n\t\tc.selfMetrics.eventReceivedCounter.WithLabelValues(message.Topic).Inc()\n\t\tif ce := c.logger.Check(zapcore.DebugLevel, \"Receive Event\"); ce != nil {\n\t\t\tce.Write(\n\t\t\t\tzap.String(\"message\", string(message.Value)),\n\t\t\t)\n\t\t}\n\t\tsession.MarkMessage(message, \"\")\n\t\tc.selfMetrics.eventReceivedBytesCounter.WithLabelValues(message.Topic).Add(float64(len(message.Value)))\n\t\tstart := time.Now().UnixMilli()\n<file_path>internal/util/timeutil.go\npackage util\n\nimport (\n\t\"github.com/rs/zerolog/log\"\n\t\"time\"\n)\n\nfunc GetTimeDuration(duration string) time.Duration {\n\tdur, err := time.ParseDuration(duration)\n\tif err != nil {\n\t\tlog.Err(err).Msgf(\"duration parse failed ! return default 1 hour; durationStr: %s\", duration)\n\t\treturn time.Duration(1) * time.Minute\n\t}\n\treturn dur\n}\n<file_path>internal/util/md5.go\npackage util\n\nimport (\n\t\"crypto/md5\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"sort\"\n)\n\nfunc GetMd5Str(str string) string {\n\thas := md5.Sum([]byte(str))\n\tmd5Str := fmt.Sprintf(\"%x\", has)\n\treturn md5Str\n}\n\nfunc GetMd5Sign(params map[string]string) string {\n\tkeys := make([]string, 0, len(params))\n\tfor k := range params {\n\t\tkeys = append(keys, k)\n\t}\n\tsort.Strings(keys)\n\tsign := \"metric\"\n\tfor _, key := range keys {\n\t\tsign += key\n\t\tsign += params[key]\n\t}\n\thash := md5.Sum([]byte(sign))\n\treturn hex.EncodeToString(hash[:])\n}\n<file_path>internal/util/http.go\n\tprom.PromDelayMetric.WithLabelValues(\"sentinel\").Observe(float64(delay))\n\ttimeoutD := 60 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = time.Duration(timeout) * time.Second\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tdata := url.Values{\n\t\t\"query\":   {query},\n\t\t\"time\":    {strconv.Itoa(promTime)},\n\t\t\"timeout\": {timeoutD.String()},\n\t}\n\trequest, err := http.NewRequest(http.MethodPost, promUrl, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trequest.Header.Add(PromSource, PromSourceVal)\n\trequest.URL.RawQuery = data.Encode()\n\tresp, err := client.Do(request)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc StructToQueryParams(data interface{}) string {\n\tvalues := url.Values{}\n\tv := reflect.ValueOf(data)\n\tt := v.Type()\n\tfor i := 0; i < v.NumField(); i++ {\n\t\tfieldName := t.Field(i).Tag.Get(\"url\")\n\t\tif fieldName == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tfieldValue := fmt.Sprintf(\"%v\", v.Field(i))\n\t\tvalues.Set(fieldName, fieldValue)\n\t}\n\treturn values.Encode()\n}\n\nfunc GetWholeUrl(address string, secret string, args map[string]string) string {\n\tsign := GetSign(secret, args)\n\turlBuilder := urlBuilderPool.Get()\n\tdefer urlBuilderPool.Put(urlBuilder)\n\turlBytes, _ := urlBuilder.([]byte)\n\turlBytes = urlBytes[:0]\n\n\turlBytes = append(urlBytes, address...)\n\turlBytes = append(urlBytes, \"?sign=\"...)\n\turlBytes = append(urlBytes, sign...)\n\tfor key, value := range args {\n\t\turlBytes = append(urlBytes, \"&\"...)\n\t\turlBytes = append(urlBytes, key...)\n\t\turlBytes = append(urlBytes, \"=\"...)\n\t\turlBytes = append(urlBytes, value...)\n\t}\n\treturn string(urlBytes)\n}","relevantFileList":[],"filePath":"internal/redis/client.go","template":"go","multiRes":{"hipilot":"ror) {\n    return c.client.GetDel(c.ctx, key).Result()\n}\n\nfunc (c *Client) Close() error {\n    return c.client.Close()\n}","CodeQwen2.5-7B":"ror) {\n\treturn c.client.GetDel(c.ctx, key).Result()\n}","copilot":"func (c *Client) ZCount(key string, min, max string) (int64, error) {\n\treturn c.client.ZCount(c.ctx, key, min, max).Result()\n}\n\n// ZRe","deepseek-chat-lite":"ror) {","CodeQwen1.5-7B":" ror) {"}},{"prefix":"package redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/go-redis/redis/v8\"\n\t\"github.com/rs/zerolog/log\"\n\t\"ths-monitor-apm-onlineuser/internal/config\"\n\t\"time\"\n)\n\nvar RedisClient *Client\n\ntype Client struct {\n\tclient *redis.Client\n\tctx    context.Context\n}\n\nfunc InitRedis(conf config.RedisConfig) {\n\tRedisClient = NewRedisClient(conf)\n}\n\nfunc NewRedisClient(conf config.RedisConfig) *Client {\n\tclient := redis.NewClient(&redis.Options{\n\t\tAddr:     conf.Addr,\n\t\tPassword: conf.Password,\n\t\tPoolSize: conf.PoolSize,\n\t})\n\tpong, err := client.Ping(context.Background()).Result()\n\tif err != nil {\n\t\tlog.Err(err).Msg(\"redis connect failed! exiting...\")\n\t\tpanic(fmt.Sprintf(\"redis connect failed! exiting... , err:%s \", err.Error()))\n\t}\n\tlog.Info().Msgf(\"redis ping result : %s\", pong)\n\treturn &Client{\n\t\tclient: client,\n\t\tctx:    context.Background(),\n\t}\n}\n\nfunc (c *Client) Set(key string, value string) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetExpire(key string, value string, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) Expire(key string, expiration time.Duration) error {\n\treturn c.client.Expire(c.ctx, key, expiration).Err()\n}\n\nfunc (c *Client) HSetExpire(key string, value map[string]interface{}, expiration time.Duration) error {\n\n\terr := c.client.HSet(c.ctx, key, value).Err()\n\tif err != nil {\n\t\treturn err\n\t} else {\n\t\treturn c.client.Expire(c.ctx, key, expiration).Err()\n\t}\n}\n\nfunc (c *Client) HSet(key string, value map[string]interface{}) error {\n\treturn c.client.HSet(c.ctx, key, value).Err()\n}\n\nfunc (c *Client) HGetAll(key string) (map[string]string, error) {\n\n\treturn c.client.HGetAll(c.ctx, key).Result()\n}\n\nfunc (c *Client) HGet(key string, field string) (string, error) {\n\n\treturn c.client.HGet(c.ctx, key, field).Result()\n}\n\nfunc (c *Client) Get(key string) (string, error) {\n\n\treturn c.client.Get(c.ctx, key).Result()\n}\n\nfunc (c *Client) SetBytes(key string, value []byte) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetBytesExpire(key string, value []byte, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) GetBytes(key string) ([]byte, error) {\n\n\treturn c.client.Get(c.ctx, key).Bytes()\n}\n\nfunc (c *Client) SetNX(key string, value string, expiration time.Duration) bool {\n\treturn c.client.SetNX(c.ctx, key, value, expiration).Val()\n}\n\nfunc (c *Client) GetDel(key string) (string, error) {\n\n\treturn c.client.GetDel(c.ctx, key).Result()\n}\n\nfunc (c *Client) Del(key string) error {\n\treturn c.client.Del(c.ctx, key).Err()\n}\n\nfunc (c *Client) RPush(key string, val string) error {\n\treturn c.client.RPush(c.ctx, key, val).Err()\n}\n\nfunc (c *Client) LPop(key string) (string, error) {\n\treturn c.client.LPop(c.ctx, key).Result()\n}\n\n// ZAdd adds the specified member with the specified score to the sorted set stored at key.\nfunc (c *Client) ZAdd(key string, member string, score float64) error {\n\treturn c.client.ZAdd(c.ctx, key, &redis.Z{Score: score, Member: member}).Err()\n}\n\n// ZCount returns the number of elements in the sorted set at key with a score between min and max.\nfunc (c *Client) ZCount(key string, min, max string) (int64, error) {\n\treturn c.client.ZCount(c.ctx, key, min, max).Result()\n}\n\n// ZRem","infill":" removes the specified members from the sorted set stored at key.\nfunc (c *Client) ZRem(key string, members ...string) error {\n\treturn c.client.ZRem(c.ctx, key, members...).Err()\n}\n","suffix":"","relevantFile":"<file_path>internal/util/stringutil.go\npackage util\n\nfunc GetFixedLengthStr(str string, length int) string {\n\tif len(str) > length {\n\t\treturn str[0:length] + \"...\"\n\t}\n\treturn str\n}\n<file_path>internal/util/md5.go\npackage util\n\nimport (\n\t\"crypto/md5\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"sort\"\n)\n\nfunc GetMd5Str(str string) string {\n\thas := md5.Sum([]byte(str))\n\tmd5Str := fmt.Sprintf(\"%x\", has)\n\treturn md5Str\n}\n\nfunc GetMd5Sign(params map[string]string) string {\n\tkeys := make([]string, 0, len(params))\n\tfor k := range params {\n\t\tkeys = append(keys, k)\n\t}\n\tsort.Strings(keys)\n\tsign := \"metric\"\n\tfor _, key := range keys {\n\t\tsign += key\n\t\tsign += params[key]\n\t}\n\thash := md5.Sum([]byte(sign))\n\treturn hex.EncodeToString(hash[:])\n}\n<file_path>internal/util/timeutil.go\npackage util\n\nimport (\n\t\"github.com/rs/zerolog/log\"\n\t\"time\"\n)\n\nfunc GetTimeDuration(duration string) time.Duration {\n\tdur, err := time.ParseDuration(duration)\n\tif err != nil {\n\t\tlog.Err(err).Msgf(\"duration parse failed ! return default 1 hour; durationStr: %s\", duration)\n\t\treturn time.Duration(1) * time.Minute\n\t}\n\treturn dur\n}\n<file_path>internal/util/http.go\n\t\"time\"\n)\n\nconst (\n\tPromSource    = \"Source\"\n\tPromSourceVal = \"inspector\"\n)\n\nvar urlBuilderPool sync.Pool\n\nfunc init() {\n\turlBuilderPool = sync.Pool{New: func() interface{} {\n\t\treturn make([]byte, 0, 150)\n\t}}\n}\nfunc HttpGet(url string) ([]byte, error) {\n\tc := http.Client{\n\t\tTimeout: 30 * time.Second,\n\t}\n\tresp, err := c.Get(url)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPostForm(policyurl string, data url.Values, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.PostForm(policyurl, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPost(url string, bodyBytes []byte, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.Post(url, \"application/json\", bytes.NewBuffer(bodyBytes))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trespBytes, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn respBytes, err\n\t}","relevantFileList":[],"filePath":"internal/redis/client.go","template":"go","multiRes":{"hipilot":" removes the specified member from the sorted set stored at key.\nfunc (c *Client) ZRem(key string, members ...string) (int64, error) {\n    return c.client.ZRem(c.ctx, key, members).Result()\n}\n\n// ZRange","CodeQwen2.5-7B":" removes the specified members from the sorted set stored at key.\nfunc (c *Client) ZRem(key string, members ...string) (int64, error) {\n\treturn c.client.ZRem(c.ctx, key, members...).Result()\n}","copilot":" removes the specified members from the sorted set stored at key.\nfunc (c *Client) ZRem(key string, members ...string) error {\n\treturn c.client.ZRem(c.ctx, key, members...).Err()\n}\n","deepseek-chat-lite":" removes the specified member from the sorted set stored at key.","CodeQwen1.5-7B":"  removes the specified members from the sorted set stored at key. Non existing members are ignored."}},{"prefix":"package redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/go-redis/redis/v8\"\n\t\"github.com/rs/zerolog/log\"\n\t\"ths-monitor-apm-onlineuser/internal/config\"\n\t\"time\"\n)\n\nvar RedisClient *Client\n\ntype Client struct {\n\tclient *redis.Client\n\tctx    context.Context\n}\n\nfunc InitRedis(conf config.RedisConfig) {\n\tRedisClient = NewRedisClient(conf)\n}\n\nfunc NewRedisClient(conf config.RedisConfig) *Client {\n\tclient := redis.NewClient(&redis.Options{\n\t\tAddr:     conf.Addr,\n\t\tPassword: conf.Password,\n\t\tPoolSize: conf.PoolSize,\n\t})\n\tpong, err := client.Ping(context.Background()).Result()\n\tif err != nil {\n\t\tlog.Err(err).Msg(\"redis connect failed! exiting...\")\n\t\tpanic(fmt.Sprintf(\"redis connect failed! exiting... , err:%s \", err.Error()))\n\t}\n\tlog.Info().Msgf(\"redis ping result : %s\", pong)\n\treturn &Client{\n\t\tclient: client,\n\t\tctx:    context.Background(),\n\t}\n}\n\nfunc (c *Client) Set(key string, value string) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetExpire(key string, value string, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) Expire(key string, expiration time.Duration) error {\n\treturn c.client.Expire(c.ctx, key, expiration).Err()\n}\n\nfunc (c *Client) HSetExpire(key string, value map[string]interface{}, expiration time.Duration) error {\n\n\terr := c.client.HSet(c.ctx, key, value).Err()\n\tif err != nil {\n\t\treturn err\n\t} else {\n\t\treturn c.client.Expire(c.ctx, key, expiration).Err()\n\t}\n}\n\nfunc (c *Client) HSet(key string, value map[string]interface{}) error {\n\treturn c.client.HSet(c.ctx, key, value).Err()\n}\n\nfunc (c *Client) HGetAll(key string) (map[string]string, error) {\n\n\treturn c.client.HGetAll(c.ctx, key).Result()\n}\n\nfunc (c *Client) HGet(key string, field string) (string, error) {\n\n\treturn c.client.HGet(c.ctx, key, field).Result()\n}\n\nfunc (c *Client) Get(key string) (string, error) {\n\n\treturn c.client.Get(c.ctx, key).Result()\n}\n\nfunc (c *Client) SetBytes(key string, value []byte) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetBytesExpire(key string, value []byte, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) GetBytes(key string) ([]byte, error) {\n\n\treturn c.client.Get(c.ctx, key).Bytes()\n}\n\nfunc (c *Client) SetNX(key string, value string, expiration time.Duration) bool {\n\treturn c.client.SetNX(c.ctx, key, value, expiration).Val()\n}\n\nfunc (c *Client) GetDel(key string) (string, error) {\n\n\treturn c.client.GetDel(c.ctx, key).Result()\n}\n\nfunc (c *Client) Del(key string) error {\n\treturn c.client.Del(c.ctx, key).Err()\n}\n\nfunc (c *Client) RPush(key string, val string) error {\n\treturn c.client.RPush(c.ctx, key, val).Err()\n}\n\nfunc (c *Client) LPop(key string) (string, error) {\n\treturn c.client.LPop(c.ctx, key).Result()\n}\n\n// ZAdd adds the specified member with the specified score to the sorted set stored at key.\nfunc (c *Client) ZAdd(key string, member string, score float64) error {\n\treturn c.client.ZAdd(c.ctx, key, &redis.Z{Score: score, Member: member}).Err()\n}\n\n// ZCount returns the number of elements in the sorted set at key with a score between min and max.\nfunc (c *Client) ZCount(key string, min, max string) (int64, error) {\n\treturn c.client.ZCount(c.ctx, key, min, max).Result()\n}\n\n// ZRem removes the specified members from the sorted set stored at key.","infill":"func (c *Client) ZRemRangeByScore(key string, members ...string) error {\n\treturn c.client.ZRemRangeByScore(c.ctx, key, members...).Err()\n}","suffix":"\n","relevantFile":"<file_path>internal/util/stringutil.go\npackage util\n\nfunc GetFixedLengthStr(str string, length int) string {\n\tif len(str) > length {\n\t\treturn str[0:length] + \"...\"\n\t}\n\treturn str\n}\n<file_path>internal/util/md5.go\npackage util\n\nimport (\n\t\"crypto/md5\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"sort\"\n)\n\nfunc GetMd5Str(str string) string {\n\thas := md5.Sum([]byte(str))\n\tmd5Str := fmt.Sprintf(\"%x\", has)\n\treturn md5Str\n}\n\nfunc GetMd5Sign(params map[string]string) string {\n\tkeys := make([]string, 0, len(params))\n\tfor k := range params {\n\t\tkeys = append(keys, k)\n\t}\n\tsort.Strings(keys)\n\tsign := \"metric\"\n\tfor _, key := range keys {\n\t\tsign += key\n\t\tsign += params[key]\n\t}\n\thash := md5.Sum([]byte(sign))\n\treturn hex.EncodeToString(hash[:])\n}\n<file_path>internal/util/timeutil.go\npackage util\n\nimport (\n\t\"github.com/rs/zerolog/log\"\n\t\"time\"\n)\n\nfunc GetTimeDuration(duration string) time.Duration {\n\tdur, err := time.ParseDuration(duration)\n\tif err != nil {\n\t\tlog.Err(err).Msgf(\"duration parse failed ! return default 1 hour; durationStr: %s\", duration)\n\t\treturn time.Duration(1) * time.Minute\n\t}\n\treturn dur\n}\n<file_path>internal/util/http.go\n\t\"time\"\n)\n\nconst (\n\tPromSource    = \"Source\"\n\tPromSourceVal = \"inspector\"\n)\n\nvar urlBuilderPool sync.Pool\n\nfunc init() {\n\turlBuilderPool = sync.Pool{New: func() interface{} {\n\t\treturn make([]byte, 0, 150)\n\t}}\n}\nfunc HttpGet(url string) ([]byte, error) {\n\tc := http.Client{\n\t\tTimeout: 30 * time.Second,\n\t}\n\tresp, err := c.Get(url)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPostForm(policyurl string, data url.Values, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.PostForm(policyurl, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPost(url string, bodyBytes []byte, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.Post(url, \"application/json\", bytes.NewBuffer(bodyBytes))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trespBytes, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn respBytes, err\n\t}","relevantFileList":[],"filePath":"internal/redis/client.go","template":"go","multiRes":{"hipilot":"","CodeQwen2.5-7B":"\nfunc (c *Client) ZRem(key string, members ...string) (int64, error) {\n\treturn c.client.ZRem(c.ctx, key, members...).Result()\n}","copilot":"func (c *Client) ZRemRangeByScore(key string, members ...string) error {\n\treturn c.client.ZRemRangeByScore(c.ctx, key, members...).Err()\n}","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}},{"prefix":"package redis\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"github.com/go-redis/redis/v8\"\n\t\"github.com/rs/zerolog/log\"\n\t\"ths-monitor-apm-onlineuser/internal/config\"\n\t\"time\"\n)\n\nvar RedisClient *Client\n\ntype Client struct {\n\tclient *redis.Client\n\tctx    context.Context\n}\n\nfunc InitRedis(conf config.RedisConfig) {\n\tRedisClient = NewRedisClient(conf)\n}\n\nfunc NewRedisClient(conf config.RedisConfig) *Client {\n\tclient := redis.NewClient(&redis.Options{\n\t\tAddr:     conf.Addr,\n\t\tPassword: conf.Password,\n\t\tPoolSize: conf.PoolSize,\n\t})\n\tpong, err := client.Ping(context.Background()).Result()\n\tif err != nil {\n\t\tlog.Err(err).Msg(\"redis connect failed! exiting...\")\n\t\tpanic(fmt.Sprintf(\"redis connect failed! exiting... , err:%s \", err.Error()))\n\t}\n\tlog.Info().Msgf(\"redis ping result : %s\", pong)\n\treturn &Client{\n\t\tclient: client,\n\t\tctx:    context.Background(),\n\t}\n}\n\nfunc (c *Client) Set(key string, value string) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetExpire(key string, value string, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) Expire(key string, expiration time.Duration) error {\n\treturn c.client.Expire(c.ctx, key, expiration).Err()\n}\n\nfunc (c *Client) HSetExpire(key string, value map[string]interface{}, expiration time.Duration) error {\n\n\terr := c.client.HSet(c.ctx, key, value).Err()\n\tif err != nil {\n\t\treturn err\n\t} else {\n\t\treturn c.client.Expire(c.ctx, key, expiration).Err()\n\t}\n}\n\nfunc (c *Client) HSet(key string, value map[string]interface{}) error {\n\treturn c.client.HSet(c.ctx, key, value).Err()\n}\n\nfunc (c *Client) HGetAll(key string) (map[string]string, error) {\n\n\treturn c.client.HGetAll(c.ctx, key).Result()\n}\n\nfunc (c *Client) HGet(key string, field string) (string, error) {\n\n\treturn c.client.HGet(c.ctx, key, field).Result()\n}\n\nfunc (c *Client) Get(key string) (string, error) {\n\n\treturn c.client.Get(c.ctx, key).Result()\n}\n\nfunc (c *Client) SetBytes(key string, value []byte) error {\n\n\treturn c.client.Set(c.ctx, key, value, 0).Err()\n}\n\nfunc (c *Client) SetBytesExpire(key string, value []byte, expiration time.Duration) error {\n\n\treturn c.client.Set(c.ctx, key, value, expiration).Err()\n}\n\nfunc (c *Client) GetBytes(key string) ([]byte, error) {\n\n\treturn c.client.Get(c.ctx, key).Bytes()\n}\n\nfunc (c *Client) SetNX(key string, value string, expiration time.Duration) bool {\n\treturn c.client.SetNX(c.ctx, key, value, expiration).Val()\n}\n\nfunc (c *Client) GetDel(key string) (string, error) {\n\n\treturn c.client.GetDel(c.ctx, key).Result()\n}\n\nfunc (c *Client) Del(key string) error {\n\treturn c.client.Del(c.ctx, key).Err()\n}\n\nfunc (c *Client) RPush(key string, val string) error {\n\treturn c.client.RPush(c.ctx, key, val).Err()\n}\n\nfunc (c *Client) LPop(key string) (string, error) {\n\treturn c.client.LPop(c.ctx, key).Result()\n}\n\n// ZAdd adds the specified member with the specified score to the sorted set stored at key.\nfunc (c *Client) ZAdd(key string, member string, score float64) error {\n\treturn c.client.ZAdd(c.ctx, key, &redis.Z{Score: score, Member: member}).Err()\n}\n\n// ZCount returns the number of elements in the sorted set at key with a score between min and max.\nfunc (c *Client) ZCount(key string, min, max string) (int64, error) {\n\treturn c.client.ZCount(c.ctx, key, min, max).Result()\n}\n\n// ZRemRangeByScore removes the specified members from the sorted set stored at key.\nfunc (c *Client) ZRemRangeByScore(key string, members ...string) error {","infill":"\n\treturn c.client.ZRemRangeByScore(c.ctx, key, members...).Err()\n}\n","suffix":"","relevantFile":"<file_path>internal/util/stringutil.go\npackage util\n\nfunc GetFixedLengthStr(str string, length int) string {\n\tif len(str) > length {\n\t\treturn str[0:length] + \"...\"\n\t}\n\treturn str\n}\n<file_path>internal/util/md5.go\npackage util\n\nimport (\n\t\"crypto/md5\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"sort\"\n)\n\nfunc GetMd5Str(str string) string {\n\thas := md5.Sum([]byte(str))\n\tmd5Str := fmt.Sprintf(\"%x\", has)\n\treturn md5Str\n}\n\nfunc GetMd5Sign(params map[string]string) string {\n\tkeys := make([]string, 0, len(params))\n\tfor k := range params {\n\t\tkeys = append(keys, k)\n\t}\n\tsort.Strings(keys)\n\tsign := \"metric\"\n\tfor _, key := range keys {\n\t\tsign += key\n\t\tsign += params[key]\n\t}\n\thash := md5.Sum([]byte(sign))\n\treturn hex.EncodeToString(hash[:])\n}\n<file_path>internal/util/timeutil.go\npackage util\n\nimport (\n\t\"github.com/rs/zerolog/log\"\n\t\"time\"\n)\n\nfunc GetTimeDuration(duration string) time.Duration {\n\tdur, err := time.ParseDuration(duration)\n\tif err != nil {\n\t\tlog.Err(err).Msgf(\"duration parse failed ! return default 1 hour; durationStr: %s\", duration)\n\t\treturn time.Duration(1) * time.Minute\n\t}\n\treturn dur\n}\n<file_path>internal/util/http.go\n\t\"time\"\n)\n\nconst (\n\tPromSource    = \"Source\"\n\tPromSourceVal = \"inspector\"\n)\n\nvar urlBuilderPool sync.Pool\n\nfunc init() {\n\turlBuilderPool = sync.Pool{New: func() interface{} {\n\t\treturn make([]byte, 0, 150)\n\t}}\n}\nfunc HttpGet(url string) ([]byte, error) {\n\tc := http.Client{\n\t\tTimeout: 30 * time.Second,\n\t}\n\tresp, err := c.Get(url)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPostForm(policyurl string, data url.Values, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.PostForm(policyurl, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer resp.Body.Close()\n\treturn ioutil.ReadAll(resp.Body)\n}\n\nfunc HttpPost(url string, bodyBytes []byte, timeout time.Duration) ([]byte, error) {\n\ttimeoutD := 10 * time.Second\n\tif timeout >= 0 {\n\t\ttimeoutD = timeout\n\t}\n\tclient := http.Client{\n\t\tTimeout: timeoutD,\n\t}\n\tresp, err := client.Post(url, \"application/json\", bytes.NewBuffer(bodyBytes))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trespBytes, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn respBytes, err\n\t}","relevantFileList":[],"filePath":"internal/redis/client.go","template":"go","multiRes":{"hipilot":"\n    return c.client.ZRemRangeByScore(c.ctx, key, members...).Err()\n}\n\n// ZRem removes the specified member from the sorted set stored at key.\nfunc (c *Client) ZRem(key string, member string) error {\n    return","CodeQwen2.5-7B":"\n\treturn c.client.ZRemRangeByScore(c.ctx, key, \"-inf\", \"+inf\").Err()\n}","copilot":"\n\treturn c.client.ZRemRangeByScore(c.ctx, key, members...).Err()\n}\n","deepseek-chat-lite":"","CodeQwen1.5-7B":" "}}]